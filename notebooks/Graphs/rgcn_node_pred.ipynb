{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outcome Prediction with RGCN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data from \"processed_data\" folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import RGCNConv\n",
    "from torch_geometric.utils import to_undirected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<http://nvasc.org/9580e5bd-472d-49e1-9bed-c2b6d0029253> <http://sphn.org/hasSubjectPseudoIdentifier> <http://nvasc.org/synth_patient_843> .\n",
      "\n",
      "                                                   s  \\\n",
      "0  <http://nvasc.org/2c77cf90-594b-49bd-a5eb-a9b5...   \n",
      "1  <http://nvasc.org/66c76a7b-87bb-4341-bbae-93dd...   \n",
      "2  <http://nvasc.org/a44c2a60-b5a5-49c3-b230-09dd...   \n",
      "3  <http://nvasc.org/016c2574-c8d5-4e5d-b523-c5b9...   \n",
      "4  <http://nvasc.org/14848ddf-f39b-4126-8f5f-79b9...   \n",
      "\n",
      "                                              r  \\\n",
      "0  <http://sphn.org/hasSubjectPseudoIdentifier>   \n",
      "1           <http://sphn.org/hasRecordDateTime>   \n",
      "2  <http://sphn.org/hasSubjectPseudoIdentifier>   \n",
      "3  <http://sphn.org/hasSubjectPseudoIdentifier>   \n",
      "4  <http://sphn.org/hasSubjectPseudoIdentifier>   \n",
      "\n",
      "                                                   d  \n",
      "0              <http://nvasc.org/synth_patient_2238>  \n",
      "1  \"2022-05-27T14:30:00\"^^<http://www.w3.org/2001...  \n",
      "2              <http://nvasc.org/synth_patient_5125>  \n",
      "3              <http://nvasc.org/synth_patient_5123>  \n",
      "4              <http://nvasc.org/synth_patient_5048>  \n"
     ]
    }
   ],
   "source": [
    "dataset = '../Data Generation/sphn_transductive_1000_0.nt'\n",
    "inverse_triples = True\n",
    "\n",
    "def preprocess(data_name):\n",
    "    s_list, r_list, d_list = [], [], []\n",
    "    \n",
    "    with open(data_name) as f:\n",
    "        s = next(f)\n",
    "        print(s)\n",
    "        for idx, line in enumerate(f):\n",
    "            e = line.strip().split(' ')\n",
    "            s = e[0]\n",
    "            r = e[1]\n",
    "            d = e[2]            \n",
    "            \n",
    "            s_list.append(s)\n",
    "            r_list.append(r)\n",
    "            d_list.append(d)            \n",
    "            \n",
    "    return pd.DataFrame({\n",
    "        's':s_list, \n",
    "        'r':r_list, \n",
    "        'd':d_list,                          \n",
    "        })\n",
    "\n",
    "df = preprocess(dataset)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_patients = 1000\n",
    "\n",
    "outcome = df['d'].str.contains('outcome_0.0|outcome_1.0|outcome_2.0')\n",
    "node_df = df[~outcome]\n",
    "node_df = node_df.reset_index(drop=True)\n",
    "outcome = node_df['s'].str.contains('outcome_0.0|outcome_1.0|outcome_2.0')\n",
    "node_df = node_df[~outcome]\n",
    "node_df = node_df.reset_index(drop=True)\n",
    "\n",
    "ent_to_id = {k: v for v, k in enumerate(set(node_df['s']).union(set(node_df['d'])), start=0)}\n",
    "rel_to_id = {k: v for v, k in enumerate(set(node_df['r']), start=0)}\n",
    "\n",
    "patients = [f\"<http://nvasc.org/synth_patient_{i}>\" for i in range(num_patients)]\n",
    "patient_id = []\n",
    "for patient in patients:\n",
    "    patient_id.append(ent_to_id[patient])\n",
    "\n",
    "num_nodes = max(ent_to_id.values()) + 1\n",
    "num_rels = max(rel_to_id.values()) + 1\n",
    "\n",
    "events = node_df.copy()\n",
    "events[\"s\"] = node_df.s.map(ent_to_id)\n",
    "events[\"d\"] = node_df.d.map(ent_to_id)\n",
    "events[\"r\"] = node_df.r.map(rel_to_id)\n",
    "\n",
    "ent_to_id = pd.DataFrame.from_dict(ent_to_id, orient='index')\n",
    "rel_to_id = pd.DataFrame.from_dict(rel_to_id, orient='index')\n",
    "\n",
    "path = 'processed_data'\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "events.to_csv(f'{path}/sphn_events_noOutcome.tsv', sep='\\t', index=False, header=None)\n",
    "ent_to_id.to_csv(f'{path}/sphn_entities_noOutcome.tsv', sep='\\t', header=None)\n",
    "rel_to_id.to_csv(f'{path}/sphn_relations_noOutcome.tsv', sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_patients = 1000\n",
    "inverse_triples=True\n",
    "embed_dim = 32\n",
    "\n",
    "entity = pd.read_csv('processed_data/sphn_entities_noOutcome.tsv', sep='\\t', index_col=0, header=None)\n",
    "entity = entity.to_dict()[1]\n",
    "\n",
    "indices = []\n",
    "for i in range(num_patients):\n",
    "    idx = f'<http://nvasc.org/synth_patient_{i}>'\n",
    "    indices.append(entity[idx])\n",
    "# events = pd.read_csv('../data/SPHN_events_noOutcome.tsv', sep='\\t', header=None)\n",
    "events = pd.read_csv('processed_data/sphn_events_noOutcome.tsv', sep='\\t', header=None)\n",
    "y = joblib.load('../Data Generation/outcomes_1000_0.joblib')\n",
    "\n",
    "non_valid_X, valid_X, non_valid_y, valid_y = train_test_split(indices, y, stratify=y, test_size=0.2)\n",
    "train_X, testing_X, train_y, testing_y = train_test_split(non_valid_X, non_valid_y, stratify=non_valid_y, test_size=1./8)\n",
    "\n",
    "# rus = RandomUnderSampler(random_state=0)\n",
    "# under_idx, under_y = rus.fit_resample(np.asarray(train_X).reshape(-1, 1), np.asarray(train_y).reshape(-1,1))\n",
    "# indices = np.asarray(indices)\n",
    "# y = np.asarray(y)\n",
    "\n",
    "edge_index = torch.vstack((torch.Tensor(events[0]).long(),torch.Tensor(events[2]).long()))\n",
    "edge_type = torch.Tensor(events[1]).long()\n",
    "train_idx = torch.Tensor(train_X).long().squeeze()\n",
    "train_y = torch.Tensor(train_y).long()\n",
    "val_idx = torch.Tensor(valid_X).long()\n",
    "val_y = torch.Tensor(valid_y).long()\n",
    "test_idx = torch.Tensor(testing_X).long()\n",
    "test_y = torch.Tensor(testing_y).long()\n",
    "num_nodes = len(entity)\n",
    "\n",
    "if inverse_triples == True:\n",
    "    edge_index = to_undirected(edge_index)\n",
    "    edge_type = torch.cat((edge_type, edge_type))\n",
    "\n",
    "data = Data(\n",
    "    edge_index=edge_index,\n",
    "    edge_type=edge_type,\n",
    "    train_idx=train_idx,\n",
    "    train_y=train_y,\n",
    "    val_idx=val_idx,\n",
    "    val_y=val_y,\n",
    "    test_idx=test_idx,\n",
    "    test_y=test_y,\n",
    "    num_nodes=num_nodes,\n",
    ")\n",
    "embedding = torch.nn.Embedding(data.num_nodes, embed_dim)\n",
    "data.x = embedding.weight\n",
    "data.num_relations = data.num_edge_types\n",
    "data.num_classes = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(edge_index=[2, 223868], edge_type=[223868], train_idx=[700], train_y=[700], val_idx=[200], val_y=[200], test_idx=[100], test_y=[100], num_nodes=32563, x=[32563, 32], num_relations=8, num_classes=3)\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = RGCNConv(embed_dim, embed_dim, data.num_relations,\n",
    "                          num_bases=10)\n",
    "        self.conv2 = RGCNConv(embed_dim, data.num_classes, data.num_relations,\n",
    "                          num_bases=10)\n",
    "\n",
    "    def forward(self, edge_index, edge_type):\n",
    "        x = F.relu(self.conv1(data.x, edge_index, edge_type))\n",
    "        x = F.dropout(x, p=0.5)\n",
    "        x = self.conv2(x, edge_index, edge_type)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "model, data = Net().to(device), data.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "# class_weight = torch.bincount(train_y).to(device)\n",
    "# class_weight = class_weight / class_weight.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.edge_index, data.edge_type)\n",
    "    loss = F.nll_loss(out[data.train_idx], data.train_y, \n",
    "                    #   weight=class_weight,\n",
    "                      )\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return float(loss)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    pred = model(data.edge_index, data.edge_type).argmax(dim=-1)\n",
    "    train_acc = float((pred[data.train_idx] == data.train_y).float().mean())\n",
    "    val_acc = float((pred[data.val_idx] == data.val_y).float().mean())\n",
    "    test_acc = float((pred[data.test_idx] == data.test_y).float().mean())\n",
    "    return train_acc, val_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50, Loss: 1.6164 | Train: 0.6100 Valid: 0.4200 Test: 0.4800\n",
      "Epoch: 100, Loss: 0.9528 | Train: 0.6443 Valid: 0.3900 Test: 0.4700\n",
      "Epoch: 150, Loss: 0.7092 | Train: 0.7171 Valid: 0.4200 Test: 0.4700\n",
      "Epoch: 200, Loss: 0.4994 | Train: 0.7857 Valid: 0.4700 Test: 0.3600\n",
      "Epoch: 250, Loss: 0.4057 | Train: 0.8271 Valid: 0.4450 Test: 0.4200\n",
      "Epoch: 300, Loss: 0.3339 | Train: 0.8514 Valid: 0.3800 Test: 0.4600\n",
      "Epoch: 350, Loss: 0.3049 | Train: 0.8814 Valid: 0.4200 Test: 0.4800\n",
      "Epoch: 400, Loss: 0.2538 | Train: 0.9057 Valid: 0.4150 Test: 0.4600\n",
      "Epoch: 450, Loss: 0.2359 | Train: 0.9086 Valid: 0.4200 Test: 0.4700\n",
      "Epoch: 500, Loss: 0.2058 | Train: 0.9400 Valid: 0.4300 Test: 0.4500\n",
      "Epoch: 550, Loss: 0.1604 | Train: 0.9429 Valid: 0.4200 Test: 0.4900\n",
      "Epoch: 600, Loss: 0.1717 | Train: 0.9500 Valid: 0.4650 Test: 0.4800\n",
      "Epoch: 650, Loss: 0.1321 | Train: 0.9443 Valid: 0.4250 Test: 0.4600\n",
      "Epoch: 700, Loss: 0.1227 | Train: 0.9729 Valid: 0.4350 Test: 0.4600\n",
      "Epoch: 750, Loss: 0.1106 | Train: 0.9686 Valid: 0.4700 Test: 0.5200\n",
      "Epoch: 800, Loss: 0.0814 | Train: 0.9700 Valid: 0.4300 Test: 0.4300\n",
      "Epoch: 850, Loss: 0.0859 | Train: 0.9686 Valid: 0.4650 Test: 0.5100\n",
      "Epoch: 900, Loss: 0.0929 | Train: 0.9671 Valid: 0.4750 Test: 0.4800\n",
      "Epoch: 950, Loss: 0.0830 | Train: 0.9786 Valid: 0.4050 Test: 0.4700\n",
      "Epoch: 1000, Loss: 0.0920 | Train: 0.9786 Valid: 0.4650 Test: 0.4800\n",
      "Epoch: 1050, Loss: 0.0621 | Train: 0.9714 Valid: 0.4300 Test: 0.4600\n",
      "Epoch: 1100, Loss: 0.0481 | Train: 0.9786 Valid: 0.4750 Test: 0.4500\n",
      "Epoch: 1150, Loss: 0.0588 | Train: 0.9886 Valid: 0.4150 Test: 0.4100\n",
      "Epoch: 1200, Loss: 0.0729 | Train: 0.9843 Valid: 0.4600 Test: 0.4700\n",
      "Epoch: 1250, Loss: 0.0435 | Train: 0.9786 Valid: 0.4800 Test: 0.4500\n",
      "Epoch: 1300, Loss: 0.0503 | Train: 0.9886 Valid: 0.4250 Test: 0.4800\n",
      "Epoch: 1350, Loss: 0.0475 | Train: 0.9843 Valid: 0.4850 Test: 0.4800\n",
      "Epoch: 1400, Loss: 0.0408 | Train: 0.9900 Valid: 0.4750 Test: 0.5200\n",
      "Epoch: 1450, Loss: 0.0370 | Train: 0.9814 Valid: 0.4850 Test: 0.4900\n",
      "Epoch: 1500, Loss: 0.0279 | Train: 0.9871 Valid: 0.4400 Test: 0.4400\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[101]\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_acc = 0.\n",
    "times = []\n",
    "for epoch in range(1, 10001):\n",
    "    start = time.time()\n",
    "    loss = train()\n",
    "    if epoch % 50 == 0:\n",
    "        train_acc, val_acc, test_acc = test()\n",
    "        print(f'Epoch: {epoch:02d}, Loss: {loss:.4f} | ' \n",
    "            f'Train: {train_acc:.4f} '\n",
    "            f'Valid: {val_acc:.4f} '\n",
    "            f'Test: {test_acc:.4f}')    \n",
    "        times.append(time.time() - start)\n",
    "        # # Early stopping\n",
    "        # if val_acc > best_acc:\n",
    "        #     best_acc = val_acc\n",
    "        #     patience = 10  # Reset patience counter\n",
    "        # else:\n",
    "        #     patience -= 1\n",
    "        #     if patience == 0:\n",
    "        #         print(\"Early stopping...\")\n",
    "        #         break\n",
    "print(f\"Median time per epoch: {torch.tensor(times).median():.4f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC score: 0.4968\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(data.edge_index, data.edge_type).cpu()\n",
    "    prob = F.softmax(out, dim=1)\n",
    "auc = roc_auc_score(data.test_y.cpu(), prob[data.test_idx.cpu()], multi_class='ovr')\n",
    "print(f'ROC AUC score: {auc:.4f}')\n",
    "# print(prob.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neurovasc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
