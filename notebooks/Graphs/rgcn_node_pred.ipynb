{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outcome Prediction with RGCN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data from \"processed_data\" folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import RGCNConv\n",
    "from torch_geometric.utils import to_undirected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_patients = 1000\n",
    "inverse_triples=True\n",
    "embed_dim = 32\n",
    "\n",
    "entity = pd.read_csv(f'processed_data/sphn_entities_{num_patients}_noOutcome.tsv', sep='\\t', index_col=0, header=None)\n",
    "entity = entity.to_dict()[1]\n",
    "\n",
    "indices = []\n",
    "for i in range(num_patients):\n",
    "    idx = f'<http://nvasc.org/synth_patient_{i}>'\n",
    "    indices.append(entity[idx])\n",
    "# events = pd.read_csv('../data/SPHN_events_noOutcome.tsv', sep='\\t', header=None)\n",
    "events = pd.read_csv(f'processed_data/sphn_events_{num_patients}_noOutcome.tsv', sep='\\t', header=None)\n",
    "y = joblib.load(f'../Data Generation/outcomes_{num_patients}_0.joblib')\n",
    "\n",
    "non_valid_X, valid_X, non_valid_y, valid_y = train_test_split(indices, y, stratify=y, test_size=0.2)\n",
    "train_X, testing_X, train_y, testing_y = train_test_split(non_valid_X, non_valid_y, stratify=non_valid_y, test_size=1./8)\n",
    "\n",
    "# rus = RandomUnderSampler(random_state=0)\n",
    "# under_idx, under_y = rus.fit_resample(np.asarray(train_X).reshape(-1, 1), np.asarray(train_y).reshape(-1,1))\n",
    "# indices = np.asarray(indices)\n",
    "# y = np.asarray(y)\n",
    "\n",
    "edge_index = torch.vstack((torch.Tensor(events[0]).long(),torch.Tensor(events[2]).long()))\n",
    "edge_type = torch.Tensor(events[1]).long()\n",
    "train_idx = torch.Tensor(train_X).long().squeeze()\n",
    "train_y = torch.Tensor(train_y).long()\n",
    "val_idx = torch.Tensor(valid_X).long()\n",
    "val_y = torch.Tensor(valid_y).long()\n",
    "test_idx = torch.Tensor(testing_X).long()\n",
    "test_y = torch.Tensor(testing_y).long()\n",
    "num_nodes = len(entity)\n",
    "\n",
    "if inverse_triples == True:\n",
    "    edge_index = to_undirected(edge_index)\n",
    "    edge_type = torch.cat((edge_type, edge_type))\n",
    "\n",
    "data = Data(\n",
    "    edge_index=edge_index,\n",
    "    edge_type=edge_type,\n",
    "    train_idx=train_idx,\n",
    "    train_y=train_y,\n",
    "    val_idx=val_idx,\n",
    "    val_y=val_y,\n",
    "    test_idx=test_idx,\n",
    "    test_y=test_y,\n",
    "    num_nodes=num_nodes,\n",
    ")\n",
    "embedding = Parameter(torch.empty(num_nodes, embed_dim))\n",
    "torch.nn.init.xavier_uniform_(embedding, gain=math.sqrt(2.0))\n",
    "data.x = embedding\n",
    "data.num_relations = data.num_edge_types\n",
    "data.num_classes = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(edge_index=[2, 221870], edge_type=[221870], train_idx=[700], train_y=[700], val_idx=[200], val_y=[200], test_idx=[100], test_y=[100], num_nodes=33563, x=[33563, 32], num_relations=10, num_classes=3)\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = RGCNConv(embed_dim, embed_dim, data.num_relations,\n",
    "                          num_bases=4)\n",
    "        self.conv2 = RGCNConv(embed_dim, data.num_classes, data.num_relations,\n",
    "                          num_bases=4)\n",
    "\n",
    "    def forward(self, edge_index, edge_type):\n",
    "        x = F.relu(self.conv1(data.x, edge_index, edge_type))\n",
    "        x = F.dropout(x, p=0.2)\n",
    "        x = self.conv2(x, edge_index, edge_type)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "model, data = Net().to(device), data.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "# class_weight = torch.bincount(train_y).to(device)\n",
    "# class_weight = class_weight / class_weight.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.edge_index, data.edge_type)\n",
    "    loss = F.nll_loss(out[data.train_idx], data.train_y, \n",
    "                    #   weight=class_weight,\n",
    "                      )\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return float(loss)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    pred = model(data.edge_index, data.edge_type).argmax(dim=-1)\n",
    "    train_acc = float((pred[data.train_idx] == data.train_y).float().mean())\n",
    "    val_acc = float((pred[data.val_idx] == data.val_y).float().mean())\n",
    "    test_acc = float((pred[data.test_idx] == data.test_y).float().mean())\n",
    "    return train_acc, val_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50, Loss: 0.9813 | Train: 0.5029 Valid: 0.4550 Test: 0.4300\n",
      "Epoch: 100, Loss: 0.9423 | Train: 0.5500 Valid: 0.4600 Test: 0.4800\n",
      "Epoch: 150, Loss: 0.9024 | Train: 0.6200 Valid: 0.4200 Test: 0.4600\n",
      "Epoch: 200, Loss: 0.8458 | Train: 0.6371 Valid: 0.4450 Test: 0.4800\n",
      "Epoch: 250, Loss: 0.7707 | Train: 0.6800 Valid: 0.4750 Test: 0.4500\n",
      "Epoch: 300, Loss: 0.6911 | Train: 0.7314 Valid: 0.4850 Test: 0.4600\n",
      "Epoch: 350, Loss: 0.5845 | Train: 0.7871 Valid: 0.4250 Test: 0.3900\n",
      "Epoch: 400, Loss: 0.5061 | Train: 0.8243 Valid: 0.4500 Test: 0.4300\n",
      "Epoch: 450, Loss: 0.4183 | Train: 0.8514 Valid: 0.4550 Test: 0.4200\n",
      "Epoch: 500, Loss: 0.3616 | Train: 0.9071 Valid: 0.4800 Test: 0.3600\n",
      "Epoch: 550, Loss: 0.2949 | Train: 0.9114 Valid: 0.4450 Test: 0.4000\n",
      "Epoch: 600, Loss: 0.2348 | Train: 0.9443 Valid: 0.4700 Test: 0.3800\n",
      "Epoch: 650, Loss: 0.2006 | Train: 0.9543 Valid: 0.4400 Test: 0.4000\n",
      "Epoch: 700, Loss: 0.1875 | Train: 0.9686 Valid: 0.4250 Test: 0.4200\n",
      "Epoch: 750, Loss: 0.1479 | Train: 0.9700 Valid: 0.4350 Test: 0.4400\n",
      "Epoch: 800, Loss: 0.1432 | Train: 0.9657 Valid: 0.4600 Test: 0.4600\n",
      "Epoch: 850, Loss: 0.1069 | Train: 0.9686 Valid: 0.4450 Test: 0.4200\n",
      "Epoch: 900, Loss: 0.1020 | Train: 0.9843 Valid: 0.4450 Test: 0.4300\n",
      "Epoch: 950, Loss: 0.0995 | Train: 0.9800 Valid: 0.4200 Test: 0.4000\n",
      "Epoch: 1000, Loss: 0.0886 | Train: 0.9800 Valid: 0.4500 Test: 0.3600\n",
      "Median time per epoch: 0.0439s\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0.\n",
    "times = []\n",
    "for epoch in range(1, 1001):\n",
    "    start = time.time()\n",
    "    loss = train()\n",
    "    if epoch % 50 == 0:\n",
    "        train_acc, val_acc, test_acc = test()\n",
    "        print(f'Epoch: {epoch:02d}, Loss: {loss:.4f} | ' \n",
    "            f'Train: {train_acc:.4f} '\n",
    "            f'Valid: {val_acc:.4f} '\n",
    "            f'Test: {test_acc:.4f}')    \n",
    "        times.append(time.time() - start)\n",
    "        # # Early stopping\n",
    "        # if val_acc > best_acc:\n",
    "        #     best_acc = val_acc\n",
    "        #     patience = 10  # Reset patience counter\n",
    "        # else:\n",
    "        #     patience -= 1\n",
    "        #     if patience == 0:\n",
    "        #         print(\"Early stopping...\")\n",
    "        #         break\n",
    "print(f\"Median time per epoch: {torch.tensor(times).median():.4f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC score: 0.4771\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(data.edge_index, data.edge_type).cpu()\n",
    "    prob = F.softmax(out, dim=1)\n",
    "auc = roc_auc_score(data.test_y.cpu(), prob[data.test_idx.cpu()], multi_class='ovr')\n",
    "print(f'ROC AUC score: {auc:.4f}')\n",
    "# print(prob.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neurovasc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
