{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outcome Prediction (Node Classification) with TransE Model + MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pykeen.triples import TriplesFactory\n",
    "from pykeen.predict import predict_target\n",
    "from pykeen.pipeline import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load KG and remove outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>s</th>\n",
       "      <th>r</th>\n",
       "      <th>d</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;http://nvasc.org/f3fa4fb2-13a0-4a66-ba7d-6dd8...</td>\n",
       "      <td>&lt;http://sphn.org/hasSubjectPseudoIdentifier&gt;</td>\n",
       "      <td>&lt;http://nvasc.org/synth_patient_297&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;http://nvasc.org/b07f8ee5-4567-44cc-b73b-c422...</td>\n",
       "      <td>&lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#type&gt;</td>\n",
       "      <td>&lt;http://sphn.org/Diagnosis&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;http://nvasc.org/98e1eab9-5a65-406d-bb22-a493...</td>\n",
       "      <td>&lt;http://sphn.org/hasCode&gt;</td>\n",
       "      <td>&lt;http://nvasc.org/code_ttt_1.0&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;http://nvasc.org/37c5b02b-aab7-4dc0-95c5-45e3...</td>\n",
       "      <td>&lt;http://sphn.org/hasCode&gt;</td>\n",
       "      <td>&lt;http://nvasc.org/code_hct_0.0&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;http://nvasc.org/bba15f0e-1564-46fd-8dfe-1a3f...</td>\n",
       "      <td>&lt;http://www.w3.org/2000/01/rdf-schema#label&gt;</td>\n",
       "      <td>entry^^&lt;http://www.w3.org/2001/XMLSchema#string&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111930</th>\n",
       "      <td>_:nc344c15bf8b1413c8d0015e72ea473a0b1</td>\n",
       "      <td>&lt;http://sphn.org/hasValue&gt;</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111931</th>\n",
       "      <td>_:nbbac1cd7013a40b69fd09a15a979ccf9b1</td>\n",
       "      <td>&lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#type&gt;</td>\n",
       "      <td>&lt;http://sphn.org/Quantity&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111932</th>\n",
       "      <td>&lt;http://nvasc.org/cd26af13-47f0-4df4-8216-c629...</td>\n",
       "      <td>&lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#type&gt;</td>\n",
       "      <td>&lt;http://sphn.org/Diagnosis&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111933</th>\n",
       "      <td>_:ndc3844cacda3493496853bdf6c2ea8f9b1</td>\n",
       "      <td>&lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#type&gt;</td>\n",
       "      <td>&lt;http://sphn.org/Quantity&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111934</th>\n",
       "      <td>&lt;http://nvasc.org/f8ddd9c8-dcc5-45c8-939c-72eb...</td>\n",
       "      <td>&lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#type&gt;</td>\n",
       "      <td>&lt;http://sphn.org/Diagnosis&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>111935 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        s  \\\n",
       "0       <http://nvasc.org/f3fa4fb2-13a0-4a66-ba7d-6dd8...   \n",
       "1       <http://nvasc.org/b07f8ee5-4567-44cc-b73b-c422...   \n",
       "2       <http://nvasc.org/98e1eab9-5a65-406d-bb22-a493...   \n",
       "3       <http://nvasc.org/37c5b02b-aab7-4dc0-95c5-45e3...   \n",
       "4       <http://nvasc.org/bba15f0e-1564-46fd-8dfe-1a3f...   \n",
       "...                                                   ...   \n",
       "111930              _:nc344c15bf8b1413c8d0015e72ea473a0b1   \n",
       "111931              _:nbbac1cd7013a40b69fd09a15a979ccf9b1   \n",
       "111932  <http://nvasc.org/cd26af13-47f0-4df4-8216-c629...   \n",
       "111933              _:ndc3844cacda3493496853bdf6c2ea8f9b1   \n",
       "111934  <http://nvasc.org/f8ddd9c8-dcc5-45c8-939c-72eb...   \n",
       "\n",
       "                                                        r  \\\n",
       "0            <http://sphn.org/hasSubjectPseudoIdentifier>   \n",
       "1       <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>   \n",
       "2                               <http://sphn.org/hasCode>   \n",
       "3                               <http://sphn.org/hasCode>   \n",
       "4            <http://www.w3.org/2000/01/rdf-schema#label>   \n",
       "...                                                   ...   \n",
       "111930                         <http://sphn.org/hasValue>   \n",
       "111931  <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>   \n",
       "111932  <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>   \n",
       "111933  <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>   \n",
       "111934  <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>   \n",
       "\n",
       "                                                       d  \n",
       "0                   <http://nvasc.org/synth_patient_297>  \n",
       "1                            <http://sphn.org/Diagnosis>  \n",
       "2                        <http://nvasc.org/code_ttt_1.0>  \n",
       "3                        <http://nvasc.org/code_hct_0.0>  \n",
       "4       entry^^<http://www.w3.org/2001/XMLSchema#string>  \n",
       "...                                                  ...  \n",
       "111930                                                49  \n",
       "111931                        <http://sphn.org/Quantity>  \n",
       "111932                       <http://sphn.org/Diagnosis>  \n",
       "111933                        <http://sphn.org/Quantity>  \n",
       "111934                       <http://sphn.org/Diagnosis>  \n",
       "\n",
       "[111935 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_patients = 1000\n",
    "df = pd.read_csv(f\"../Data Generation/sphn_transductive_{num_patients}_0.nt\", sep=\" \", header=None)\n",
    "df.drop(columns=df.columns[-1], axis=1, inplace=True)\n",
    "df.columns=['s', 'r', 'd']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove outcomes from KG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome = df['d'].str.contains('outcome_0.0|outcome_1.0|outcome_2.0')\n",
    "node_df = df[~outcome]\n",
    "node_df = node_df.reset_index(drop=True)\n",
    "outcome = node_df['s'].str.contains('outcome_0.0|outcome_1.0|outcome_2.0')\n",
    "node_df = node_df[~outcome]\n",
    "node_df = node_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_to_id = {k: v for v, k in enumerate(set(node_df['s']).union(set(node_df['d'])), start=0)}\n",
    "rel_to_id = {k: v for v, k in enumerate(set(node_df['r']), start=0)}\n",
    "\n",
    "patients = [f\"<http://nvasc.org/synth_patient_{i}>\" for i in range(num_patients)]\n",
    "patient_id = []\n",
    "for patient in patients:\n",
    "    patient_id.append(ent_to_id[patient])\n",
    "\n",
    "num_nodes = max(ent_to_id.values()) + 1\n",
    "num_rels = max(rel_to_id.values()) + 1\n",
    "\n",
    "events = node_df.copy()\n",
    "events[\"s\"] = node_df.s.map(ent_to_id)\n",
    "events[\"d\"] = node_df.d.map(ent_to_id)\n",
    "events[\"r\"] = node_df.r.map(rel_to_id)\n",
    "\n",
    "ent_to_id = pd.DataFrame.from_dict(ent_to_id, orient='index')\n",
    "rel_to_id = pd.DataFrame.from_dict(rel_to_id, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save events, entities and relations to 'processed_data' folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'processed_data'\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "events.to_csv(f'{path}/sphn_events_{num_patients}_noOutcome.tsv', sep='\\t', index=False, header=None)\n",
    "ent_to_id.to_csv(f'{path}/sphn_entities_{num_patients}_noOutcome.tsv', sep='\\t', header=None)\n",
    "rel_to_id.to_csv(f'{path}/sphn_relations_{num_patients}_noOutcome.tsv', sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TriplesFactory(num_entities=33563, num_relations=20, create_inverse_triples=True, num_triples=110935, path=\"/home/baical77/projects/neurovasc/notebooks/Graphs/processed_data/sphn_events_1000_noOutcome.tsv\")\n"
     ]
    }
   ],
   "source": [
    "path = 'processed_data'\n",
    "tf = TriplesFactory.from_path(f'{path}/sphn_events_{num_patients}_noOutcome.tsv', create_inverse_triples=True)\n",
    "print(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No random seed is specified. Setting to 3610880659.\n",
      "INFO:pykeen.triples.triples_factory:Creating inverse triples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f1cb85ec7b64a0ab40d41a9fae36ef1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epochs on cuda:0:   0%|          | 0/10 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pykeen.triples.triples_factory:Creating inverse triples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b55d9e070ae4d0992ca623e5bd3d1cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating on cuda:0:   0%|          | 0.00/111k [00:00<?, ?triple/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pykeen.evaluation.evaluator:Evaluation took 272.43s seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = 'TransE'\n",
    "embedding_dim = 32\n",
    "epochs = 10\n",
    "\n",
    "result = pipeline(\n",
    "    training=tf,\n",
    "    testing=tf,\n",
    "    model=model_name,\n",
    "    model_kwargs=dict(\n",
    "        embedding_dim=embedding_dim,\n",
    "        loss=\"softPointwiseHingeLoss\",\n",
    "    ),\n",
    "    training_kwargs=dict(\n",
    "        num_epochs=epochs,\n",
    "        # label_smoothing=0.1,\n",
    "        use_tqdm_batch=False,\n",
    "    ),\n",
    "    optimizer_kwargs=dict(\n",
    "        lr=0.01,\n",
    "        weight_decay=1e-5,\n",
    "    ),\n",
    "    training_loop='sLCWA',\n",
    "    negative_sampler='basic',\n",
    "    device='gpu',\n",
    "    use_tqdm=True,\n",
    ")\n",
    "\n",
    "#plot loss\n",
    "loss_plot = result.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient embedding size: torch.Size([1000, 32])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "entity = pd.read_csv(f'processed_data/sphn_entities_{num_patients}_noOutcome.tsv', sep='\\t', index_col=0, header=None)\n",
    "entity = entity.to_dict()[1]\n",
    "patient_id = []\n",
    "for i in range(num_patients):\n",
    "    idx = f'<http://nvasc.org/synth_patient_{i}>'\n",
    "    patient_id.append(entity[idx])\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = result.model\n",
    "entity_embedding = model.entity_representations[0](indices=None).detach().cpu()\n",
    "patient_embedding = entity_embedding[patient_id]\n",
    "print(f'Patient embedding size: {patient_embedding.shape}')\n",
    "\n",
    "patient_embedding = patient_embedding.to(device)\n",
    "y = joblib.load(f'../Data Generation/outcomes_{num_patients}_0.joblib')\n",
    "y = torch.Tensor(y).long().to(device)\n",
    "\n",
    "train_x, train_y = patient_embedding[:int(num_patients*0.8)], y[:int(num_patients*0.8)]\n",
    "val_x, val_y = patient_embedding[int(num_patients*0.8):int(num_patients*0.9)], y[int(num_patients*0.8):int(num_patients*0.9)]\n",
    "test_x, test_y = patient_embedding[int(num_patients*0.9):], y[int(num_patients*0.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Loss: 1.0723, | Train: 0.4338, Val: 0.4500 Test: 0.3700\n",
      "Epoch: 02, Loss: 1.0481, | Train: 0.4338, Val: 0.4500 Test: 0.3700\n",
      "Epoch: 03, Loss: 1.0248, | Train: 0.4338, Val: 0.4500 Test: 0.3700\n",
      "Epoch: 04, Loss: 1.0026, | Train: 0.4363, Val: 0.4500 Test: 0.3700\n",
      "Epoch: 05, Loss: 0.9842, | Train: 0.4387, Val: 0.4500 Test: 0.3700\n",
      "Epoch: 06, Loss: 0.9770, | Train: 0.4950, Val: 0.5500 Test: 0.4100\n",
      "Epoch: 07, Loss: 0.9737, | Train: 0.4637, Val: 0.4900 Test: 0.4200\n",
      "Epoch: 08, Loss: 0.9795, | Train: 0.4550, Val: 0.4700 Test: 0.4400\n",
      "Epoch: 09, Loss: 0.9801, | Train: 0.4637, Val: 0.4700 Test: 0.4700\n",
      "Epoch: 10, Loss: 0.9843, | Train: 0.4612, Val: 0.4700 Test: 0.4600\n",
      "Epoch: 11, Loss: 0.9769, | Train: 0.4612, Val: 0.4700 Test: 0.4400\n",
      "Epoch: 12, Loss: 0.9689, | Train: 0.4625, Val: 0.5100 Test: 0.4300\n",
      "Epoch: 13, Loss: 0.9668, | Train: 0.4762, Val: 0.5100 Test: 0.3800\n",
      "Epoch: 14, Loss: 0.9652, | Train: 0.4612, Val: 0.5000 Test: 0.3500\n",
      "Epoch: 15, Loss: 0.9672, | Train: 0.4575, Val: 0.5000 Test: 0.3500\n",
      "Epoch: 16, Loss: 0.9661, | Train: 0.4562, Val: 0.5100 Test: 0.3500\n",
      "Epoch: 17, Loss: 0.9655, | Train: 0.4575, Val: 0.5100 Test: 0.3600\n",
      "Epoch: 18, Loss: 0.9622, | Train: 0.4637, Val: 0.5100 Test: 0.4100\n",
      "Epoch: 19, Loss: 0.9620, | Train: 0.5087, Val: 0.5500 Test: 0.4200\n",
      "Epoch: 20, Loss: 0.9641, | Train: 0.5037, Val: 0.4900 Test: 0.4600\n",
      "Epoch: 21, Loss: 0.9609, | Train: 0.4800, Val: 0.5000 Test: 0.4500\n",
      "Epoch: 22, Loss: 0.9620, | Train: 0.4725, Val: 0.4800 Test: 0.4400\n",
      "Epoch: 23, Loss: 0.9571, | Train: 0.4700, Val: 0.4700 Test: 0.4500\n",
      "Epoch: 24, Loss: 0.9597, | Train: 0.4762, Val: 0.4800 Test: 0.4300\n",
      "Epoch: 25, Loss: 0.9566, | Train: 0.4775, Val: 0.4800 Test: 0.4500\n",
      "Epoch: 26, Loss: 0.9524, | Train: 0.4812, Val: 0.4700 Test: 0.4500\n",
      "Epoch: 27, Loss: 0.9551, | Train: 0.4900, Val: 0.4800 Test: 0.4500\n",
      "Epoch: 28, Loss: 0.9535, | Train: 0.4862, Val: 0.4800 Test: 0.4500\n",
      "Epoch: 29, Loss: 0.9532, | Train: 0.4912, Val: 0.4700 Test: 0.4700\n",
      "Epoch: 30, Loss: 0.9541, | Train: 0.4975, Val: 0.4600 Test: 0.4600\n",
      "Epoch: 31, Loss: 0.9455, | Train: 0.4937, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 32, Loss: 0.9506, | Train: 0.5112, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 33, Loss: 0.9465, | Train: 0.5263, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 34, Loss: 0.9464, | Train: 0.5375, Val: 0.4600 Test: 0.4900\n",
      "Epoch: 35, Loss: 0.9416, | Train: 0.5425, Val: 0.4600 Test: 0.4800\n",
      "Epoch: 36, Loss: 0.9434, | Train: 0.5512, Val: 0.4600 Test: 0.4900\n",
      "Epoch: 37, Loss: 0.9417, | Train: 0.5437, Val: 0.4500 Test: 0.4600\n",
      "Epoch: 38, Loss: 0.9349, | Train: 0.5562, Val: 0.4400 Test: 0.4600\n",
      "Epoch: 39, Loss: 0.9387, | Train: 0.5375, Val: 0.4300 Test: 0.4900\n",
      "Epoch: 40, Loss: 0.9294, | Train: 0.5350, Val: 0.4300 Test: 0.4900\n",
      "Epoch: 41, Loss: 0.9382, | Train: 0.5525, Val: 0.4400 Test: 0.4400\n",
      "Epoch: 42, Loss: 0.9280, | Train: 0.5562, Val: 0.4700 Test: 0.4700\n",
      "Epoch: 43, Loss: 0.9244, | Train: 0.5537, Val: 0.4500 Test: 0.4600\n",
      "Epoch: 44, Loss: 0.9267, | Train: 0.5600, Val: 0.4500 Test: 0.4600\n",
      "Epoch: 45, Loss: 0.9184, | Train: 0.5575, Val: 0.4400 Test: 0.4700\n",
      "Epoch: 46, Loss: 0.9165, | Train: 0.5562, Val: 0.4700 Test: 0.4800\n",
      "Epoch: 47, Loss: 0.9171, | Train: 0.5550, Val: 0.4400 Test: 0.4500\n",
      "Epoch: 48, Loss: 0.9155, | Train: 0.5600, Val: 0.4700 Test: 0.4400\n",
      "Epoch: 49, Loss: 0.9162, | Train: 0.5587, Val: 0.4700 Test: 0.4400\n",
      "Epoch: 50, Loss: 0.9083, | Train: 0.5637, Val: 0.4500 Test: 0.4500\n",
      "Epoch: 51, Loss: 0.8991, | Train: 0.5687, Val: 0.4400 Test: 0.4700\n",
      "Epoch: 52, Loss: 0.9108, | Train: 0.5712, Val: 0.4300 Test: 0.4500\n",
      "Epoch: 53, Loss: 0.9039, | Train: 0.5788, Val: 0.4500 Test: 0.4400\n",
      "Epoch: 54, Loss: 0.8992, | Train: 0.5763, Val: 0.4600 Test: 0.4500\n",
      "Epoch: 55, Loss: 0.8958, | Train: 0.5763, Val: 0.4700 Test: 0.4400\n",
      "Epoch: 56, Loss: 0.8974, | Train: 0.5750, Val: 0.4700 Test: 0.4500\n",
      "Epoch: 57, Loss: 0.8876, | Train: 0.5725, Val: 0.4600 Test: 0.4500\n",
      "Epoch: 58, Loss: 0.8882, | Train: 0.5737, Val: 0.4800 Test: 0.4800\n",
      "Epoch: 59, Loss: 0.8923, | Train: 0.5850, Val: 0.4700 Test: 0.4600\n",
      "Epoch: 60, Loss: 0.8814, | Train: 0.5938, Val: 0.4700 Test: 0.4600\n",
      "Epoch: 61, Loss: 0.8875, | Train: 0.5863, Val: 0.4500 Test: 0.4200\n",
      "Epoch: 62, Loss: 0.8874, | Train: 0.5838, Val: 0.4600 Test: 0.4200\n",
      "Epoch: 63, Loss: 0.8844, | Train: 0.5888, Val: 0.4600 Test: 0.4500\n",
      "Epoch: 64, Loss: 0.8790, | Train: 0.5925, Val: 0.4800 Test: 0.4600\n",
      "Epoch: 65, Loss: 0.8826, | Train: 0.5975, Val: 0.4900 Test: 0.4800\n",
      "Epoch: 66, Loss: 0.8710, | Train: 0.5938, Val: 0.4800 Test: 0.4600\n",
      "Epoch: 67, Loss: 0.8630, | Train: 0.5962, Val: 0.4800 Test: 0.4400\n",
      "Epoch: 68, Loss: 0.8636, | Train: 0.6062, Val: 0.4800 Test: 0.4400\n",
      "Epoch: 69, Loss: 0.8661, | Train: 0.6100, Val: 0.5100 Test: 0.4600\n",
      "Epoch: 70, Loss: 0.8597, | Train: 0.6162, Val: 0.5100 Test: 0.4400\n",
      "Epoch: 71, Loss: 0.8531, | Train: 0.6087, Val: 0.5000 Test: 0.4500\n",
      "Epoch: 72, Loss: 0.8559, | Train: 0.6137, Val: 0.4800 Test: 0.4600\n",
      "Epoch: 73, Loss: 0.8493, | Train: 0.6162, Val: 0.5000 Test: 0.4500\n",
      "Epoch: 74, Loss: 0.8524, | Train: 0.6237, Val: 0.4800 Test: 0.4700\n",
      "Epoch: 75, Loss: 0.8441, | Train: 0.6200, Val: 0.4900 Test: 0.4800\n",
      "Epoch: 76, Loss: 0.8537, | Train: 0.6250, Val: 0.5000 Test: 0.4800\n",
      "Epoch: 77, Loss: 0.8414, | Train: 0.6287, Val: 0.5100 Test: 0.4700\n",
      "Epoch: 78, Loss: 0.8377, | Train: 0.6350, Val: 0.4700 Test: 0.4700\n",
      "Epoch: 79, Loss: 0.8347, | Train: 0.6400, Val: 0.4700 Test: 0.4700\n",
      "Epoch: 80, Loss: 0.8299, | Train: 0.6400, Val: 0.4800 Test: 0.4600\n",
      "Epoch: 81, Loss: 0.8339, | Train: 0.6400, Val: 0.4800 Test: 0.4500\n",
      "Epoch: 82, Loss: 0.8260, | Train: 0.6413, Val: 0.4900 Test: 0.4600\n",
      "Epoch: 83, Loss: 0.8266, | Train: 0.6450, Val: 0.5000 Test: 0.4900\n",
      "Epoch: 84, Loss: 0.8173, | Train: 0.6425, Val: 0.5100 Test: 0.4800\n",
      "Epoch: 85, Loss: 0.8089, | Train: 0.6438, Val: 0.4900 Test: 0.4600\n",
      "Epoch: 86, Loss: 0.8110, | Train: 0.6450, Val: 0.4900 Test: 0.4700\n",
      "Epoch: 87, Loss: 0.8176, | Train: 0.6375, Val: 0.4800 Test: 0.4700\n",
      "Epoch: 88, Loss: 0.8054, | Train: 0.6463, Val: 0.4800 Test: 0.4900\n",
      "Epoch: 89, Loss: 0.8073, | Train: 0.6438, Val: 0.4500 Test: 0.4600\n",
      "Epoch: 90, Loss: 0.8072, | Train: 0.6525, Val: 0.4800 Test: 0.4500\n",
      "Epoch: 91, Loss: 0.8031, | Train: 0.6650, Val: 0.4900 Test: 0.4800\n",
      "Epoch: 92, Loss: 0.8031, | Train: 0.6625, Val: 0.4900 Test: 0.4600\n",
      "Epoch: 93, Loss: 0.7924, | Train: 0.6562, Val: 0.4700 Test: 0.4700\n",
      "Epoch: 94, Loss: 0.7859, | Train: 0.6575, Val: 0.4400 Test: 0.4400\n",
      "Epoch: 95, Loss: 0.7867, | Train: 0.6637, Val: 0.4900 Test: 0.4700\n",
      "Epoch: 96, Loss: 0.7867, | Train: 0.6587, Val: 0.4700 Test: 0.4700\n",
      "Epoch: 97, Loss: 0.7969, | Train: 0.6612, Val: 0.4700 Test: 0.4800\n",
      "Epoch: 98, Loss: 0.7913, | Train: 0.6562, Val: 0.4900 Test: 0.4400\n",
      "Epoch: 99, Loss: 0.8004, | Train: 0.6650, Val: 0.4900 Test: 0.4400\n",
      "Epoch: 100, Loss: 0.7761, | Train: 0.6750, Val: 0.4800 Test: 0.4600\n",
      "Epoch: 101, Loss: 0.7825, | Train: 0.6762, Val: 0.4800 Test: 0.4600\n",
      "Epoch: 102, Loss: 0.7583, | Train: 0.6737, Val: 0.4700 Test: 0.4400\n",
      "Epoch: 103, Loss: 0.7745, | Train: 0.6750, Val: 0.4700 Test: 0.4500\n",
      "Epoch: 104, Loss: 0.7761, | Train: 0.6762, Val: 0.4800 Test: 0.4700\n",
      "Epoch: 105, Loss: 0.7683, | Train: 0.6712, Val: 0.4800 Test: 0.4600\n",
      "Epoch: 106, Loss: 0.7753, | Train: 0.6650, Val: 0.4800 Test: 0.4400\n",
      "Epoch: 107, Loss: 0.7526, | Train: 0.6712, Val: 0.4400 Test: 0.4200\n",
      "Epoch: 108, Loss: 0.7688, | Train: 0.6675, Val: 0.4800 Test: 0.4400\n",
      "Epoch: 109, Loss: 0.7582, | Train: 0.6737, Val: 0.4700 Test: 0.4300\n",
      "Epoch: 110, Loss: 0.7519, | Train: 0.6862, Val: 0.4700 Test: 0.4300\n",
      "Epoch: 111, Loss: 0.7631, | Train: 0.6812, Val: 0.4500 Test: 0.4400\n",
      "Epoch: 112, Loss: 0.7474, | Train: 0.6825, Val: 0.4600 Test: 0.4300\n",
      "Epoch: 113, Loss: 0.7398, | Train: 0.6887, Val: 0.4800 Test: 0.4200\n",
      "Epoch: 114, Loss: 0.7505, | Train: 0.6887, Val: 0.4700 Test: 0.4200\n",
      "Epoch: 115, Loss: 0.7439, | Train: 0.6812, Val: 0.4800 Test: 0.4300\n",
      "Epoch: 116, Loss: 0.7446, | Train: 0.6825, Val: 0.4800 Test: 0.4300\n",
      "Epoch: 117, Loss: 0.7534, | Train: 0.6862, Val: 0.4800 Test: 0.4400\n",
      "Epoch: 118, Loss: 0.7411, | Train: 0.6937, Val: 0.4700 Test: 0.4300\n",
      "Epoch: 119, Loss: 0.7326, | Train: 0.6987, Val: 0.4600 Test: 0.4300\n",
      "Epoch: 120, Loss: 0.7308, | Train: 0.6950, Val: 0.4700 Test: 0.4300\n",
      "Epoch: 121, Loss: 0.7321, | Train: 0.7025, Val: 0.4600 Test: 0.4100\n",
      "Epoch: 122, Loss: 0.7215, | Train: 0.6987, Val: 0.4700 Test: 0.4200\n",
      "Epoch: 123, Loss: 0.7209, | Train: 0.7175, Val: 0.4800 Test: 0.4200\n",
      "Epoch: 124, Loss: 0.7369, | Train: 0.7200, Val: 0.4600 Test: 0.4100\n",
      "Epoch: 125, Loss: 0.7212, | Train: 0.7188, Val: 0.4500 Test: 0.4100\n",
      "Epoch: 126, Loss: 0.7093, | Train: 0.7150, Val: 0.4500 Test: 0.4300\n",
      "Epoch: 127, Loss: 0.7148, | Train: 0.7150, Val: 0.4600 Test: 0.4300\n",
      "Epoch: 128, Loss: 0.7030, | Train: 0.7188, Val: 0.4800 Test: 0.4200\n",
      "Epoch: 129, Loss: 0.7171, | Train: 0.7050, Val: 0.4700 Test: 0.3900\n",
      "Epoch: 130, Loss: 0.7037, | Train: 0.7100, Val: 0.4500 Test: 0.4000\n",
      "Epoch: 131, Loss: 0.7105, | Train: 0.7150, Val: 0.4500 Test: 0.4100\n",
      "Epoch: 132, Loss: 0.7087, | Train: 0.7188, Val: 0.4400 Test: 0.3900\n",
      "Epoch: 133, Loss: 0.6919, | Train: 0.7250, Val: 0.4400 Test: 0.4300\n",
      "Epoch: 134, Loss: 0.6796, | Train: 0.7175, Val: 0.4400 Test: 0.4200\n",
      "Epoch: 135, Loss: 0.6950, | Train: 0.7188, Val: 0.4200 Test: 0.4200\n",
      "Epoch: 136, Loss: 0.6922, | Train: 0.7412, Val: 0.4400 Test: 0.4400\n",
      "Epoch: 137, Loss: 0.6710, | Train: 0.7387, Val: 0.4400 Test: 0.4100\n",
      "Epoch: 138, Loss: 0.7053, | Train: 0.7337, Val: 0.4500 Test: 0.4000\n",
      "Epoch: 139, Loss: 0.6818, | Train: 0.7450, Val: 0.4500 Test: 0.3900\n",
      "Epoch: 140, Loss: 0.6857, | Train: 0.7375, Val: 0.4300 Test: 0.3900\n",
      "Epoch: 141, Loss: 0.6711, | Train: 0.7325, Val: 0.4400 Test: 0.4200\n",
      "Epoch: 142, Loss: 0.6733, | Train: 0.7350, Val: 0.4300 Test: 0.4300\n",
      "Epoch: 143, Loss: 0.6666, | Train: 0.7325, Val: 0.4900 Test: 0.4000\n",
      "Epoch: 144, Loss: 0.6730, | Train: 0.7462, Val: 0.4600 Test: 0.4000\n",
      "Epoch: 145, Loss: 0.6743, | Train: 0.7450, Val: 0.4300 Test: 0.3900\n",
      "Epoch: 146, Loss: 0.6587, | Train: 0.7425, Val: 0.4400 Test: 0.4100\n",
      "Epoch: 147, Loss: 0.6860, | Train: 0.7525, Val: 0.4600 Test: 0.4000\n",
      "Epoch: 148, Loss: 0.6771, | Train: 0.7662, Val: 0.4700 Test: 0.3800\n",
      "Epoch: 149, Loss: 0.6594, | Train: 0.7500, Val: 0.4100 Test: 0.4000\n",
      "Epoch: 150, Loss: 0.6428, | Train: 0.7537, Val: 0.4200 Test: 0.3700\n",
      "Epoch: 151, Loss: 0.6645, | Train: 0.7537, Val: 0.4500 Test: 0.3900\n",
      "Epoch: 152, Loss: 0.6668, | Train: 0.7600, Val: 0.4500 Test: 0.3900\n",
      "Epoch: 153, Loss: 0.6355, | Train: 0.7537, Val: 0.4200 Test: 0.4200\n",
      "Epoch: 154, Loss: 0.6458, | Train: 0.7537, Val: 0.4300 Test: 0.4500\n",
      "Epoch: 155, Loss: 0.6423, | Train: 0.7612, Val: 0.4300 Test: 0.4300\n",
      "Epoch: 156, Loss: 0.6468, | Train: 0.7612, Val: 0.4300 Test: 0.3900\n",
      "Epoch: 157, Loss: 0.6499, | Train: 0.7700, Val: 0.4200 Test: 0.3900\n",
      "Epoch: 158, Loss: 0.6438, | Train: 0.7650, Val: 0.4000 Test: 0.4100\n",
      "Epoch: 159, Loss: 0.6554, | Train: 0.7788, Val: 0.4200 Test: 0.3800\n",
      "Epoch: 160, Loss: 0.6512, | Train: 0.7738, Val: 0.4200 Test: 0.3900\n",
      "Epoch: 161, Loss: 0.6556, | Train: 0.7825, Val: 0.4100 Test: 0.4000\n",
      "Epoch: 162, Loss: 0.6105, | Train: 0.7738, Val: 0.4200 Test: 0.3600\n",
      "Epoch: 163, Loss: 0.6100, | Train: 0.7713, Val: 0.4400 Test: 0.3900\n",
      "Epoch: 164, Loss: 0.6059, | Train: 0.7713, Val: 0.4500 Test: 0.3800\n",
      "Epoch: 165, Loss: 0.6305, | Train: 0.7775, Val: 0.4300 Test: 0.4300\n",
      "Epoch: 166, Loss: 0.6066, | Train: 0.7750, Val: 0.4200 Test: 0.4200\n",
      "Epoch: 167, Loss: 0.6301, | Train: 0.7825, Val: 0.4200 Test: 0.4100\n",
      "Epoch: 168, Loss: 0.6321, | Train: 0.7825, Val: 0.4400 Test: 0.3800\n",
      "Epoch: 169, Loss: 0.6095, | Train: 0.7837, Val: 0.4400 Test: 0.3900\n",
      "Epoch: 170, Loss: 0.6277, | Train: 0.7788, Val: 0.4500 Test: 0.3800\n",
      "Epoch: 171, Loss: 0.6156, | Train: 0.7788, Val: 0.4200 Test: 0.4200\n",
      "Epoch: 172, Loss: 0.6021, | Train: 0.7788, Val: 0.4300 Test: 0.4200\n",
      "Epoch: 173, Loss: 0.6188, | Train: 0.7775, Val: 0.4500 Test: 0.4000\n",
      "Epoch: 174, Loss: 0.6064, | Train: 0.7800, Val: 0.4500 Test: 0.3900\n",
      "Epoch: 175, Loss: 0.6276, | Train: 0.7850, Val: 0.4600 Test: 0.4000\n",
      "Epoch: 176, Loss: 0.5991, | Train: 0.7887, Val: 0.4400 Test: 0.4200\n",
      "Epoch: 177, Loss: 0.6190, | Train: 0.7850, Val: 0.4600 Test: 0.4000\n",
      "Epoch: 178, Loss: 0.6098, | Train: 0.7925, Val: 0.4400 Test: 0.4000\n",
      "Epoch: 179, Loss: 0.6202, | Train: 0.8025, Val: 0.4100 Test: 0.3800\n",
      "Epoch: 180, Loss: 0.6159, | Train: 0.7962, Val: 0.4300 Test: 0.3800\n",
      "Epoch: 181, Loss: 0.5826, | Train: 0.7925, Val: 0.4600 Test: 0.3800\n",
      "Epoch: 182, Loss: 0.5968, | Train: 0.7950, Val: 0.4800 Test: 0.3800\n",
      "Epoch: 183, Loss: 0.5926, | Train: 0.7937, Val: 0.4300 Test: 0.4200\n",
      "Epoch: 184, Loss: 0.5785, | Train: 0.7937, Val: 0.4300 Test: 0.4300\n",
      "Epoch: 185, Loss: 0.5855, | Train: 0.7987, Val: 0.4100 Test: 0.4100\n",
      "Epoch: 186, Loss: 0.5871, | Train: 0.7925, Val: 0.4100 Test: 0.3700\n",
      "Epoch: 187, Loss: 0.5911, | Train: 0.7937, Val: 0.4300 Test: 0.3700\n",
      "Epoch: 188, Loss: 0.6027, | Train: 0.8037, Val: 0.4200 Test: 0.3700\n",
      "Epoch: 189, Loss: 0.5721, | Train: 0.8150, Val: 0.4200 Test: 0.3900\n",
      "Epoch: 190, Loss: 0.5930, | Train: 0.8062, Val: 0.4600 Test: 0.3600\n",
      "Epoch: 191, Loss: 0.5930, | Train: 0.8025, Val: 0.4600 Test: 0.3800\n",
      "Epoch: 192, Loss: 0.5571, | Train: 0.8037, Val: 0.4500 Test: 0.3700\n",
      "Epoch: 193, Loss: 0.5800, | Train: 0.8000, Val: 0.4500 Test: 0.4000\n",
      "Epoch: 194, Loss: 0.5831, | Train: 0.8025, Val: 0.4400 Test: 0.4200\n",
      "Epoch: 195, Loss: 0.5912, | Train: 0.8000, Val: 0.4300 Test: 0.4300\n",
      "Epoch: 196, Loss: 0.5765, | Train: 0.7987, Val: 0.4300 Test: 0.4300\n",
      "Epoch: 197, Loss: 0.5911, | Train: 0.8125, Val: 0.4500 Test: 0.4200\n",
      "Epoch: 198, Loss: 0.5554, | Train: 0.8200, Val: 0.4500 Test: 0.3700\n",
      "Epoch: 199, Loss: 0.5617, | Train: 0.8162, Val: 0.4500 Test: 0.3600\n",
      "Epoch: 200, Loss: 0.5662, | Train: 0.8162, Val: 0.4300 Test: 0.3800\n",
      "Epoch: 201, Loss: 0.5576, | Train: 0.8112, Val: 0.4400 Test: 0.4100\n",
      "Epoch: 202, Loss: 0.5823, | Train: 0.8025, Val: 0.4400 Test: 0.4000\n",
      "Epoch: 203, Loss: 0.5558, | Train: 0.8050, Val: 0.4200 Test: 0.4100\n",
      "Epoch: 204, Loss: 0.5662, | Train: 0.8150, Val: 0.4200 Test: 0.4300\n",
      "Epoch: 205, Loss: 0.5734, | Train: 0.8187, Val: 0.4500 Test: 0.4100\n",
      "Epoch: 206, Loss: 0.5381, | Train: 0.8250, Val: 0.4400 Test: 0.3800\n",
      "Epoch: 207, Loss: 0.5437, | Train: 0.8225, Val: 0.4500 Test: 0.3800\n",
      "Epoch: 208, Loss: 0.5492, | Train: 0.8212, Val: 0.4500 Test: 0.4100\n",
      "Epoch: 209, Loss: 0.5622, | Train: 0.8150, Val: 0.4300 Test: 0.4400\n",
      "Epoch: 210, Loss: 0.5437, | Train: 0.8112, Val: 0.4200 Test: 0.4300\n",
      "Epoch: 211, Loss: 0.5341, | Train: 0.8175, Val: 0.4200 Test: 0.4300\n",
      "Epoch: 212, Loss: 0.5664, | Train: 0.8237, Val: 0.4500 Test: 0.4200\n",
      "Epoch: 213, Loss: 0.5580, | Train: 0.8187, Val: 0.4600 Test: 0.4000\n",
      "Epoch: 214, Loss: 0.5355, | Train: 0.8275, Val: 0.4400 Test: 0.4100\n",
      "Epoch: 215, Loss: 0.5369, | Train: 0.8250, Val: 0.4400 Test: 0.4000\n",
      "Epoch: 216, Loss: 0.5241, | Train: 0.8250, Val: 0.4100 Test: 0.4300\n",
      "Epoch: 217, Loss: 0.5308, | Train: 0.8237, Val: 0.4200 Test: 0.4300\n",
      "Epoch: 218, Loss: 0.5369, | Train: 0.8237, Val: 0.4200 Test: 0.4300\n",
      "Epoch: 219, Loss: 0.5331, | Train: 0.8200, Val: 0.4200 Test: 0.4300\n",
      "Epoch: 220, Loss: 0.5232, | Train: 0.8225, Val: 0.4300 Test: 0.4300\n",
      "Epoch: 221, Loss: 0.5479, | Train: 0.8262, Val: 0.4300 Test: 0.4400\n",
      "Epoch: 222, Loss: 0.5378, | Train: 0.8312, Val: 0.4200 Test: 0.4300\n",
      "Epoch: 223, Loss: 0.5324, | Train: 0.8262, Val: 0.4100 Test: 0.4600\n",
      "Epoch: 224, Loss: 0.5352, | Train: 0.8312, Val: 0.4400 Test: 0.4300\n",
      "Epoch: 225, Loss: 0.5257, | Train: 0.8375, Val: 0.4300 Test: 0.4000\n",
      "Epoch: 226, Loss: 0.4982, | Train: 0.8150, Val: 0.4200 Test: 0.4000\n",
      "Epoch: 227, Loss: 0.5403, | Train: 0.8312, Val: 0.4300 Test: 0.3800\n",
      "Epoch: 228, Loss: 0.5033, | Train: 0.8338, Val: 0.4500 Test: 0.4200\n",
      "Epoch: 229, Loss: 0.5217, | Train: 0.8375, Val: 0.4400 Test: 0.4500\n",
      "Epoch: 230, Loss: 0.5103, | Train: 0.8312, Val: 0.4400 Test: 0.4200\n",
      "Epoch: 231, Loss: 0.5344, | Train: 0.8350, Val: 0.4300 Test: 0.4200\n",
      "Epoch: 232, Loss: 0.5148, | Train: 0.8388, Val: 0.4500 Test: 0.4400\n",
      "Epoch: 233, Loss: 0.5147, | Train: 0.8350, Val: 0.4600 Test: 0.4200\n",
      "Epoch: 234, Loss: 0.5221, | Train: 0.8338, Val: 0.4400 Test: 0.4200\n",
      "Epoch: 235, Loss: 0.5023, | Train: 0.8375, Val: 0.4200 Test: 0.4000\n",
      "Epoch: 236, Loss: 0.4976, | Train: 0.8338, Val: 0.4200 Test: 0.4200\n",
      "Epoch: 237, Loss: 0.5122, | Train: 0.8400, Val: 0.4200 Test: 0.4300\n",
      "Epoch: 238, Loss: 0.4936, | Train: 0.8400, Val: 0.4400 Test: 0.4500\n",
      "Epoch: 239, Loss: 0.5151, | Train: 0.8388, Val: 0.4200 Test: 0.4600\n",
      "Epoch: 240, Loss: 0.5138, | Train: 0.8275, Val: 0.4400 Test: 0.4400\n",
      "Epoch: 241, Loss: 0.5081, | Train: 0.8338, Val: 0.4600 Test: 0.4400\n",
      "Epoch: 242, Loss: 0.4988, | Train: 0.8462, Val: 0.4500 Test: 0.4500\n",
      "Epoch: 243, Loss: 0.5075, | Train: 0.8512, Val: 0.4700 Test: 0.4300\n",
      "Epoch: 244, Loss: 0.4941, | Train: 0.8562, Val: 0.4500 Test: 0.4400\n",
      "Epoch: 245, Loss: 0.4937, | Train: 0.8500, Val: 0.4400 Test: 0.4200\n",
      "Epoch: 246, Loss: 0.4986, | Train: 0.8487, Val: 0.4300 Test: 0.4100\n",
      "Epoch: 247, Loss: 0.5023, | Train: 0.8487, Val: 0.4300 Test: 0.4300\n",
      "Epoch: 248, Loss: 0.5179, | Train: 0.8550, Val: 0.4300 Test: 0.4200\n",
      "Epoch: 249, Loss: 0.5203, | Train: 0.8550, Val: 0.4300 Test: 0.3900\n",
      "Epoch: 250, Loss: 0.4975, | Train: 0.8500, Val: 0.4500 Test: 0.4100\n",
      "Epoch: 251, Loss: 0.4902, | Train: 0.8562, Val: 0.4600 Test: 0.4300\n",
      "Epoch: 252, Loss: 0.5004, | Train: 0.8562, Val: 0.4600 Test: 0.4300\n",
      "Epoch: 253, Loss: 0.4746, | Train: 0.8512, Val: 0.4500 Test: 0.4300\n",
      "Epoch: 254, Loss: 0.4801, | Train: 0.8600, Val: 0.4200 Test: 0.4300\n",
      "Epoch: 255, Loss: 0.4774, | Train: 0.8550, Val: 0.4200 Test: 0.4200\n",
      "Epoch: 256, Loss: 0.5110, | Train: 0.8550, Val: 0.4100 Test: 0.4400\n",
      "Epoch: 257, Loss: 0.4930, | Train: 0.8475, Val: 0.4200 Test: 0.4500\n",
      "Epoch: 258, Loss: 0.4982, | Train: 0.8525, Val: 0.4100 Test: 0.4500\n",
      "Epoch: 259, Loss: 0.4805, | Train: 0.8637, Val: 0.4300 Test: 0.4300\n",
      "Epoch: 260, Loss: 0.4951, | Train: 0.8562, Val: 0.4400 Test: 0.4200\n",
      "Epoch: 261, Loss: 0.4554, | Train: 0.8637, Val: 0.4200 Test: 0.4300\n",
      "Epoch: 262, Loss: 0.4707, | Train: 0.8600, Val: 0.4200 Test: 0.4300\n",
      "Epoch: 263, Loss: 0.4540, | Train: 0.8625, Val: 0.4200 Test: 0.4300\n",
      "Epoch: 264, Loss: 0.4625, | Train: 0.8587, Val: 0.4300 Test: 0.4300\n",
      "Epoch: 265, Loss: 0.4755, | Train: 0.8625, Val: 0.4300 Test: 0.4400\n",
      "Epoch: 266, Loss: 0.4716, | Train: 0.8725, Val: 0.4100 Test: 0.4400\n",
      "Epoch: 267, Loss: 0.4676, | Train: 0.8637, Val: 0.4200 Test: 0.4300\n",
      "Epoch: 268, Loss: 0.4677, | Train: 0.8725, Val: 0.4300 Test: 0.4200\n",
      "Epoch: 269, Loss: 0.4444, | Train: 0.8725, Val: 0.4600 Test: 0.4200\n",
      "Epoch: 270, Loss: 0.4747, | Train: 0.8650, Val: 0.4600 Test: 0.4100\n",
      "Epoch: 271, Loss: 0.4623, | Train: 0.8700, Val: 0.4600 Test: 0.4000\n",
      "Epoch: 272, Loss: 0.4595, | Train: 0.8712, Val: 0.4400 Test: 0.4400\n",
      "Epoch: 273, Loss: 0.4419, | Train: 0.8737, Val: 0.4300 Test: 0.4600\n",
      "Epoch: 274, Loss: 0.4776, | Train: 0.8625, Val: 0.4100 Test: 0.4500\n",
      "Epoch: 275, Loss: 0.4717, | Train: 0.8612, Val: 0.4300 Test: 0.4500\n",
      "Epoch: 276, Loss: 0.4638, | Train: 0.8725, Val: 0.4200 Test: 0.4200\n",
      "Epoch: 277, Loss: 0.4880, | Train: 0.8750, Val: 0.4300 Test: 0.4200\n",
      "Epoch: 278, Loss: 0.4694, | Train: 0.8737, Val: 0.4300 Test: 0.4200\n",
      "Epoch: 279, Loss: 0.4575, | Train: 0.8687, Val: 0.4300 Test: 0.4100\n",
      "Epoch: 280, Loss: 0.4428, | Train: 0.8775, Val: 0.4100 Test: 0.4300\n",
      "Epoch: 281, Loss: 0.4417, | Train: 0.8712, Val: 0.4600 Test: 0.4400\n",
      "Epoch: 282, Loss: 0.4683, | Train: 0.8800, Val: 0.4100 Test: 0.4600\n",
      "Epoch: 283, Loss: 0.4536, | Train: 0.8687, Val: 0.4300 Test: 0.4400\n",
      "Epoch: 284, Loss: 0.4594, | Train: 0.8675, Val: 0.4400 Test: 0.4100\n",
      "Epoch: 285, Loss: 0.4600, | Train: 0.8787, Val: 0.4200 Test: 0.4100\n",
      "Epoch: 286, Loss: 0.4720, | Train: 0.8775, Val: 0.4200 Test: 0.4200\n",
      "Epoch: 287, Loss: 0.4538, | Train: 0.8712, Val: 0.4300 Test: 0.4300\n",
      "Epoch: 288, Loss: 0.4363, | Train: 0.8687, Val: 0.4300 Test: 0.4500\n",
      "Epoch: 289, Loss: 0.4566, | Train: 0.8700, Val: 0.4500 Test: 0.4500\n",
      "Epoch: 290, Loss: 0.4477, | Train: 0.8712, Val: 0.4600 Test: 0.4300\n",
      "Epoch: 291, Loss: 0.4427, | Train: 0.8700, Val: 0.4400 Test: 0.4100\n",
      "Epoch: 292, Loss: 0.4571, | Train: 0.8750, Val: 0.4200 Test: 0.4200\n",
      "Epoch: 293, Loss: 0.4404, | Train: 0.8812, Val: 0.4200 Test: 0.4200\n",
      "Epoch: 294, Loss: 0.4352, | Train: 0.8787, Val: 0.4200 Test: 0.4200\n",
      "Epoch: 295, Loss: 0.4320, | Train: 0.8787, Val: 0.4200 Test: 0.4100\n",
      "Epoch: 296, Loss: 0.4190, | Train: 0.8750, Val: 0.4400 Test: 0.4100\n",
      "Epoch: 297, Loss: 0.4327, | Train: 0.8687, Val: 0.4500 Test: 0.4200\n",
      "Epoch: 298, Loss: 0.4320, | Train: 0.8725, Val: 0.4700 Test: 0.4100\n",
      "Epoch: 299, Loss: 0.4312, | Train: 0.8825, Val: 0.4300 Test: 0.4300\n",
      "Epoch: 300, Loss: 0.4323, | Train: 0.8850, Val: 0.4400 Test: 0.4100\n",
      "Epoch: 301, Loss: 0.4577, | Train: 0.8837, Val: 0.4300 Test: 0.4200\n",
      "Epoch: 302, Loss: 0.4225, | Train: 0.8812, Val: 0.4400 Test: 0.4000\n",
      "Epoch: 303, Loss: 0.4292, | Train: 0.8775, Val: 0.4400 Test: 0.4000\n",
      "Epoch: 304, Loss: 0.4361, | Train: 0.8850, Val: 0.4100 Test: 0.4100\n",
      "Epoch: 305, Loss: 0.4342, | Train: 0.8825, Val: 0.4200 Test: 0.4200\n",
      "Epoch: 306, Loss: 0.4569, | Train: 0.8675, Val: 0.4300 Test: 0.4200\n",
      "Epoch: 307, Loss: 0.4234, | Train: 0.8687, Val: 0.4300 Test: 0.4300\n",
      "Epoch: 308, Loss: 0.4479, | Train: 0.8762, Val: 0.4300 Test: 0.4200\n",
      "Epoch: 309, Loss: 0.4473, | Train: 0.8862, Val: 0.4300 Test: 0.4100\n",
      "Epoch: 310, Loss: 0.4390, | Train: 0.8862, Val: 0.4400 Test: 0.4200\n",
      "Epoch: 311, Loss: 0.4368, | Train: 0.8912, Val: 0.4200 Test: 0.4100\n",
      "Epoch: 312, Loss: 0.4441, | Train: 0.8963, Val: 0.4200 Test: 0.4000\n",
      "Epoch: 313, Loss: 0.4352, | Train: 0.8837, Val: 0.4200 Test: 0.4400\n",
      "Epoch: 314, Loss: 0.3930, | Train: 0.8837, Val: 0.4400 Test: 0.4300\n",
      "Epoch: 315, Loss: 0.4171, | Train: 0.8700, Val: 0.4300 Test: 0.4300\n",
      "Epoch: 316, Loss: 0.4237, | Train: 0.8762, Val: 0.4300 Test: 0.4200\n",
      "Epoch: 317, Loss: 0.3958, | Train: 0.8862, Val: 0.4500 Test: 0.4200\n",
      "Epoch: 318, Loss: 0.3947, | Train: 0.8925, Val: 0.4300 Test: 0.3900\n",
      "Epoch: 319, Loss: 0.4207, | Train: 0.8912, Val: 0.4200 Test: 0.4100\n",
      "Epoch: 320, Loss: 0.4257, | Train: 0.8875, Val: 0.4200 Test: 0.4200\n",
      "Epoch: 321, Loss: 0.4201, | Train: 0.8975, Val: 0.4400 Test: 0.4000\n",
      "Epoch: 322, Loss: 0.4336, | Train: 0.8950, Val: 0.4400 Test: 0.4300\n",
      "Epoch: 323, Loss: 0.4183, | Train: 0.8887, Val: 0.4400 Test: 0.4200\n",
      "Epoch: 324, Loss: 0.3952, | Train: 0.8850, Val: 0.4500 Test: 0.4300\n",
      "Epoch: 325, Loss: 0.4105, | Train: 0.8912, Val: 0.4400 Test: 0.4300\n",
      "Epoch: 326, Loss: 0.4133, | Train: 0.8912, Val: 0.4100 Test: 0.4500\n",
      "Epoch: 327, Loss: 0.4464, | Train: 0.9013, Val: 0.4200 Test: 0.4500\n",
      "Epoch: 328, Loss: 0.3993, | Train: 0.8963, Val: 0.4100 Test: 0.4500\n",
      "Epoch: 329, Loss: 0.3974, | Train: 0.8925, Val: 0.4100 Test: 0.4400\n",
      "Epoch: 330, Loss: 0.4520, | Train: 0.8963, Val: 0.4300 Test: 0.4200\n",
      "Epoch: 331, Loss: 0.4071, | Train: 0.9025, Val: 0.4500 Test: 0.4200\n",
      "Epoch: 332, Loss: 0.4009, | Train: 0.8925, Val: 0.4400 Test: 0.4100\n",
      "Epoch: 333, Loss: 0.4150, | Train: 0.8787, Val: 0.4500 Test: 0.4000\n",
      "Epoch: 334, Loss: 0.4099, | Train: 0.8787, Val: 0.4500 Test: 0.4200\n",
      "Epoch: 335, Loss: 0.4200, | Train: 0.8912, Val: 0.4500 Test: 0.4200\n",
      "Epoch: 336, Loss: 0.4353, | Train: 0.9000, Val: 0.4300 Test: 0.4200\n",
      "Epoch: 337, Loss: 0.3949, | Train: 0.8950, Val: 0.4400 Test: 0.4500\n",
      "Epoch: 338, Loss: 0.4286, | Train: 0.8937, Val: 0.4300 Test: 0.4300\n",
      "Epoch: 339, Loss: 0.3970, | Train: 0.8850, Val: 0.4500 Test: 0.4200\n",
      "Epoch: 340, Loss: 0.4252, | Train: 0.8862, Val: 0.4600 Test: 0.4100\n",
      "Epoch: 341, Loss: 0.4136, | Train: 0.9000, Val: 0.4500 Test: 0.4200\n",
      "Epoch: 342, Loss: 0.3919, | Train: 0.9000, Val: 0.4500 Test: 0.4100\n",
      "Epoch: 343, Loss: 0.3872, | Train: 0.9038, Val: 0.4500 Test: 0.4300\n",
      "Epoch: 344, Loss: 0.4223, | Train: 0.8950, Val: 0.4500 Test: 0.4200\n",
      "Epoch: 345, Loss: 0.4022, | Train: 0.8963, Val: 0.4500 Test: 0.4300\n",
      "Epoch: 346, Loss: 0.3808, | Train: 0.9025, Val: 0.4300 Test: 0.4200\n",
      "Epoch: 347, Loss: 0.4066, | Train: 0.9050, Val: 0.4500 Test: 0.4200\n",
      "Epoch: 348, Loss: 0.4263, | Train: 0.9112, Val: 0.4600 Test: 0.4100\n",
      "Epoch: 349, Loss: 0.4073, | Train: 0.9100, Val: 0.4400 Test: 0.4300\n",
      "Epoch: 350, Loss: 0.3965, | Train: 0.8988, Val: 0.4500 Test: 0.4200\n",
      "Epoch: 351, Loss: 0.3756, | Train: 0.8988, Val: 0.4600 Test: 0.4500\n",
      "Epoch: 352, Loss: 0.3848, | Train: 0.9025, Val: 0.4500 Test: 0.4400\n",
      "Epoch: 353, Loss: 0.4019, | Train: 0.9075, Val: 0.4600 Test: 0.4400\n",
      "Epoch: 354, Loss: 0.3942, | Train: 0.9050, Val: 0.4600 Test: 0.4200\n",
      "Epoch: 355, Loss: 0.3635, | Train: 0.9013, Val: 0.4500 Test: 0.4400\n",
      "Epoch: 356, Loss: 0.4169, | Train: 0.8988, Val: 0.4400 Test: 0.4000\n",
      "Epoch: 357, Loss: 0.3759, | Train: 0.9000, Val: 0.4500 Test: 0.3900\n",
      "Epoch: 358, Loss: 0.3916, | Train: 0.9087, Val: 0.4300 Test: 0.4000\n",
      "Epoch: 359, Loss: 0.3650, | Train: 0.9100, Val: 0.4500 Test: 0.3900\n",
      "Epoch: 360, Loss: 0.4017, | Train: 0.9062, Val: 0.4600 Test: 0.3900\n",
      "Epoch: 361, Loss: 0.3878, | Train: 0.9087, Val: 0.4500 Test: 0.4000\n",
      "Epoch: 362, Loss: 0.3952, | Train: 0.9100, Val: 0.4500 Test: 0.4100\n",
      "Epoch: 363, Loss: 0.4190, | Train: 0.9038, Val: 0.4500 Test: 0.3800\n",
      "Epoch: 364, Loss: 0.4009, | Train: 0.9100, Val: 0.4400 Test: 0.3700\n",
      "Epoch: 365, Loss: 0.3869, | Train: 0.9150, Val: 0.4400 Test: 0.4000\n",
      "Epoch: 366, Loss: 0.3757, | Train: 0.9062, Val: 0.4400 Test: 0.4000\n",
      "Epoch: 367, Loss: 0.3873, | Train: 0.9075, Val: 0.4600 Test: 0.4300\n",
      "Epoch: 368, Loss: 0.3803, | Train: 0.9075, Val: 0.4600 Test: 0.4400\n",
      "Epoch: 369, Loss: 0.3851, | Train: 0.9087, Val: 0.4500 Test: 0.4100\n",
      "Epoch: 370, Loss: 0.3604, | Train: 0.9038, Val: 0.4500 Test: 0.4200\n",
      "Epoch: 371, Loss: 0.3760, | Train: 0.9125, Val: 0.4400 Test: 0.4400\n",
      "Epoch: 372, Loss: 0.4078, | Train: 0.9087, Val: 0.4600 Test: 0.4100\n",
      "Epoch: 373, Loss: 0.3994, | Train: 0.9062, Val: 0.4700 Test: 0.4100\n",
      "Epoch: 374, Loss: 0.3718, | Train: 0.9025, Val: 0.4700 Test: 0.4000\n",
      "Epoch: 375, Loss: 0.3856, | Train: 0.9112, Val: 0.4400 Test: 0.4200\n",
      "Epoch: 376, Loss: 0.3496, | Train: 0.9125, Val: 0.4300 Test: 0.4200\n",
      "Epoch: 377, Loss: 0.3585, | Train: 0.9137, Val: 0.4600 Test: 0.4000\n",
      "Epoch: 378, Loss: 0.3547, | Train: 0.9112, Val: 0.4600 Test: 0.3900\n",
      "Epoch: 379, Loss: 0.3783, | Train: 0.9087, Val: 0.4500 Test: 0.4100\n",
      "Epoch: 380, Loss: 0.3872, | Train: 0.9087, Val: 0.4300 Test: 0.4200\n",
      "Epoch: 381, Loss: 0.3894, | Train: 0.9038, Val: 0.4500 Test: 0.4000\n",
      "Epoch: 382, Loss: 0.3862, | Train: 0.9087, Val: 0.4500 Test: 0.4100\n",
      "Epoch: 383, Loss: 0.3812, | Train: 0.9162, Val: 0.4600 Test: 0.4100\n",
      "Epoch: 384, Loss: 0.3921, | Train: 0.9237, Val: 0.4700 Test: 0.4200\n",
      "Epoch: 385, Loss: 0.3695, | Train: 0.9225, Val: 0.4600 Test: 0.4200\n",
      "Epoch: 386, Loss: 0.3644, | Train: 0.9075, Val: 0.4700 Test: 0.4400\n",
      "Epoch: 387, Loss: 0.3678, | Train: 0.8963, Val: 0.4700 Test: 0.4000\n",
      "Epoch: 388, Loss: 0.3930, | Train: 0.9050, Val: 0.4500 Test: 0.4000\n",
      "Epoch: 389, Loss: 0.3830, | Train: 0.9137, Val: 0.4200 Test: 0.3900\n",
      "Epoch: 390, Loss: 0.3786, | Train: 0.9087, Val: 0.4400 Test: 0.4100\n",
      "Epoch: 391, Loss: 0.3639, | Train: 0.9050, Val: 0.4800 Test: 0.4200\n",
      "Epoch: 392, Loss: 0.3784, | Train: 0.8975, Val: 0.4800 Test: 0.4200\n",
      "Epoch: 393, Loss: 0.3771, | Train: 0.9000, Val: 0.4700 Test: 0.4200\n",
      "Epoch: 394, Loss: 0.3694, | Train: 0.9050, Val: 0.4600 Test: 0.4300\n",
      "Epoch: 395, Loss: 0.3798, | Train: 0.9137, Val: 0.4800 Test: 0.4200\n",
      "Epoch: 396, Loss: 0.3639, | Train: 0.9137, Val: 0.4800 Test: 0.3900\n",
      "Epoch: 397, Loss: 0.3568, | Train: 0.9062, Val: 0.4800 Test: 0.4000\n",
      "Epoch: 398, Loss: 0.3765, | Train: 0.9200, Val: 0.4700 Test: 0.4000\n",
      "Epoch: 399, Loss: 0.3696, | Train: 0.9175, Val: 0.4500 Test: 0.3900\n",
      "Epoch: 400, Loss: 0.3885, | Train: 0.9137, Val: 0.4400 Test: 0.3800\n",
      "Epoch: 401, Loss: 0.3639, | Train: 0.9100, Val: 0.4400 Test: 0.4000\n",
      "Epoch: 402, Loss: 0.3749, | Train: 0.9013, Val: 0.4500 Test: 0.4200\n",
      "Epoch: 403, Loss: 0.3816, | Train: 0.9087, Val: 0.4500 Test: 0.3700\n",
      "Epoch: 404, Loss: 0.3462, | Train: 0.9050, Val: 0.4400 Test: 0.3700\n",
      "Epoch: 405, Loss: 0.3602, | Train: 0.9025, Val: 0.4400 Test: 0.3700\n",
      "Epoch: 406, Loss: 0.3969, | Train: 0.9087, Val: 0.4300 Test: 0.4000\n",
      "Epoch: 407, Loss: 0.3655, | Train: 0.9150, Val: 0.4400 Test: 0.4000\n",
      "Epoch: 408, Loss: 0.3493, | Train: 0.9125, Val: 0.4600 Test: 0.3900\n",
      "Epoch: 409, Loss: 0.3679, | Train: 0.9125, Val: 0.4600 Test: 0.4000\n",
      "Epoch: 410, Loss: 0.3943, | Train: 0.9150, Val: 0.4600 Test: 0.4000\n",
      "Epoch: 411, Loss: 0.3557, | Train: 0.9150, Val: 0.4500 Test: 0.4000\n",
      "Epoch: 412, Loss: 0.3678, | Train: 0.9200, Val: 0.4600 Test: 0.3900\n",
      "Epoch: 413, Loss: 0.3708, | Train: 0.9200, Val: 0.4700 Test: 0.4100\n",
      "Epoch: 414, Loss: 0.3655, | Train: 0.9212, Val: 0.4700 Test: 0.4000\n",
      "Epoch: 415, Loss: 0.3783, | Train: 0.9212, Val: 0.4700 Test: 0.3900\n",
      "Epoch: 416, Loss: 0.3758, | Train: 0.9162, Val: 0.4800 Test: 0.3900\n",
      "Epoch: 417, Loss: 0.3515, | Train: 0.9212, Val: 0.4500 Test: 0.4100\n",
      "Epoch: 418, Loss: 0.3775, | Train: 0.9125, Val: 0.4300 Test: 0.4200\n",
      "Epoch: 419, Loss: 0.3549, | Train: 0.9150, Val: 0.4300 Test: 0.4200\n",
      "Epoch: 420, Loss: 0.3709, | Train: 0.9137, Val: 0.4600 Test: 0.4300\n",
      "Epoch: 421, Loss: 0.3865, | Train: 0.9112, Val: 0.4600 Test: 0.4200\n",
      "Epoch: 422, Loss: 0.3366, | Train: 0.9100, Val: 0.4500 Test: 0.4400\n",
      "Epoch: 423, Loss: 0.3421, | Train: 0.9137, Val: 0.4400 Test: 0.4300\n",
      "Epoch: 424, Loss: 0.3682, | Train: 0.9175, Val: 0.4600 Test: 0.4100\n",
      "Epoch: 425, Loss: 0.3551, | Train: 0.9187, Val: 0.4700 Test: 0.4000\n",
      "Epoch: 426, Loss: 0.3638, | Train: 0.9200, Val: 0.4800 Test: 0.4100\n",
      "Epoch: 427, Loss: 0.3444, | Train: 0.9150, Val: 0.4600 Test: 0.4100\n",
      "Epoch: 428, Loss: 0.3456, | Train: 0.9100, Val: 0.4500 Test: 0.4200\n",
      "Epoch: 429, Loss: 0.3321, | Train: 0.9175, Val: 0.4500 Test: 0.4100\n",
      "Epoch: 430, Loss: 0.3762, | Train: 0.9075, Val: 0.4600 Test: 0.4100\n",
      "Epoch: 431, Loss: 0.3522, | Train: 0.9150, Val: 0.4300 Test: 0.4000\n",
      "Epoch: 432, Loss: 0.3840, | Train: 0.9212, Val: 0.4300 Test: 0.4400\n",
      "Epoch: 433, Loss: 0.3516, | Train: 0.9187, Val: 0.4500 Test: 0.4300\n",
      "Epoch: 434, Loss: 0.3518, | Train: 0.9125, Val: 0.4300 Test: 0.4200\n",
      "Epoch: 435, Loss: 0.3850, | Train: 0.9137, Val: 0.4500 Test: 0.3900\n",
      "Epoch: 436, Loss: 0.3857, | Train: 0.9225, Val: 0.4600 Test: 0.3800\n",
      "Epoch: 437, Loss: 0.3377, | Train: 0.9212, Val: 0.4600 Test: 0.4100\n",
      "Epoch: 438, Loss: 0.3303, | Train: 0.9175, Val: 0.4500 Test: 0.3900\n",
      "Epoch: 439, Loss: 0.3272, | Train: 0.9137, Val: 0.4700 Test: 0.4100\n",
      "Epoch: 440, Loss: 0.3509, | Train: 0.9200, Val: 0.4500 Test: 0.4000\n",
      "Epoch: 441, Loss: 0.3526, | Train: 0.9275, Val: 0.4500 Test: 0.4000\n",
      "Epoch: 442, Loss: 0.3316, | Train: 0.9262, Val: 0.4600 Test: 0.4000\n",
      "Epoch: 443, Loss: 0.3424, | Train: 0.9275, Val: 0.4500 Test: 0.3900\n",
      "Epoch: 444, Loss: 0.3442, | Train: 0.9225, Val: 0.4700 Test: 0.4100\n",
      "Epoch: 445, Loss: 0.3637, | Train: 0.9137, Val: 0.4700 Test: 0.4200\n",
      "Epoch: 446, Loss: 0.3640, | Train: 0.9200, Val: 0.4500 Test: 0.4100\n",
      "Epoch: 447, Loss: 0.3280, | Train: 0.9212, Val: 0.4500 Test: 0.4300\n",
      "Epoch: 448, Loss: 0.3541, | Train: 0.9225, Val: 0.4500 Test: 0.4100\n",
      "Epoch: 449, Loss: 0.3278, | Train: 0.9275, Val: 0.4400 Test: 0.4000\n",
      "Epoch: 450, Loss: 0.3560, | Train: 0.9250, Val: 0.4400 Test: 0.3800\n",
      "Epoch: 451, Loss: 0.3324, | Train: 0.9150, Val: 0.4400 Test: 0.4000\n",
      "Epoch: 452, Loss: 0.3597, | Train: 0.9175, Val: 0.4300 Test: 0.4200\n",
      "Epoch: 453, Loss: 0.3300, | Train: 0.9250, Val: 0.4300 Test: 0.4300\n",
      "Epoch: 454, Loss: 0.3439, | Train: 0.9287, Val: 0.4200 Test: 0.4300\n",
      "Epoch: 455, Loss: 0.3615, | Train: 0.9175, Val: 0.4300 Test: 0.4200\n",
      "Epoch: 456, Loss: 0.3342, | Train: 0.9212, Val: 0.4400 Test: 0.4100\n",
      "Epoch: 457, Loss: 0.3336, | Train: 0.9225, Val: 0.4700 Test: 0.4100\n",
      "Epoch: 458, Loss: 0.3328, | Train: 0.9275, Val: 0.4600 Test: 0.4100\n",
      "Epoch: 459, Loss: 0.3427, | Train: 0.9262, Val: 0.4600 Test: 0.4100\n",
      "Epoch: 460, Loss: 0.3382, | Train: 0.9237, Val: 0.4700 Test: 0.4200\n",
      "Epoch: 461, Loss: 0.3241, | Train: 0.9162, Val: 0.4600 Test: 0.4400\n",
      "Epoch: 462, Loss: 0.3715, | Train: 0.9250, Val: 0.4700 Test: 0.4200\n",
      "Epoch: 463, Loss: 0.3837, | Train: 0.9250, Val: 0.4600 Test: 0.4200\n",
      "Epoch: 464, Loss: 0.3613, | Train: 0.9275, Val: 0.4700 Test: 0.4000\n",
      "Epoch: 465, Loss: 0.3398, | Train: 0.9262, Val: 0.4900 Test: 0.4200\n",
      "Epoch: 466, Loss: 0.3471, | Train: 0.9250, Val: 0.4700 Test: 0.4200\n",
      "Epoch: 467, Loss: 0.3332, | Train: 0.9200, Val: 0.4800 Test: 0.4000\n",
      "Epoch: 468, Loss: 0.3398, | Train: 0.9262, Val: 0.4600 Test: 0.4200\n",
      "Epoch: 469, Loss: 0.3133, | Train: 0.9287, Val: 0.4600 Test: 0.3900\n",
      "Epoch: 470, Loss: 0.3441, | Train: 0.9275, Val: 0.4800 Test: 0.4000\n",
      "Epoch: 471, Loss: 0.3397, | Train: 0.9137, Val: 0.4900 Test: 0.4200\n",
      "Epoch: 472, Loss: 0.3268, | Train: 0.9112, Val: 0.4800 Test: 0.4300\n",
      "Epoch: 473, Loss: 0.3480, | Train: 0.9200, Val: 0.4600 Test: 0.4400\n",
      "Epoch: 474, Loss: 0.3439, | Train: 0.9275, Val: 0.4600 Test: 0.4400\n",
      "Epoch: 475, Loss: 0.3318, | Train: 0.9362, Val: 0.4900 Test: 0.4200\n",
      "Epoch: 476, Loss: 0.3352, | Train: 0.9250, Val: 0.4800 Test: 0.4200\n",
      "Epoch: 477, Loss: 0.3519, | Train: 0.9225, Val: 0.4800 Test: 0.4200\n",
      "Epoch: 478, Loss: 0.3263, | Train: 0.9225, Val: 0.4600 Test: 0.4400\n",
      "Epoch: 479, Loss: 0.3362, | Train: 0.9262, Val: 0.4500 Test: 0.4200\n",
      "Epoch: 480, Loss: 0.3381, | Train: 0.9262, Val: 0.4700 Test: 0.4300\n",
      "Epoch: 481, Loss: 0.3272, | Train: 0.9262, Val: 0.4500 Test: 0.4500\n",
      "Epoch: 482, Loss: 0.3508, | Train: 0.9275, Val: 0.4500 Test: 0.4300\n",
      "Epoch: 483, Loss: 0.3552, | Train: 0.9262, Val: 0.4400 Test: 0.4300\n",
      "Epoch: 484, Loss: 0.3332, | Train: 0.9312, Val: 0.4600 Test: 0.4100\n",
      "Epoch: 485, Loss: 0.3436, | Train: 0.9225, Val: 0.4800 Test: 0.4000\n",
      "Epoch: 486, Loss: 0.3300, | Train: 0.9187, Val: 0.4600 Test: 0.4200\n",
      "Epoch: 487, Loss: 0.3269, | Train: 0.9225, Val: 0.4600 Test: 0.4200\n",
      "Epoch: 488, Loss: 0.3134, | Train: 0.9312, Val: 0.4600 Test: 0.4400\n",
      "Epoch: 489, Loss: 0.3217, | Train: 0.9200, Val: 0.4500 Test: 0.4100\n",
      "Epoch: 490, Loss: 0.3532, | Train: 0.9175, Val: 0.4800 Test: 0.4300\n",
      "Epoch: 491, Loss: 0.3491, | Train: 0.9150, Val: 0.4600 Test: 0.4200\n",
      "Epoch: 492, Loss: 0.3218, | Train: 0.9225, Val: 0.4400 Test: 0.4300\n",
      "Epoch: 493, Loss: 0.3091, | Train: 0.9262, Val: 0.4600 Test: 0.4200\n",
      "Epoch: 494, Loss: 0.3187, | Train: 0.9300, Val: 0.4600 Test: 0.4100\n",
      "Epoch: 495, Loss: 0.3393, | Train: 0.9312, Val: 0.4600 Test: 0.4100\n",
      "Epoch: 496, Loss: 0.3286, | Train: 0.9312, Val: 0.4900 Test: 0.4200\n",
      "Epoch: 497, Loss: 0.3165, | Train: 0.9287, Val: 0.5100 Test: 0.4200\n",
      "Epoch: 498, Loss: 0.3378, | Train: 0.9300, Val: 0.5000 Test: 0.4300\n",
      "Epoch: 499, Loss: 0.3421, | Train: 0.9300, Val: 0.4800 Test: 0.4000\n",
      "Epoch: 500, Loss: 0.3238, | Train: 0.9287, Val: 0.4700 Test: 0.3900\n",
      "Epoch: 501, Loss: 0.3448, | Train: 0.9287, Val: 0.4800 Test: 0.3900\n",
      "Epoch: 502, Loss: 0.3231, | Train: 0.9275, Val: 0.4600 Test: 0.3700\n",
      "Epoch: 503, Loss: 0.3317, | Train: 0.9287, Val: 0.4800 Test: 0.4000\n",
      "Epoch: 504, Loss: 0.3141, | Train: 0.9212, Val: 0.4900 Test: 0.4100\n",
      "Epoch: 505, Loss: 0.3401, | Train: 0.9250, Val: 0.4900 Test: 0.4100\n",
      "Epoch: 506, Loss: 0.3161, | Train: 0.9312, Val: 0.4900 Test: 0.4200\n",
      "Epoch: 507, Loss: 0.3063, | Train: 0.9275, Val: 0.4700 Test: 0.4200\n",
      "Epoch: 508, Loss: 0.3233, | Train: 0.9275, Val: 0.4700 Test: 0.4000\n",
      "Epoch: 509, Loss: 0.3373, | Train: 0.9287, Val: 0.4600 Test: 0.3900\n",
      "Epoch: 510, Loss: 0.3121, | Train: 0.9350, Val: 0.4600 Test: 0.3800\n",
      "Epoch: 511, Loss: 0.3466, | Train: 0.9312, Val: 0.4800 Test: 0.3900\n",
      "Epoch: 512, Loss: 0.3182, | Train: 0.9312, Val: 0.5000 Test: 0.4000\n",
      "Epoch: 513, Loss: 0.3241, | Train: 0.9275, Val: 0.5000 Test: 0.3800\n",
      "Epoch: 514, Loss: 0.3151, | Train: 0.9325, Val: 0.5000 Test: 0.3600\n",
      "Epoch: 515, Loss: 0.3637, | Train: 0.9275, Val: 0.4800 Test: 0.3500\n",
      "Epoch: 516, Loss: 0.3186, | Train: 0.9300, Val: 0.4700 Test: 0.3500\n",
      "Epoch: 517, Loss: 0.3128, | Train: 0.9312, Val: 0.4400 Test: 0.3600\n",
      "Epoch: 518, Loss: 0.3011, | Train: 0.9250, Val: 0.4700 Test: 0.3900\n",
      "Epoch: 519, Loss: 0.3366, | Train: 0.9300, Val: 0.4900 Test: 0.4200\n",
      "Epoch: 520, Loss: 0.3108, | Train: 0.9350, Val: 0.4900 Test: 0.4200\n",
      "Epoch: 521, Loss: 0.3460, | Train: 0.9325, Val: 0.4900 Test: 0.4200\n",
      "Epoch: 522, Loss: 0.3263, | Train: 0.9362, Val: 0.4900 Test: 0.4200\n",
      "Epoch: 523, Loss: 0.2965, | Train: 0.9362, Val: 0.4900 Test: 0.4000\n",
      "Epoch: 524, Loss: 0.3128, | Train: 0.9275, Val: 0.4400 Test: 0.4000\n",
      "Epoch: 525, Loss: 0.3254, | Train: 0.9262, Val: 0.4500 Test: 0.4100\n",
      "Epoch: 526, Loss: 0.3164, | Train: 0.9362, Val: 0.4600 Test: 0.4100\n",
      "Epoch: 527, Loss: 0.3077, | Train: 0.9400, Val: 0.4500 Test: 0.4100\n",
      "Epoch: 528, Loss: 0.3087, | Train: 0.9400, Val: 0.4600 Test: 0.4100\n",
      "Epoch: 529, Loss: 0.3035, | Train: 0.9350, Val: 0.4600 Test: 0.4200\n",
      "Epoch: 530, Loss: 0.3401, | Train: 0.9237, Val: 0.4700 Test: 0.4200\n",
      "Epoch: 531, Loss: 0.3002, | Train: 0.9275, Val: 0.4800 Test: 0.4100\n",
      "Epoch: 532, Loss: 0.3449, | Train: 0.9387, Val: 0.4600 Test: 0.3800\n",
      "Epoch: 533, Loss: 0.3018, | Train: 0.9350, Val: 0.4500 Test: 0.4200\n",
      "Epoch: 534, Loss: 0.3409, | Train: 0.9287, Val: 0.4700 Test: 0.4100\n",
      "Epoch: 535, Loss: 0.3583, | Train: 0.9262, Val: 0.4600 Test: 0.4200\n",
      "Epoch: 536, Loss: 0.3222, | Train: 0.9262, Val: 0.4700 Test: 0.4200\n",
      "Epoch: 537, Loss: 0.3121, | Train: 0.9262, Val: 0.4700 Test: 0.4400\n",
      "Epoch: 538, Loss: 0.3221, | Train: 0.9300, Val: 0.4800 Test: 0.4300\n",
      "Epoch: 539, Loss: 0.3271, | Train: 0.9312, Val: 0.4800 Test: 0.4300\n",
      "Epoch: 540, Loss: 0.3030, | Train: 0.9312, Val: 0.4700 Test: 0.4300\n",
      "Epoch: 541, Loss: 0.3132, | Train: 0.9312, Val: 0.4700 Test: 0.4300\n",
      "Epoch: 542, Loss: 0.2871, | Train: 0.9325, Val: 0.4600 Test: 0.4200\n",
      "Epoch: 543, Loss: 0.3279, | Train: 0.9362, Val: 0.4700 Test: 0.4300\n",
      "Epoch: 544, Loss: 0.2864, | Train: 0.9375, Val: 0.4700 Test: 0.4300\n",
      "Epoch: 545, Loss: 0.3131, | Train: 0.9412, Val: 0.4600 Test: 0.3900\n",
      "Epoch: 546, Loss: 0.3544, | Train: 0.9425, Val: 0.4600 Test: 0.4000\n",
      "Epoch: 547, Loss: 0.2929, | Train: 0.9412, Val: 0.4500 Test: 0.3900\n",
      "Epoch: 548, Loss: 0.3079, | Train: 0.9400, Val: 0.4800 Test: 0.4100\n",
      "Epoch: 549, Loss: 0.3219, | Train: 0.9362, Val: 0.4800 Test: 0.4100\n",
      "Epoch: 550, Loss: 0.3143, | Train: 0.9362, Val: 0.4600 Test: 0.3900\n",
      "Epoch: 551, Loss: 0.2942, | Train: 0.9375, Val: 0.4800 Test: 0.3900\n",
      "Epoch: 552, Loss: 0.3154, | Train: 0.9350, Val: 0.4700 Test: 0.4000\n",
      "Epoch: 553, Loss: 0.2986, | Train: 0.9362, Val: 0.4600 Test: 0.4200\n",
      "Epoch: 554, Loss: 0.2836, | Train: 0.9312, Val: 0.4700 Test: 0.4100\n",
      "Epoch: 555, Loss: 0.3143, | Train: 0.9250, Val: 0.4500 Test: 0.4400\n",
      "Epoch: 556, Loss: 0.2988, | Train: 0.9262, Val: 0.4600 Test: 0.4300\n",
      "Epoch: 557, Loss: 0.2989, | Train: 0.9337, Val: 0.4500 Test: 0.4400\n",
      "Epoch: 558, Loss: 0.3090, | Train: 0.9337, Val: 0.4800 Test: 0.4500\n",
      "Epoch: 559, Loss: 0.3127, | Train: 0.9250, Val: 0.4700 Test: 0.4500\n",
      "Epoch: 560, Loss: 0.3116, | Train: 0.9300, Val: 0.4600 Test: 0.4500\n",
      "Epoch: 561, Loss: 0.3010, | Train: 0.9337, Val: 0.4500 Test: 0.4100\n",
      "Epoch: 562, Loss: 0.2880, | Train: 0.9387, Val: 0.4400 Test: 0.4000\n",
      "Epoch: 563, Loss: 0.3097, | Train: 0.9350, Val: 0.4400 Test: 0.3900\n",
      "Epoch: 564, Loss: 0.3072, | Train: 0.9462, Val: 0.4500 Test: 0.3900\n",
      "Epoch: 565, Loss: 0.3013, | Train: 0.9300, Val: 0.4700 Test: 0.3700\n",
      "Epoch: 566, Loss: 0.3031, | Train: 0.9300, Val: 0.4900 Test: 0.4100\n",
      "Epoch: 567, Loss: 0.2966, | Train: 0.9375, Val: 0.4900 Test: 0.4300\n",
      "Epoch: 568, Loss: 0.2941, | Train: 0.9287, Val: 0.4800 Test: 0.4200\n",
      "Epoch: 569, Loss: 0.2891, | Train: 0.9262, Val: 0.4500 Test: 0.4000\n",
      "Epoch: 570, Loss: 0.3030, | Train: 0.9300, Val: 0.4700 Test: 0.4300\n",
      "Epoch: 571, Loss: 0.3001, | Train: 0.9262, Val: 0.4700 Test: 0.4400\n",
      "Epoch: 572, Loss: 0.2869, | Train: 0.9275, Val: 0.4900 Test: 0.4200\n",
      "Epoch: 573, Loss: 0.3187, | Train: 0.9325, Val: 0.4900 Test: 0.3900\n",
      "Epoch: 574, Loss: 0.2951, | Train: 0.9400, Val: 0.5000 Test: 0.3800\n",
      "Epoch: 575, Loss: 0.3153, | Train: 0.9400, Val: 0.4800 Test: 0.3800\n",
      "Epoch: 576, Loss: 0.2902, | Train: 0.9387, Val: 0.4900 Test: 0.4000\n",
      "Epoch: 577, Loss: 0.3051, | Train: 0.9362, Val: 0.5000 Test: 0.4100\n",
      "Epoch: 578, Loss: 0.2953, | Train: 0.9362, Val: 0.4700 Test: 0.4000\n",
      "Epoch: 579, Loss: 0.3160, | Train: 0.9362, Val: 0.4400 Test: 0.3900\n",
      "Epoch: 580, Loss: 0.2782, | Train: 0.9387, Val: 0.4500 Test: 0.4000\n",
      "Epoch: 581, Loss: 0.2845, | Train: 0.9312, Val: 0.4800 Test: 0.4000\n",
      "Epoch: 582, Loss: 0.3109, | Train: 0.9300, Val: 0.4900 Test: 0.3900\n",
      "Epoch: 583, Loss: 0.2764, | Train: 0.9375, Val: 0.4900 Test: 0.4000\n",
      "Epoch: 584, Loss: 0.2686, | Train: 0.9450, Val: 0.4900 Test: 0.4200\n",
      "Epoch: 585, Loss: 0.3049, | Train: 0.9450, Val: 0.4800 Test: 0.4300\n",
      "Epoch: 586, Loss: 0.2971, | Train: 0.9387, Val: 0.4800 Test: 0.4200\n",
      "Epoch: 587, Loss: 0.2863, | Train: 0.9400, Val: 0.5000 Test: 0.4100\n",
      "Epoch: 588, Loss: 0.3006, | Train: 0.9350, Val: 0.5000 Test: 0.4100\n",
      "Epoch: 589, Loss: 0.3017, | Train: 0.9425, Val: 0.4900 Test: 0.4200\n",
      "Epoch: 590, Loss: 0.3128, | Train: 0.9387, Val: 0.4700 Test: 0.4200\n",
      "Epoch: 591, Loss: 0.2994, | Train: 0.9387, Val: 0.4600 Test: 0.4200\n",
      "Epoch: 592, Loss: 0.2987, | Train: 0.9337, Val: 0.4500 Test: 0.4300\n",
      "Epoch: 593, Loss: 0.3048, | Train: 0.9325, Val: 0.4600 Test: 0.4300\n",
      "Epoch: 594, Loss: 0.3121, | Train: 0.9362, Val: 0.5000 Test: 0.4400\n",
      "Epoch: 595, Loss: 0.3077, | Train: 0.9350, Val: 0.4900 Test: 0.4300\n",
      "Epoch: 596, Loss: 0.2726, | Train: 0.9400, Val: 0.5000 Test: 0.4100\n",
      "Epoch: 597, Loss: 0.2959, | Train: 0.9450, Val: 0.5000 Test: 0.4200\n",
      "Epoch: 598, Loss: 0.3153, | Train: 0.9487, Val: 0.4800 Test: 0.4000\n",
      "Epoch: 599, Loss: 0.2764, | Train: 0.9437, Val: 0.4800 Test: 0.4100\n",
      "Epoch: 600, Loss: 0.3046, | Train: 0.9462, Val: 0.4600 Test: 0.3900\n",
      "Epoch: 601, Loss: 0.2812, | Train: 0.9462, Val: 0.4700 Test: 0.3800\n",
      "Epoch: 602, Loss: 0.3041, | Train: 0.9475, Val: 0.4700 Test: 0.3900\n",
      "Epoch: 603, Loss: 0.2543, | Train: 0.9450, Val: 0.4700 Test: 0.3800\n",
      "Epoch: 604, Loss: 0.2946, | Train: 0.9425, Val: 0.4500 Test: 0.3900\n",
      "Epoch: 605, Loss: 0.3185, | Train: 0.9350, Val: 0.4400 Test: 0.4100\n",
      "Epoch: 606, Loss: 0.2937, | Train: 0.9375, Val: 0.4700 Test: 0.4100\n",
      "Epoch: 607, Loss: 0.2969, | Train: 0.9400, Val: 0.4500 Test: 0.3900\n",
      "Epoch: 608, Loss: 0.2721, | Train: 0.9387, Val: 0.4500 Test: 0.4000\n",
      "Epoch: 609, Loss: 0.3033, | Train: 0.9387, Val: 0.4400 Test: 0.3900\n",
      "Epoch: 610, Loss: 0.3230, | Train: 0.9500, Val: 0.4300 Test: 0.3900\n",
      "Epoch: 611, Loss: 0.3034, | Train: 0.9437, Val: 0.4400 Test: 0.3800\n",
      "Epoch: 612, Loss: 0.2982, | Train: 0.9387, Val: 0.4600 Test: 0.4000\n",
      "Epoch: 613, Loss: 0.2975, | Train: 0.9387, Val: 0.4600 Test: 0.3900\n",
      "Epoch: 614, Loss: 0.2806, | Train: 0.9362, Val: 0.4700 Test: 0.4100\n",
      "Epoch: 615, Loss: 0.3055, | Train: 0.9362, Val: 0.4500 Test: 0.4000\n",
      "Epoch: 616, Loss: 0.2715, | Train: 0.9362, Val: 0.4700 Test: 0.4000\n",
      "Epoch: 617, Loss: 0.3047, | Train: 0.9362, Val: 0.4700 Test: 0.4100\n",
      "Epoch: 618, Loss: 0.2908, | Train: 0.9387, Val: 0.4400 Test: 0.4200\n",
      "Epoch: 619, Loss: 0.3169, | Train: 0.9412, Val: 0.4200 Test: 0.4200\n",
      "Epoch: 620, Loss: 0.3052, | Train: 0.9412, Val: 0.4300 Test: 0.4000\n",
      "Epoch: 621, Loss: 0.2944, | Train: 0.9275, Val: 0.4300 Test: 0.3700\n",
      "Epoch: 622, Loss: 0.2960, | Train: 0.9387, Val: 0.4400 Test: 0.3700\n",
      "Epoch: 623, Loss: 0.3090, | Train: 0.9450, Val: 0.4500 Test: 0.3600\n",
      "Epoch: 624, Loss: 0.2736, | Train: 0.9462, Val: 0.4300 Test: 0.3700\n",
      "Epoch: 625, Loss: 0.2823, | Train: 0.9500, Val: 0.4500 Test: 0.4000\n",
      "Epoch: 626, Loss: 0.2840, | Train: 0.9450, Val: 0.4500 Test: 0.4000\n",
      "Epoch: 627, Loss: 0.2899, | Train: 0.9475, Val: 0.4800 Test: 0.4000\n",
      "Epoch: 628, Loss: 0.2932, | Train: 0.9400, Val: 0.4400 Test: 0.4000\n",
      "Epoch: 629, Loss: 0.2992, | Train: 0.9362, Val: 0.4400 Test: 0.4200\n",
      "Epoch: 630, Loss: 0.2901, | Train: 0.9350, Val: 0.4500 Test: 0.4200\n",
      "Epoch: 631, Loss: 0.2647, | Train: 0.9375, Val: 0.4500 Test: 0.4200\n",
      "Epoch: 632, Loss: 0.2913, | Train: 0.9475, Val: 0.4400 Test: 0.4000\n",
      "Epoch: 633, Loss: 0.2780, | Train: 0.9475, Val: 0.4600 Test: 0.4100\n",
      "Epoch: 634, Loss: 0.2700, | Train: 0.9487, Val: 0.4600 Test: 0.4100\n",
      "Epoch: 635, Loss: 0.2870, | Train: 0.9412, Val: 0.4800 Test: 0.4100\n",
      "Epoch: 636, Loss: 0.2665, | Train: 0.9312, Val: 0.4700 Test: 0.4200\n",
      "Epoch: 637, Loss: 0.2870, | Train: 0.9350, Val: 0.4600 Test: 0.4200\n",
      "Epoch: 638, Loss: 0.2585, | Train: 0.9437, Val: 0.4500 Test: 0.4200\n",
      "Epoch: 639, Loss: 0.2607, | Train: 0.9425, Val: 0.4400 Test: 0.4200\n",
      "Epoch: 640, Loss: 0.2980, | Train: 0.9462, Val: 0.4200 Test: 0.4000\n",
      "Epoch: 641, Loss: 0.2674, | Train: 0.9425, Val: 0.4500 Test: 0.4300\n",
      "Epoch: 642, Loss: 0.2861, | Train: 0.9400, Val: 0.4500 Test: 0.4200\n",
      "Epoch: 643, Loss: 0.2864, | Train: 0.9475, Val: 0.4700 Test: 0.4100\n",
      "Epoch: 644, Loss: 0.2952, | Train: 0.9450, Val: 0.4600 Test: 0.4200\n",
      "Epoch: 645, Loss: 0.2852, | Train: 0.9450, Val: 0.4600 Test: 0.4300\n",
      "Epoch: 646, Loss: 0.3471, | Train: 0.9387, Val: 0.4600 Test: 0.4400\n",
      "Epoch: 647, Loss: 0.2807, | Train: 0.9387, Val: 0.4800 Test: 0.4200\n",
      "Epoch: 648, Loss: 0.3127, | Train: 0.9475, Val: 0.4700 Test: 0.4300\n",
      "Epoch: 649, Loss: 0.2952, | Train: 0.9425, Val: 0.4500 Test: 0.4100\n",
      "Epoch: 650, Loss: 0.2962, | Train: 0.9450, Val: 0.4400 Test: 0.4100\n",
      "Epoch: 651, Loss: 0.2927, | Train: 0.9500, Val: 0.4200 Test: 0.4000\n",
      "Epoch: 652, Loss: 0.2751, | Train: 0.9462, Val: 0.4700 Test: 0.4300\n",
      "Epoch: 653, Loss: 0.2835, | Train: 0.9425, Val: 0.4800 Test: 0.4300\n",
      "Epoch: 654, Loss: 0.2898, | Train: 0.9387, Val: 0.4500 Test: 0.4300\n",
      "Epoch: 655, Loss: 0.3108, | Train: 0.9450, Val: 0.4500 Test: 0.4100\n",
      "Epoch: 656, Loss: 0.2630, | Train: 0.9475, Val: 0.4400 Test: 0.4300\n",
      "Epoch: 657, Loss: 0.2698, | Train: 0.9500, Val: 0.4200 Test: 0.4300\n",
      "Epoch: 658, Loss: 0.2864, | Train: 0.9525, Val: 0.4400 Test: 0.4100\n",
      "Epoch: 659, Loss: 0.2642, | Train: 0.9512, Val: 0.4500 Test: 0.4000\n",
      "Epoch: 660, Loss: 0.2777, | Train: 0.9512, Val: 0.4700 Test: 0.4100\n",
      "Epoch: 661, Loss: 0.2643, | Train: 0.9450, Val: 0.4600 Test: 0.3900\n",
      "Epoch: 662, Loss: 0.2789, | Train: 0.9462, Val: 0.4900 Test: 0.4100\n",
      "Epoch: 663, Loss: 0.2678, | Train: 0.9450, Val: 0.4600 Test: 0.4100\n",
      "Epoch: 664, Loss: 0.2929, | Train: 0.9462, Val: 0.4800 Test: 0.4100\n",
      "Epoch: 665, Loss: 0.2753, | Train: 0.9475, Val: 0.4900 Test: 0.4200\n",
      "Epoch: 666, Loss: 0.2874, | Train: 0.9462, Val: 0.4700 Test: 0.4300\n",
      "Epoch: 667, Loss: 0.2667, | Train: 0.9425, Val: 0.4800 Test: 0.4300\n",
      "Epoch: 668, Loss: 0.2761, | Train: 0.9450, Val: 0.5000 Test: 0.4100\n",
      "Epoch: 669, Loss: 0.2619, | Train: 0.9400, Val: 0.4900 Test: 0.4000\n",
      "Epoch: 670, Loss: 0.2983, | Train: 0.9425, Val: 0.4600 Test: 0.3900\n",
      "Epoch: 671, Loss: 0.2787, | Train: 0.9487, Val: 0.4700 Test: 0.4000\n",
      "Epoch: 672, Loss: 0.2676, | Train: 0.9500, Val: 0.4500 Test: 0.3900\n",
      "Epoch: 673, Loss: 0.2966, | Train: 0.9475, Val: 0.4600 Test: 0.3900\n",
      "Epoch: 674, Loss: 0.2857, | Train: 0.9437, Val: 0.4300 Test: 0.4100\n",
      "Epoch: 675, Loss: 0.2833, | Train: 0.9437, Val: 0.4500 Test: 0.4100\n",
      "Epoch: 676, Loss: 0.2519, | Train: 0.9487, Val: 0.4500 Test: 0.4100\n",
      "Epoch: 677, Loss: 0.2843, | Train: 0.9437, Val: 0.4100 Test: 0.4300\n",
      "Epoch: 678, Loss: 0.2771, | Train: 0.9412, Val: 0.4200 Test: 0.4200\n",
      "Epoch: 679, Loss: 0.2910, | Train: 0.9375, Val: 0.4300 Test: 0.4200\n",
      "Epoch: 680, Loss: 0.2537, | Train: 0.9387, Val: 0.4500 Test: 0.4000\n",
      "Epoch: 681, Loss: 0.3148, | Train: 0.9437, Val: 0.4500 Test: 0.4200\n",
      "Epoch: 682, Loss: 0.2432, | Train: 0.9425, Val: 0.4300 Test: 0.4000\n",
      "Epoch: 683, Loss: 0.2687, | Train: 0.9412, Val: 0.4100 Test: 0.3800\n",
      "Epoch: 684, Loss: 0.2797, | Train: 0.9462, Val: 0.4200 Test: 0.4000\n",
      "Epoch: 685, Loss: 0.2984, | Train: 0.9437, Val: 0.4300 Test: 0.3900\n",
      "Epoch: 686, Loss: 0.2688, | Train: 0.9437, Val: 0.4400 Test: 0.4000\n",
      "Epoch: 687, Loss: 0.2621, | Train: 0.9425, Val: 0.4300 Test: 0.4100\n",
      "Epoch: 688, Loss: 0.2851, | Train: 0.9500, Val: 0.4300 Test: 0.4200\n",
      "Epoch: 689, Loss: 0.2739, | Train: 0.9525, Val: 0.4000 Test: 0.4200\n",
      "Epoch: 690, Loss: 0.2561, | Train: 0.9512, Val: 0.4400 Test: 0.3900\n",
      "Epoch: 691, Loss: 0.2930, | Train: 0.9412, Val: 0.4300 Test: 0.4100\n",
      "Epoch: 692, Loss: 0.2559, | Train: 0.9462, Val: 0.4300 Test: 0.4000\n",
      "Epoch: 693, Loss: 0.2864, | Train: 0.9537, Val: 0.4300 Test: 0.4000\n",
      "Epoch: 694, Loss: 0.2551, | Train: 0.9462, Val: 0.4000 Test: 0.4100\n",
      "Epoch: 695, Loss: 0.2794, | Train: 0.9500, Val: 0.4300 Test: 0.4100\n",
      "Epoch: 696, Loss: 0.2705, | Train: 0.9450, Val: 0.4200 Test: 0.4000\n",
      "Epoch: 697, Loss: 0.2673, | Train: 0.9462, Val: 0.4200 Test: 0.4100\n",
      "Epoch: 698, Loss: 0.2559, | Train: 0.9412, Val: 0.4400 Test: 0.4100\n",
      "Epoch: 699, Loss: 0.2688, | Train: 0.9450, Val: 0.4500 Test: 0.4100\n",
      "Epoch: 700, Loss: 0.2605, | Train: 0.9500, Val: 0.4100 Test: 0.4200\n",
      "Epoch: 701, Loss: 0.3078, | Train: 0.9500, Val: 0.4400 Test: 0.3900\n",
      "Epoch: 702, Loss: 0.2891, | Train: 0.9512, Val: 0.4500 Test: 0.4000\n",
      "Epoch: 703, Loss: 0.2588, | Train: 0.9512, Val: 0.4400 Test: 0.4100\n",
      "Epoch: 704, Loss: 0.2631, | Train: 0.9487, Val: 0.4300 Test: 0.4300\n",
      "Epoch: 705, Loss: 0.2520, | Train: 0.9475, Val: 0.4300 Test: 0.4300\n",
      "Epoch: 706, Loss: 0.2667, | Train: 0.9462, Val: 0.4500 Test: 0.4100\n",
      "Epoch: 707, Loss: 0.2888, | Train: 0.9450, Val: 0.4500 Test: 0.4100\n",
      "Epoch: 708, Loss: 0.2495, | Train: 0.9475, Val: 0.4500 Test: 0.4000\n",
      "Epoch: 709, Loss: 0.2730, | Train: 0.9500, Val: 0.4500 Test: 0.4100\n",
      "Epoch: 710, Loss: 0.2893, | Train: 0.9512, Val: 0.4400 Test: 0.4000\n",
      "Epoch: 711, Loss: 0.2666, | Train: 0.9512, Val: 0.4200 Test: 0.4000\n",
      "Epoch: 712, Loss: 0.2787, | Train: 0.9550, Val: 0.4100 Test: 0.3900\n",
      "Epoch: 713, Loss: 0.2721, | Train: 0.9575, Val: 0.4200 Test: 0.3800\n",
      "Epoch: 714, Loss: 0.2560, | Train: 0.9537, Val: 0.4200 Test: 0.3900\n",
      "Epoch: 715, Loss: 0.2610, | Train: 0.9500, Val: 0.4400 Test: 0.4100\n",
      "Epoch: 716, Loss: 0.2688, | Train: 0.9425, Val: 0.4500 Test: 0.4100\n",
      "Epoch: 717, Loss: 0.2863, | Train: 0.9400, Val: 0.4700 Test: 0.4100\n",
      "Epoch: 718, Loss: 0.3002, | Train: 0.9450, Val: 0.4700 Test: 0.4100\n",
      "Epoch: 719, Loss: 0.2790, | Train: 0.9500, Val: 0.4500 Test: 0.4200\n",
      "Epoch: 720, Loss: 0.2811, | Train: 0.9512, Val: 0.4700 Test: 0.4100\n",
      "Epoch: 721, Loss: 0.2658, | Train: 0.9475, Val: 0.4500 Test: 0.4200\n",
      "Epoch: 722, Loss: 0.2769, | Train: 0.9462, Val: 0.4500 Test: 0.4100\n",
      "Epoch: 723, Loss: 0.2550, | Train: 0.9425, Val: 0.4500 Test: 0.4000\n",
      "Epoch: 724, Loss: 0.2591, | Train: 0.9475, Val: 0.4500 Test: 0.3700\n",
      "Epoch: 725, Loss: 0.2362, | Train: 0.9475, Val: 0.4500 Test: 0.3900\n",
      "Epoch: 726, Loss: 0.2752, | Train: 0.9487, Val: 0.4200 Test: 0.4100\n",
      "Epoch: 727, Loss: 0.2801, | Train: 0.9475, Val: 0.4400 Test: 0.4200\n",
      "Epoch: 728, Loss: 0.2812, | Train: 0.9500, Val: 0.4400 Test: 0.4200\n",
      "Epoch: 729, Loss: 0.2365, | Train: 0.9500, Val: 0.4500 Test: 0.4100\n",
      "Epoch: 730, Loss: 0.2581, | Train: 0.9500, Val: 0.4500 Test: 0.4100\n",
      "Epoch: 731, Loss: 0.2753, | Train: 0.9512, Val: 0.4600 Test: 0.3900\n",
      "Epoch: 732, Loss: 0.2800, | Train: 0.9537, Val: 0.4400 Test: 0.4000\n",
      "Epoch: 733, Loss: 0.2538, | Train: 0.9437, Val: 0.4500 Test: 0.4200\n",
      "Epoch: 734, Loss: 0.2606, | Train: 0.9450, Val: 0.4500 Test: 0.4200\n",
      "Epoch: 735, Loss: 0.2503, | Train: 0.9437, Val: 0.4700 Test: 0.4300\n",
      "Epoch: 736, Loss: 0.2803, | Train: 0.9462, Val: 0.4800 Test: 0.4200\n",
      "Epoch: 737, Loss: 0.2800, | Train: 0.9512, Val: 0.4900 Test: 0.4400\n",
      "Epoch: 738, Loss: 0.2749, | Train: 0.9487, Val: 0.4600 Test: 0.4300\n",
      "Epoch: 739, Loss: 0.2924, | Train: 0.9487, Val: 0.4500 Test: 0.4300\n",
      "Epoch: 740, Loss: 0.2527, | Train: 0.9487, Val: 0.4700 Test: 0.4400\n",
      "Epoch: 741, Loss: 0.2774, | Train: 0.9412, Val: 0.4800 Test: 0.4300\n",
      "Epoch: 742, Loss: 0.2786, | Train: 0.9450, Val: 0.4600 Test: 0.4600\n",
      "Epoch: 743, Loss: 0.2687, | Train: 0.9512, Val: 0.4400 Test: 0.4100\n",
      "Epoch: 744, Loss: 0.2579, | Train: 0.9512, Val: 0.4300 Test: 0.4200\n",
      "Epoch: 745, Loss: 0.2712, | Train: 0.9525, Val: 0.4500 Test: 0.4300\n",
      "Epoch: 746, Loss: 0.2645, | Train: 0.9550, Val: 0.4700 Test: 0.4000\n",
      "Epoch: 747, Loss: 0.2865, | Train: 0.9500, Val: 0.4700 Test: 0.4000\n",
      "Epoch: 748, Loss: 0.2577, | Train: 0.9462, Val: 0.4800 Test: 0.4300\n",
      "Epoch: 749, Loss: 0.2740, | Train: 0.9500, Val: 0.4800 Test: 0.4400\n",
      "Epoch: 750, Loss: 0.2378, | Train: 0.9437, Val: 0.5100 Test: 0.4300\n",
      "Epoch: 751, Loss: 0.2784, | Train: 0.9525, Val: 0.4800 Test: 0.4200\n",
      "Epoch: 752, Loss: 0.2762, | Train: 0.9525, Val: 0.4400 Test: 0.3900\n",
      "Epoch: 753, Loss: 0.2654, | Train: 0.9562, Val: 0.4400 Test: 0.4000\n",
      "Epoch: 754, Loss: 0.2727, | Train: 0.9525, Val: 0.4600 Test: 0.3900\n",
      "Epoch: 755, Loss: 0.3002, | Train: 0.9462, Val: 0.4300 Test: 0.4100\n",
      "Epoch: 756, Loss: 0.2715, | Train: 0.9412, Val: 0.4100 Test: 0.4300\n",
      "Epoch: 757, Loss: 0.2594, | Train: 0.9437, Val: 0.4400 Test: 0.4400\n",
      "Epoch: 758, Loss: 0.2787, | Train: 0.9500, Val: 0.4500 Test: 0.4300\n",
      "Epoch: 759, Loss: 0.2815, | Train: 0.9512, Val: 0.4600 Test: 0.4100\n",
      "Epoch: 760, Loss: 0.2971, | Train: 0.9512, Val: 0.4600 Test: 0.4100\n",
      "Epoch: 761, Loss: 0.2852, | Train: 0.9487, Val: 0.5000 Test: 0.4200\n",
      "Epoch: 762, Loss: 0.2972, | Train: 0.9450, Val: 0.4400 Test: 0.4100\n",
      "Epoch: 763, Loss: 0.2977, | Train: 0.9462, Val: 0.4400 Test: 0.4000\n",
      "Epoch: 764, Loss: 0.2821, | Train: 0.9400, Val: 0.4400 Test: 0.4000\n",
      "Epoch: 765, Loss: 0.2790, | Train: 0.9487, Val: 0.4300 Test: 0.3900\n",
      "Epoch: 766, Loss: 0.2789, | Train: 0.9512, Val: 0.4500 Test: 0.3900\n",
      "Epoch: 767, Loss: 0.2647, | Train: 0.9537, Val: 0.4500 Test: 0.4000\n",
      "Epoch: 768, Loss: 0.2748, | Train: 0.9537, Val: 0.4800 Test: 0.4100\n",
      "Epoch: 769, Loss: 0.2551, | Train: 0.9500, Val: 0.4700 Test: 0.4300\n",
      "Epoch: 770, Loss: 0.2693, | Train: 0.9487, Val: 0.4700 Test: 0.4400\n",
      "Epoch: 771, Loss: 0.2656, | Train: 0.9462, Val: 0.4400 Test: 0.4200\n",
      "Epoch: 772, Loss: 0.2639, | Train: 0.9487, Val: 0.4500 Test: 0.4300\n",
      "Epoch: 773, Loss: 0.2898, | Train: 0.9475, Val: 0.4500 Test: 0.4200\n",
      "Epoch: 774, Loss: 0.3468, | Train: 0.9462, Val: 0.4500 Test: 0.4000\n",
      "Epoch: 775, Loss: 0.2866, | Train: 0.9462, Val: 0.4600 Test: 0.4100\n",
      "Epoch: 776, Loss: 0.2842, | Train: 0.9475, Val: 0.4500 Test: 0.4200\n",
      "Epoch: 777, Loss: 0.2536, | Train: 0.9500, Val: 0.4500 Test: 0.4200\n",
      "Epoch: 778, Loss: 0.2867, | Train: 0.9537, Val: 0.4200 Test: 0.4200\n",
      "Epoch: 779, Loss: 0.2955, | Train: 0.9512, Val: 0.4500 Test: 0.4300\n",
      "Epoch: 780, Loss: 0.2854, | Train: 0.9412, Val: 0.4600 Test: 0.4100\n",
      "Epoch: 781, Loss: 0.2697, | Train: 0.9412, Val: 0.4900 Test: 0.4100\n",
      "Epoch: 782, Loss: 0.2669, | Train: 0.9425, Val: 0.4900 Test: 0.4100\n",
      "Epoch: 783, Loss: 0.2686, | Train: 0.9500, Val: 0.4600 Test: 0.4000\n",
      "Epoch: 784, Loss: 0.2631, | Train: 0.9437, Val: 0.4400 Test: 0.4200\n",
      "Epoch: 785, Loss: 0.2769, | Train: 0.9500, Val: 0.4700 Test: 0.4300\n",
      "Epoch: 786, Loss: 0.2561, | Train: 0.9512, Val: 0.4700 Test: 0.4400\n",
      "Epoch: 787, Loss: 0.2786, | Train: 0.9475, Val: 0.4700 Test: 0.4400\n",
      "Epoch: 788, Loss: 0.3143, | Train: 0.9487, Val: 0.4800 Test: 0.4300\n",
      "Epoch: 789, Loss: 0.3416, | Train: 0.9437, Val: 0.4900 Test: 0.4100\n",
      "Epoch: 790, Loss: 0.2761, | Train: 0.9462, Val: 0.4800 Test: 0.3800\n",
      "Epoch: 791, Loss: 0.2772, | Train: 0.9450, Val: 0.4900 Test: 0.4000\n",
      "Epoch: 792, Loss: 0.2865, | Train: 0.9475, Val: 0.4600 Test: 0.4000\n",
      "Epoch: 793, Loss: 0.2883, | Train: 0.9412, Val: 0.4700 Test: 0.4000\n",
      "Epoch: 794, Loss: 0.2604, | Train: 0.9425, Val: 0.4700 Test: 0.4400\n",
      "Epoch: 795, Loss: 0.2384, | Train: 0.9462, Val: 0.4500 Test: 0.4400\n",
      "Epoch: 796, Loss: 0.2536, | Train: 0.9525, Val: 0.4500 Test: 0.4200\n",
      "Epoch: 797, Loss: 0.2397, | Train: 0.9512, Val: 0.4600 Test: 0.4100\n",
      "Epoch: 798, Loss: 0.2628, | Train: 0.9475, Val: 0.4400 Test: 0.3800\n",
      "Epoch: 799, Loss: 0.2634, | Train: 0.9512, Val: 0.4500 Test: 0.3600\n",
      "Epoch: 800, Loss: 0.2535, | Train: 0.9500, Val: 0.4200 Test: 0.3800\n",
      "Epoch: 801, Loss: 0.2765, | Train: 0.9437, Val: 0.4200 Test: 0.4100\n",
      "Epoch: 802, Loss: 0.2733, | Train: 0.9525, Val: 0.4400 Test: 0.4200\n",
      "Epoch: 803, Loss: 0.2533, | Train: 0.9487, Val: 0.4400 Test: 0.4400\n",
      "Epoch: 804, Loss: 0.2669, | Train: 0.9512, Val: 0.4400 Test: 0.4300\n",
      "Epoch: 805, Loss: 0.2561, | Train: 0.9550, Val: 0.4500 Test: 0.4300\n",
      "Epoch: 806, Loss: 0.2644, | Train: 0.9525, Val: 0.4600 Test: 0.4200\n",
      "Epoch: 807, Loss: 0.2607, | Train: 0.9512, Val: 0.4500 Test: 0.4000\n",
      "Epoch: 808, Loss: 0.2543, | Train: 0.9462, Val: 0.4300 Test: 0.4100\n",
      "Epoch: 809, Loss: 0.2310, | Train: 0.9475, Val: 0.4500 Test: 0.4200\n",
      "Epoch: 810, Loss: 0.2968, | Train: 0.9462, Val: 0.4900 Test: 0.4200\n",
      "Epoch: 811, Loss: 0.2673, | Train: 0.9462, Val: 0.4900 Test: 0.4100\n",
      "Epoch: 812, Loss: 0.2721, | Train: 0.9512, Val: 0.4800 Test: 0.4100\n",
      "Epoch: 813, Loss: 0.2353, | Train: 0.9525, Val: 0.4800 Test: 0.4200\n",
      "Epoch: 814, Loss: 0.2332, | Train: 0.9437, Val: 0.4600 Test: 0.4100\n",
      "Epoch: 815, Loss: 0.2471, | Train: 0.9487, Val: 0.4800 Test: 0.4000\n",
      "Epoch: 816, Loss: 0.2475, | Train: 0.9537, Val: 0.4800 Test: 0.4000\n",
      "Epoch: 817, Loss: 0.2596, | Train: 0.9512, Val: 0.4800 Test: 0.4200\n",
      "Epoch: 818, Loss: 0.2575, | Train: 0.9512, Val: 0.4700 Test: 0.4000\n",
      "Epoch: 819, Loss: 0.2593, | Train: 0.9537, Val: 0.4700 Test: 0.4100\n",
      "Epoch: 820, Loss: 0.2658, | Train: 0.9525, Val: 0.4600 Test: 0.4100\n",
      "Epoch: 821, Loss: 0.2452, | Train: 0.9487, Val: 0.4400 Test: 0.4300\n",
      "Epoch: 822, Loss: 0.2459, | Train: 0.9512, Val: 0.4500 Test: 0.4100\n",
      "Epoch: 823, Loss: 0.2985, | Train: 0.9537, Val: 0.4400 Test: 0.3900\n",
      "Epoch: 824, Loss: 0.2531, | Train: 0.9575, Val: 0.4500 Test: 0.3800\n",
      "Epoch: 825, Loss: 0.2299, | Train: 0.9487, Val: 0.4600 Test: 0.4200\n",
      "Epoch: 826, Loss: 0.2565, | Train: 0.9437, Val: 0.4500 Test: 0.4200\n",
      "Epoch: 827, Loss: 0.2416, | Train: 0.9475, Val: 0.4500 Test: 0.4300\n",
      "Epoch: 828, Loss: 0.2660, | Train: 0.9450, Val: 0.4400 Test: 0.4100\n",
      "Epoch: 829, Loss: 0.2580, | Train: 0.9525, Val: 0.4500 Test: 0.4100\n",
      "Epoch: 830, Loss: 0.2280, | Train: 0.9550, Val: 0.4700 Test: 0.4100\n",
      "Epoch: 831, Loss: 0.2402, | Train: 0.9562, Val: 0.4600 Test: 0.4000\n",
      "Epoch: 832, Loss: 0.2726, | Train: 0.9537, Val: 0.4800 Test: 0.4100\n",
      "Epoch: 833, Loss: 0.2591, | Train: 0.9525, Val: 0.4800 Test: 0.4200\n",
      "Epoch: 834, Loss: 0.2574, | Train: 0.9487, Val: 0.4900 Test: 0.4100\n",
      "Epoch: 835, Loss: 0.2562, | Train: 0.9462, Val: 0.5000 Test: 0.4200\n",
      "Epoch: 836, Loss: 0.2316, | Train: 0.9512, Val: 0.4600 Test: 0.4200\n",
      "Epoch: 837, Loss: 0.2826, | Train: 0.9562, Val: 0.4900 Test: 0.4000\n",
      "Epoch: 838, Loss: 0.2693, | Train: 0.9587, Val: 0.4700 Test: 0.3800\n",
      "Epoch: 839, Loss: 0.2303, | Train: 0.9575, Val: 0.4800 Test: 0.3900\n",
      "Epoch: 840, Loss: 0.2515, | Train: 0.9575, Val: 0.4700 Test: 0.4000\n",
      "Epoch: 841, Loss: 0.2577, | Train: 0.9525, Val: 0.4600 Test: 0.4100\n",
      "Epoch: 842, Loss: 0.2599, | Train: 0.9537, Val: 0.4800 Test: 0.4300\n",
      "Epoch: 843, Loss: 0.2505, | Train: 0.9537, Val: 0.4300 Test: 0.3800\n",
      "Epoch: 844, Loss: 0.2440, | Train: 0.9575, Val: 0.4400 Test: 0.3700\n",
      "Epoch: 845, Loss: 0.2569, | Train: 0.9587, Val: 0.4700 Test: 0.3700\n",
      "Epoch: 846, Loss: 0.2544, | Train: 0.9587, Val: 0.4800 Test: 0.3900\n",
      "Epoch: 847, Loss: 0.2388, | Train: 0.9525, Val: 0.4600 Test: 0.3800\n",
      "Epoch: 848, Loss: 0.2551, | Train: 0.9512, Val: 0.4500 Test: 0.3900\n",
      "Epoch: 849, Loss: 0.2473, | Train: 0.9462, Val: 0.4500 Test: 0.3700\n",
      "Epoch: 850, Loss: 0.2586, | Train: 0.9475, Val: 0.4400 Test: 0.3900\n",
      "Epoch: 851, Loss: 0.2572, | Train: 0.9487, Val: 0.4400 Test: 0.3900\n",
      "Epoch: 852, Loss: 0.2314, | Train: 0.9550, Val: 0.4700 Test: 0.3900\n",
      "Epoch: 853, Loss: 0.2473, | Train: 0.9562, Val: 0.4800 Test: 0.4100\n",
      "Epoch: 854, Loss: 0.2249, | Train: 0.9575, Val: 0.4800 Test: 0.3900\n",
      "Epoch: 855, Loss: 0.2669, | Train: 0.9587, Val: 0.4800 Test: 0.4000\n",
      "Epoch: 856, Loss: 0.2624, | Train: 0.9600, Val: 0.4500 Test: 0.3900\n",
      "Epoch: 857, Loss: 0.2467, | Train: 0.9587, Val: 0.4400 Test: 0.3800\n",
      "Epoch: 858, Loss: 0.2239, | Train: 0.9587, Val: 0.4500 Test: 0.3800\n",
      "Epoch: 859, Loss: 0.2439, | Train: 0.9500, Val: 0.4500 Test: 0.4000\n",
      "Epoch: 860, Loss: 0.2751, | Train: 0.9512, Val: 0.4500 Test: 0.4000\n",
      "Epoch: 861, Loss: 0.2211, | Train: 0.9500, Val: 0.4700 Test: 0.4300\n",
      "Epoch: 862, Loss: 0.2406, | Train: 0.9500, Val: 0.4500 Test: 0.4000\n",
      "Epoch: 863, Loss: 0.2258, | Train: 0.9525, Val: 0.4500 Test: 0.3800\n",
      "Epoch: 864, Loss: 0.2536, | Train: 0.9537, Val: 0.4500 Test: 0.4000\n",
      "Epoch: 865, Loss: 0.2139, | Train: 0.9562, Val: 0.4600 Test: 0.3900\n",
      "Epoch: 866, Loss: 0.2758, | Train: 0.9550, Val: 0.4800 Test: 0.4000\n",
      "Epoch: 867, Loss: 0.2276, | Train: 0.9537, Val: 0.4600 Test: 0.4200\n",
      "Epoch: 868, Loss: 0.2528, | Train: 0.9525, Val: 0.4500 Test: 0.3900\n",
      "Epoch: 869, Loss: 0.2309, | Train: 0.9600, Val: 0.4700 Test: 0.3900\n",
      "Epoch: 870, Loss: 0.2767, | Train: 0.9613, Val: 0.4600 Test: 0.3700\n",
      "Epoch: 871, Loss: 0.2581, | Train: 0.9638, Val: 0.4600 Test: 0.3600\n",
      "Epoch: 872, Loss: 0.2323, | Train: 0.9650, Val: 0.4700 Test: 0.3500\n",
      "Epoch: 873, Loss: 0.2167, | Train: 0.9638, Val: 0.4600 Test: 0.3600\n",
      "Epoch: 874, Loss: 0.2385, | Train: 0.9587, Val: 0.4500 Test: 0.3600\n",
      "Epoch: 875, Loss: 0.2532, | Train: 0.9550, Val: 0.4400 Test: 0.3800\n",
      "Epoch: 876, Loss: 0.2600, | Train: 0.9550, Val: 0.4300 Test: 0.3700\n",
      "Epoch: 877, Loss: 0.2706, | Train: 0.9550, Val: 0.4400 Test: 0.3700\n",
      "Epoch: 878, Loss: 0.2540, | Train: 0.9525, Val: 0.4600 Test: 0.3700\n",
      "Epoch: 879, Loss: 0.2583, | Train: 0.9525, Val: 0.4700 Test: 0.3800\n",
      "Epoch: 880, Loss: 0.2719, | Train: 0.9562, Val: 0.4600 Test: 0.3700\n",
      "Epoch: 881, Loss: 0.2545, | Train: 0.9525, Val: 0.4600 Test: 0.3800\n",
      "Epoch: 882, Loss: 0.2363, | Train: 0.9512, Val: 0.4800 Test: 0.3900\n",
      "Epoch: 883, Loss: 0.2382, | Train: 0.9525, Val: 0.4700 Test: 0.3900\n",
      "Epoch: 884, Loss: 0.2536, | Train: 0.9500, Val: 0.4700 Test: 0.4000\n",
      "Epoch: 885, Loss: 0.2546, | Train: 0.9550, Val: 0.4700 Test: 0.3900\n",
      "Epoch: 886, Loss: 0.2407, | Train: 0.9525, Val: 0.4800 Test: 0.3900\n",
      "Epoch: 887, Loss: 0.2737, | Train: 0.9512, Val: 0.4500 Test: 0.4100\n",
      "Epoch: 888, Loss: 0.2613, | Train: 0.9562, Val: 0.4400 Test: 0.4000\n",
      "Epoch: 889, Loss: 0.2328, | Train: 0.9537, Val: 0.4400 Test: 0.4200\n",
      "Epoch: 890, Loss: 0.2179, | Train: 0.9550, Val: 0.4600 Test: 0.4000\n",
      "Epoch: 891, Loss: 0.2472, | Train: 0.9475, Val: 0.4400 Test: 0.3900\n",
      "Epoch: 892, Loss: 0.2867, | Train: 0.9537, Val: 0.4900 Test: 0.3900\n",
      "Epoch: 893, Loss: 0.2432, | Train: 0.9600, Val: 0.4800 Test: 0.4000\n",
      "Epoch: 894, Loss: 0.2393, | Train: 0.9600, Val: 0.4800 Test: 0.3800\n",
      "Epoch: 895, Loss: 0.2394, | Train: 0.9600, Val: 0.4600 Test: 0.4000\n",
      "Epoch: 896, Loss: 0.2462, | Train: 0.9562, Val: 0.4400 Test: 0.3900\n",
      "Epoch: 897, Loss: 0.2429, | Train: 0.9562, Val: 0.4700 Test: 0.4000\n",
      "Epoch: 898, Loss: 0.2320, | Train: 0.9537, Val: 0.4600 Test: 0.3700\n",
      "Epoch: 899, Loss: 0.2239, | Train: 0.9525, Val: 0.4700 Test: 0.3800\n",
      "Epoch: 900, Loss: 0.2504, | Train: 0.9537, Val: 0.4500 Test: 0.3700\n",
      "Epoch: 901, Loss: 0.2367, | Train: 0.9587, Val: 0.4400 Test: 0.3900\n",
      "Epoch: 902, Loss: 0.2240, | Train: 0.9575, Val: 0.4500 Test: 0.4000\n",
      "Epoch: 903, Loss: 0.2349, | Train: 0.9587, Val: 0.4700 Test: 0.4000\n",
      "Epoch: 904, Loss: 0.2672, | Train: 0.9587, Val: 0.4500 Test: 0.3900\n",
      "Epoch: 905, Loss: 0.2326, | Train: 0.9525, Val: 0.4500 Test: 0.4000\n",
      "Epoch: 906, Loss: 0.2256, | Train: 0.9512, Val: 0.4600 Test: 0.3800\n",
      "Epoch: 907, Loss: 0.2533, | Train: 0.9587, Val: 0.4600 Test: 0.3800\n",
      "Epoch: 908, Loss: 0.2583, | Train: 0.9575, Val: 0.4500 Test: 0.3800\n",
      "Epoch: 909, Loss: 0.2658, | Train: 0.9562, Val: 0.4400 Test: 0.3800\n",
      "Epoch: 910, Loss: 0.2463, | Train: 0.9600, Val: 0.4500 Test: 0.3800\n",
      "Epoch: 911, Loss: 0.2379, | Train: 0.9550, Val: 0.4600 Test: 0.4000\n",
      "Epoch: 912, Loss: 0.2478, | Train: 0.9537, Val: 0.4500 Test: 0.3900\n",
      "Epoch: 913, Loss: 0.2575, | Train: 0.9550, Val: 0.4500 Test: 0.3900\n",
      "Epoch: 914, Loss: 0.2643, | Train: 0.9550, Val: 0.4500 Test: 0.3900\n",
      "Epoch: 915, Loss: 0.2313, | Train: 0.9587, Val: 0.4400 Test: 0.3900\n",
      "Epoch: 916, Loss: 0.2310, | Train: 0.9562, Val: 0.4300 Test: 0.3900\n",
      "Epoch: 917, Loss: 0.2480, | Train: 0.9562, Val: 0.4400 Test: 0.3800\n",
      "Epoch: 918, Loss: 0.2217, | Train: 0.9613, Val: 0.4300 Test: 0.3800\n",
      "Epoch: 919, Loss: 0.2535, | Train: 0.9562, Val: 0.4400 Test: 0.4000\n",
      "Epoch: 920, Loss: 0.2309, | Train: 0.9550, Val: 0.4600 Test: 0.4000\n",
      "Epoch: 921, Loss: 0.2257, | Train: 0.9587, Val: 0.4600 Test: 0.4100\n",
      "Epoch: 922, Loss: 0.2243, | Train: 0.9663, Val: 0.4500 Test: 0.4200\n",
      "Epoch: 923, Loss: 0.2298, | Train: 0.9625, Val: 0.4400 Test: 0.4100\n",
      "Epoch: 924, Loss: 0.2415, | Train: 0.9638, Val: 0.4200 Test: 0.4100\n",
      "Epoch: 925, Loss: 0.2116, | Train: 0.9587, Val: 0.4400 Test: 0.4200\n",
      "Epoch: 926, Loss: 0.2454, | Train: 0.9600, Val: 0.4400 Test: 0.4400\n",
      "Epoch: 927, Loss: 0.2404, | Train: 0.9562, Val: 0.4300 Test: 0.4300\n",
      "Epoch: 928, Loss: 0.2490, | Train: 0.9562, Val: 0.4300 Test: 0.4100\n",
      "Epoch: 929, Loss: 0.2226, | Train: 0.9562, Val: 0.4400 Test: 0.4200\n",
      "Epoch: 930, Loss: 0.2353, | Train: 0.9500, Val: 0.4400 Test: 0.4300\n",
      "Epoch: 931, Loss: 0.2494, | Train: 0.9525, Val: 0.4300 Test: 0.4200\n",
      "Epoch: 932, Loss: 0.2381, | Train: 0.9550, Val: 0.4400 Test: 0.4200\n",
      "Epoch: 933, Loss: 0.2828, | Train: 0.9587, Val: 0.4200 Test: 0.4100\n",
      "Epoch: 934, Loss: 0.2353, | Train: 0.9562, Val: 0.4300 Test: 0.4200\n",
      "Epoch: 935, Loss: 0.2439, | Train: 0.9550, Val: 0.4600 Test: 0.4100\n",
      "Epoch: 936, Loss: 0.2276, | Train: 0.9525, Val: 0.4700 Test: 0.4100\n",
      "Epoch: 937, Loss: 0.2373, | Train: 0.9550, Val: 0.4600 Test: 0.4300\n",
      "Epoch: 938, Loss: 0.2354, | Train: 0.9550, Val: 0.4600 Test: 0.3900\n",
      "Epoch: 939, Loss: 0.2352, | Train: 0.9587, Val: 0.4600 Test: 0.3800\n",
      "Epoch: 940, Loss: 0.2606, | Train: 0.9675, Val: 0.4500 Test: 0.3800\n",
      "Epoch: 941, Loss: 0.2585, | Train: 0.9663, Val: 0.4400 Test: 0.4000\n",
      "Epoch: 942, Loss: 0.2589, | Train: 0.9613, Val: 0.4400 Test: 0.4000\n",
      "Epoch: 943, Loss: 0.2251, | Train: 0.9562, Val: 0.4300 Test: 0.4300\n",
      "Epoch: 944, Loss: 0.2395, | Train: 0.9525, Val: 0.4500 Test: 0.4100\n",
      "Epoch: 945, Loss: 0.2401, | Train: 0.9487, Val: 0.4600 Test: 0.4400\n",
      "Epoch: 946, Loss: 0.2613, | Train: 0.9537, Val: 0.4500 Test: 0.4300\n",
      "Epoch: 947, Loss: 0.2318, | Train: 0.9550, Val: 0.4500 Test: 0.4000\n",
      "Epoch: 948, Loss: 0.2424, | Train: 0.9475, Val: 0.4300 Test: 0.3900\n",
      "Epoch: 949, Loss: 0.2350, | Train: 0.9475, Val: 0.4800 Test: 0.4300\n",
      "Epoch: 950, Loss: 0.2662, | Train: 0.9525, Val: 0.4800 Test: 0.4600\n",
      "Epoch: 951, Loss: 0.2443, | Train: 0.9537, Val: 0.4800 Test: 0.4200\n",
      "Epoch: 952, Loss: 0.2307, | Train: 0.9525, Val: 0.4900 Test: 0.4200\n",
      "Epoch: 953, Loss: 0.2467, | Train: 0.9587, Val: 0.4700 Test: 0.3900\n",
      "Epoch: 954, Loss: 0.2489, | Train: 0.9562, Val: 0.4600 Test: 0.4100\n",
      "Epoch: 955, Loss: 0.2500, | Train: 0.9450, Val: 0.4500 Test: 0.4200\n",
      "Epoch: 956, Loss: 0.2460, | Train: 0.9525, Val: 0.4500 Test: 0.4200\n",
      "Epoch: 957, Loss: 0.2514, | Train: 0.9450, Val: 0.4700 Test: 0.4300\n",
      "Epoch: 958, Loss: 0.2454, | Train: 0.9400, Val: 0.4700 Test: 0.4500\n",
      "Epoch: 959, Loss: 0.2628, | Train: 0.9550, Val: 0.4500 Test: 0.4300\n",
      "Epoch: 960, Loss: 0.2402, | Train: 0.9650, Val: 0.4400 Test: 0.4100\n",
      "Epoch: 961, Loss: 0.2701, | Train: 0.9688, Val: 0.4400 Test: 0.4100\n",
      "Epoch: 962, Loss: 0.2627, | Train: 0.9625, Val: 0.4600 Test: 0.4100\n",
      "Epoch: 963, Loss: 0.2637, | Train: 0.9638, Val: 0.4700 Test: 0.4000\n",
      "Epoch: 964, Loss: 0.2552, | Train: 0.9600, Val: 0.4400 Test: 0.4000\n",
      "Epoch: 965, Loss: 0.2672, | Train: 0.9537, Val: 0.4400 Test: 0.4600\n",
      "Epoch: 966, Loss: 0.2309, | Train: 0.9537, Val: 0.4600 Test: 0.4700\n",
      "Epoch: 967, Loss: 0.2462, | Train: 0.9587, Val: 0.4600 Test: 0.4500\n",
      "Epoch: 968, Loss: 0.2440, | Train: 0.9587, Val: 0.4400 Test: 0.4300\n",
      "Epoch: 969, Loss: 0.2676, | Train: 0.9650, Val: 0.4500 Test: 0.4300\n",
      "Epoch: 970, Loss: 0.2403, | Train: 0.9587, Val: 0.4600 Test: 0.4500\n",
      "Epoch: 971, Loss: 0.2613, | Train: 0.9537, Val: 0.4500 Test: 0.4500\n",
      "Epoch: 972, Loss: 0.2400, | Train: 0.9512, Val: 0.4600 Test: 0.4700\n",
      "Epoch: 973, Loss: 0.2400, | Train: 0.9512, Val: 0.4700 Test: 0.4800\n",
      "Epoch: 974, Loss: 0.2594, | Train: 0.9512, Val: 0.4900 Test: 0.4600\n",
      "Epoch: 975, Loss: 0.2651, | Train: 0.9462, Val: 0.4800 Test: 0.4800\n",
      "Epoch: 976, Loss: 0.2172, | Train: 0.9487, Val: 0.4600 Test: 0.4600\n",
      "Epoch: 977, Loss: 0.2369, | Train: 0.9525, Val: 0.4600 Test: 0.4600\n",
      "Epoch: 978, Loss: 0.2363, | Train: 0.9512, Val: 0.4600 Test: 0.4800\n",
      "Epoch: 979, Loss: 0.2283, | Train: 0.9525, Val: 0.4600 Test: 0.4700\n",
      "Epoch: 980, Loss: 0.2413, | Train: 0.9500, Val: 0.4800 Test: 0.4500\n",
      "Epoch: 981, Loss: 0.2580, | Train: 0.9575, Val: 0.4800 Test: 0.4400\n",
      "Epoch: 982, Loss: 0.2749, | Train: 0.9663, Val: 0.4300 Test: 0.4200\n",
      "Epoch: 983, Loss: 0.2307, | Train: 0.9600, Val: 0.4300 Test: 0.3900\n",
      "Epoch: 984, Loss: 0.2550, | Train: 0.9638, Val: 0.4400 Test: 0.3900\n",
      "Epoch: 985, Loss: 0.2453, | Train: 0.9613, Val: 0.4600 Test: 0.4100\n",
      "Epoch: 986, Loss: 0.3018, | Train: 0.9613, Val: 0.4600 Test: 0.4200\n",
      "Epoch: 987, Loss: 0.2740, | Train: 0.9575, Val: 0.4700 Test: 0.4200\n",
      "Epoch: 988, Loss: 0.2680, | Train: 0.9575, Val: 0.5000 Test: 0.4100\n",
      "Epoch: 989, Loss: 0.2297, | Train: 0.9525, Val: 0.4900 Test: 0.4300\n",
      "Epoch: 990, Loss: 0.2323, | Train: 0.9638, Val: 0.4600 Test: 0.4200\n",
      "Epoch: 991, Loss: 0.2377, | Train: 0.9587, Val: 0.4600 Test: 0.4300\n",
      "Epoch: 992, Loss: 0.2418, | Train: 0.9600, Val: 0.4600 Test: 0.4200\n",
      "Epoch: 993, Loss: 0.2218, | Train: 0.9537, Val: 0.4500 Test: 0.4300\n",
      "Epoch: 994, Loss: 0.2404, | Train: 0.9575, Val: 0.4600 Test: 0.4400\n",
      "Epoch: 995, Loss: 0.2473, | Train: 0.9587, Val: 0.4600 Test: 0.4400\n",
      "Epoch: 996, Loss: 0.2200, | Train: 0.9562, Val: 0.4500 Test: 0.4100\n",
      "Epoch: 997, Loss: 0.2029, | Train: 0.9462, Val: 0.4700 Test: 0.4300\n",
      "Epoch: 998, Loss: 0.2173, | Train: 0.9575, Val: 0.4700 Test: 0.4300\n",
      "Epoch: 999, Loss: 0.2293, | Train: 0.9575, Val: 0.4900 Test: 0.4200\n",
      "Epoch: 1000, Loss: 0.2219, | Train: 0.9650, Val: 0.4700 Test: 0.3900\n",
      "Median time per epoch: 0.0016s\n"
     ]
    }
   ],
   "source": [
    "# MLP classifier\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(p=0.2),\n",
    "            nn.Linear(32, 3),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_relu_stack(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "model = NeuralNetwork().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-5)\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(train_x)\n",
    "    loss = F.nll_loss(out, train_y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return float(loss)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    pred = model(patient_embedding).argmax(dim=-1)\n",
    "    train_acc = float((pred[:int(num_patients*0.8)] == train_y).float().mean())\n",
    "    val_acc = float((pred[int(num_patients*0.8):int(num_patients*0.9)] == val_y).float().mean())\n",
    "    test_acc = float((pred[int(num_patients*0.9):] == test_y).float().mean())\n",
    "    return train_acc, val_acc, test_acc\n",
    "\n",
    "import time\n",
    "\n",
    "times = []\n",
    "best_acc = float(0.)\n",
    "\n",
    "for epoch in range(1, 1001):\n",
    "    start = time.time()\n",
    "    loss = train()\n",
    "    train_acc, val_acc, test_acc = test()\n",
    "    print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}, | Train: {train_acc:.4f}, Val: {val_acc:.4f} '\n",
    "        f'Test: {test_acc:.4f}')\n",
    "    # # Early stopping\n",
    "    # if val_acc >= best_acc:\n",
    "    #     best_acc = val_acc\n",
    "    #     patience = 50  # Reset patience counter\n",
    "    # else:\n",
    "    #     patience -= 1\n",
    "    #     if patience == 0:\n",
    "    #         print(\"Early stopping...\")\n",
    "    #         break\n",
    "    times.append(time.time() - start)\n",
    "print(f\"Median time per epoch: {torch.tensor(times).median():.4f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC score: 0.4636\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(patient_embedding).cpu()\n",
    "    prob = F.softmax(out, dim=1)\n",
    "auc = roc_auc_score(test_y.cpu(), prob[int(num_patients*0.9):], multi_class='ovr')\n",
    "print(f'ROC AUC score: {auc:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neurovasc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
