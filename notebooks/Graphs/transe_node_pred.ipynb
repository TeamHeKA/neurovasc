{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outcome Prediction (Node Classification) with TransE Model + MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pykeen.triples import TriplesFactory\n",
    "from pykeen.predict import predict_target\n",
    "from pykeen.pipeline import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load KG and remove outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>s</th>\n",
       "      <th>r</th>\n",
       "      <th>d</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;http://nvasc.org/7d69a2a1-8681-4e71-9ca2-fa75...</td>\n",
       "      <td>&lt;http://www.w3.org/2000/01/rdf-schema#label&gt;</td>\n",
       "      <td>entry_code^^&lt;http://www.w3.org/2001/XMLSchema#...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;http://nvasc.org/f5856cd3-2d82-4d7b-b8ab-18cd...</td>\n",
       "      <td>&lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#type&gt;</td>\n",
       "      <td>&lt;http://sphn.org/Diagnosis&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;http://nvasc.org/db9d32b8-a429-4ab6-8ddf-3753...</td>\n",
       "      <td>&lt;http://sphn.org/hasSubjectPseudoIdentifier&gt;</td>\n",
       "      <td>&lt;http://nvasc.org/synth_patient_769&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;http://nvasc.org/6982d639-68e1-4d97-bbdc-32b7...</td>\n",
       "      <td>&lt;http://sphn.org/hasSubjectPseudoIdentifier&gt;</td>\n",
       "      <td>&lt;http://nvasc.org/synth_patient_397&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;http://nvasc.org/a24f2ca8-60bb-485a-9428-1e88...</td>\n",
       "      <td>&lt;http://www.w3.org/2000/01/rdf-schema#label&gt;</td>\n",
       "      <td>iot^^&lt;http://www.w3.org/2001/XMLSchema#string&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111930</th>\n",
       "      <td>&lt;http://nvasc.org/synth_patient_138&gt;</td>\n",
       "      <td>&lt;http://nvasc.org/hasOutcome&gt;</td>\n",
       "      <td>&lt;http://nvasc.org/outcome_0.0&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111931</th>\n",
       "      <td>_:na4af9ca095e94f96acb25b3c689a54b5b1</td>\n",
       "      <td>&lt;http://sphn.org/hasValue&gt;</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111932</th>\n",
       "      <td>_:n857dfe592f82471cafdeebc283906835b1</td>\n",
       "      <td>&lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#type&gt;</td>\n",
       "      <td>&lt;http://sphn.org/Quantity&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111933</th>\n",
       "      <td>&lt;http://nvasc.org/c656d49c-08c6-49cd-90b0-7778...</td>\n",
       "      <td>&lt;http://www.w3.org/2000/01/rdf-schema#label&gt;</td>\n",
       "      <td>ica_therapy^^&lt;http://www.w3.org/2001/XMLSchema...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111934</th>\n",
       "      <td>_:nc30f7ea2704d49afac1d8f0c3e49d00db1</td>\n",
       "      <td>&lt;http://sphn.org/hasValue&gt;</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>111935 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        s  \\\n",
       "0       <http://nvasc.org/7d69a2a1-8681-4e71-9ca2-fa75...   \n",
       "1       <http://nvasc.org/f5856cd3-2d82-4d7b-b8ab-18cd...   \n",
       "2       <http://nvasc.org/db9d32b8-a429-4ab6-8ddf-3753...   \n",
       "3       <http://nvasc.org/6982d639-68e1-4d97-bbdc-32b7...   \n",
       "4       <http://nvasc.org/a24f2ca8-60bb-485a-9428-1e88...   \n",
       "...                                                   ...   \n",
       "111930               <http://nvasc.org/synth_patient_138>   \n",
       "111931              _:na4af9ca095e94f96acb25b3c689a54b5b1   \n",
       "111932              _:n857dfe592f82471cafdeebc283906835b1   \n",
       "111933  <http://nvasc.org/c656d49c-08c6-49cd-90b0-7778...   \n",
       "111934              _:nc30f7ea2704d49afac1d8f0c3e49d00db1   \n",
       "\n",
       "                                                        r  \\\n",
       "0            <http://www.w3.org/2000/01/rdf-schema#label>   \n",
       "1       <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>   \n",
       "2            <http://sphn.org/hasSubjectPseudoIdentifier>   \n",
       "3            <http://sphn.org/hasSubjectPseudoIdentifier>   \n",
       "4            <http://www.w3.org/2000/01/rdf-schema#label>   \n",
       "...                                                   ...   \n",
       "111930                      <http://nvasc.org/hasOutcome>   \n",
       "111931                         <http://sphn.org/hasValue>   \n",
       "111932  <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>   \n",
       "111933       <http://www.w3.org/2000/01/rdf-schema#label>   \n",
       "111934                         <http://sphn.org/hasValue>   \n",
       "\n",
       "                                                        d  \n",
       "0       entry_code^^<http://www.w3.org/2001/XMLSchema#...  \n",
       "1                             <http://sphn.org/Diagnosis>  \n",
       "2                    <http://nvasc.org/synth_patient_769>  \n",
       "3                    <http://nvasc.org/synth_patient_397>  \n",
       "4          iot^^<http://www.w3.org/2001/XMLSchema#string>  \n",
       "...                                                   ...  \n",
       "111930                     <http://nvasc.org/outcome_0.0>  \n",
       "111931                                                  5  \n",
       "111932                         <http://sphn.org/Quantity>  \n",
       "111933  ica_therapy^^<http://www.w3.org/2001/XMLSchema...  \n",
       "111934                                                 32  \n",
       "\n",
       "[111935 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_patients = 1000\n",
    "df = pd.read_csv(f\"../Data Generation/sphn_transductive_{num_patients}_0.nt\", sep=\" \", header=None)\n",
    "df.drop(columns=df.columns[-1], axis=1, inplace=True)\n",
    "df.columns=['s', 'r', 'd']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove outcomes from KG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome = df['d'].str.contains('outcome_0.0|outcome_1.0|outcome_2.0')\n",
    "node_df = df[~outcome]\n",
    "node_df = node_df.reset_index(drop=True)\n",
    "outcome = node_df['s'].str.contains('outcome_0.0|outcome_1.0|outcome_2.0')\n",
    "node_df = node_df[~outcome]\n",
    "node_df = node_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_to_id = {k: v for v, k in enumerate(set(node_df['s']).union(set(node_df['d'])), start=0)}\n",
    "rel_to_id = {k: v for v, k in enumerate(set(node_df['r']), start=0)}\n",
    "\n",
    "patients = [f\"<http://nvasc.org/synth_patient_{i}>\" for i in range(num_patients)]\n",
    "patient_id = []\n",
    "for patient in patients:\n",
    "    patient_id.append(ent_to_id[patient])\n",
    "\n",
    "num_nodes = max(ent_to_id.values()) + 1\n",
    "num_rels = max(rel_to_id.values()) + 1\n",
    "\n",
    "events = node_df.copy()\n",
    "events[\"s\"] = node_df.s.map(ent_to_id)\n",
    "events[\"d\"] = node_df.d.map(ent_to_id)\n",
    "events[\"r\"] = node_df.r.map(rel_to_id)\n",
    "\n",
    "ent_to_id = pd.DataFrame.from_dict(ent_to_id, orient='index')\n",
    "rel_to_id = pd.DataFrame.from_dict(rel_to_id, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save events, entities and relations to 'processed_data' folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'processed_data'\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "events.to_csv(f'{path}/sphn_events_{num_patients}_noOutcome.tsv', sep='\\t', index=False, header=None)\n",
    "ent_to_id.to_csv(f'{path}/sphn_entities_{num_patients}_noOutcome.tsv', sep='\\t', header=None)\n",
    "rel_to_id.to_csv(f'{path}/sphn_relations_{num_patients}_noOutcome.tsv', sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TriplesFactory(num_entities=33563, num_relations=20, create_inverse_triples=True, num_triples=110935, path=\"/home/baical77/projects/neurovasc/notebooks/Graphs/processed_data/sphn_events_1000_noOutcome.tsv\")\n"
     ]
    }
   ],
   "source": [
    "path = 'processed_data'\n",
    "tf = TriplesFactory.from_path(f'{path}/sphn_events_{num_patients}_noOutcome.tsv', create_inverse_triples=True)\n",
    "print(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pykeen.pipeline.api:No random seed is specified. Setting to 733373903.\n",
      "INFO:pykeen.pipeline.api:Using device: gpu\n",
      "INFO:pykeen.triples.triples_factory:Creating inverse triples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "552a9cba48a24a218f24cb730af8144b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epochs on cuda:0:   0%|          | 0/10 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pykeen.triples.triples_factory:Creating inverse triples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dddee07ccf054d9086451e0c7dd7ccb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating on cuda:0:   0%|          | 0.00/111k [00:00<?, ?triple/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pykeen.evaluation.evaluator:Evaluation took 209.72s seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = 'TransE'\n",
    "embedding_dim = 32\n",
    "epochs = 10\n",
    "\n",
    "result = pipeline(\n",
    "    training=tf,\n",
    "    testing=tf,\n",
    "    model=model_name,\n",
    "    model_kwargs=dict(\n",
    "        embedding_dim=embedding_dim,\n",
    "        loss=\"softplus\",\n",
    "    ),\n",
    "    training_kwargs=dict(\n",
    "        num_epochs=epochs,\n",
    "        # label_smoothing=0.1,\n",
    "        use_tqdm_batch=False,\n",
    "    ),\n",
    "    optimizer_kwargs=dict(\n",
    "        lr=0.01,\n",
    "        weight_decay=1e-5,\n",
    "    ),\n",
    "    training_loop='sLCWA',\n",
    "    negative_sampler='basic',\n",
    "    device='gpu',\n",
    "    use_tqdm=True,\n",
    ")\n",
    "\n",
    "#plot loss\n",
    "loss_plot = result.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient embedding size: torch.Size([1000, 32])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "entity = pd.read_csv(f'processed_data/sphn_entities_{num_patients}_noOutcome.tsv', sep='\\t', index_col=0, header=None)\n",
    "entity = entity.to_dict()[1]\n",
    "patient_id = []\n",
    "for i in range(num_patients):\n",
    "    idx = f'<http://nvasc.org/synth_patient_{i}>'\n",
    "    patient_id.append(entity[idx])\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = result.model\n",
    "entity_embedding = model.entity_representations[0](indices=None).detach().cpu()\n",
    "patient_embedding = entity_embedding[patient_id]\n",
    "print(f'Patient embedding size: {patient_embedding.shape}')\n",
    "\n",
    "patient_embedding = patient_embedding.to(device)\n",
    "y = joblib.load('../Data Generation/outcomes_10_0.joblib')\n",
    "y = torch.Tensor(y).long().to(device)\n",
    "\n",
    "train_x, train_y = patient_embedding[:int(num_patients*0.8)], y[:int(num_patients*0.8)]\n",
    "val_x, val_y = patient_embedding[int(num_patients*0.8):int(num_patients*0.9)], y[int(num_patients*0.8):int(num_patients*0.9)]\n",
    "test_x, test_y = patient_embedding[int(num_patients*0.9):], y[int(num_patients*0.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Loss: 1.1199, | Train: 0.4338, Val: 0.4500 Test: 0.3700\n",
      "Epoch: 02, Loss: 1.0861, | Train: 0.4338, Val: 0.4500 Test: 0.3700\n",
      "Epoch: 03, Loss: 1.0595, | Train: 0.4338, Val: 0.4500 Test: 0.3700\n",
      "Epoch: 04, Loss: 1.0360, | Train: 0.4338, Val: 0.4500 Test: 0.3700\n",
      "Epoch: 05, Loss: 1.0148, | Train: 0.4338, Val: 0.4500 Test: 0.3700\n",
      "Epoch: 06, Loss: 0.9994, | Train: 0.4338, Val: 0.4500 Test: 0.3700\n",
      "Epoch: 07, Loss: 0.9861, | Train: 0.4338, Val: 0.4500 Test: 0.3700\n",
      "Epoch: 08, Loss: 0.9791, | Train: 0.4338, Val: 0.4500 Test: 0.3700\n",
      "Epoch: 09, Loss: 0.9800, | Train: 0.4650, Val: 0.4900 Test: 0.4800\n",
      "Epoch: 10, Loss: 0.9828, | Train: 0.4525, Val: 0.4500 Test: 0.4900\n",
      "Epoch: 11, Loss: 0.9858, | Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 12, Loss: 0.9867, | Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 13, Loss: 0.9821, | Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 14, Loss: 0.9784, | Train: 0.4575, Val: 0.4600 Test: 0.5000\n",
      "Epoch: 15, Loss: 0.9721, | Train: 0.4612, Val: 0.5000 Test: 0.5000\n",
      "Epoch: 16, Loss: 0.9672, | Train: 0.4675, Val: 0.5000 Test: 0.4900\n",
      "Epoch: 17, Loss: 0.9666, | Train: 0.4912, Val: 0.4900 Test: 0.4700\n",
      "Epoch: 18, Loss: 0.9691, | Train: 0.4500, Val: 0.4900 Test: 0.4500\n",
      "Epoch: 19, Loss: 0.9703, | Train: 0.4487, Val: 0.4800 Test: 0.4500\n",
      "Epoch: 20, Loss: 0.9658, | Train: 0.4475, Val: 0.4800 Test: 0.4500\n",
      "Epoch: 21, Loss: 0.9677, | Train: 0.4500, Val: 0.4800 Test: 0.4500\n",
      "Epoch: 22, Loss: 0.9665, | Train: 0.4500, Val: 0.4900 Test: 0.4500\n",
      "Epoch: 23, Loss: 0.9674, | Train: 0.4575, Val: 0.4900 Test: 0.4600\n",
      "Epoch: 24, Loss: 0.9660, | Train: 0.4725, Val: 0.5200 Test: 0.4700\n",
      "Epoch: 25, Loss: 0.9638, | Train: 0.4812, Val: 0.5000 Test: 0.5000\n",
      "Epoch: 26, Loss: 0.9619, | Train: 0.4700, Val: 0.4700 Test: 0.4900\n",
      "Epoch: 27, Loss: 0.9645, | Train: 0.4663, Val: 0.4900 Test: 0.4700\n",
      "Epoch: 28, Loss: 0.9638, | Train: 0.4675, Val: 0.4900 Test: 0.4800\n",
      "Epoch: 29, Loss: 0.9605, | Train: 0.4675, Val: 0.4900 Test: 0.4800\n",
      "Epoch: 30, Loss: 0.9602, | Train: 0.4675, Val: 0.4900 Test: 0.4600\n",
      "Epoch: 31, Loss: 0.9591, | Train: 0.4637, Val: 0.4700 Test: 0.4800\n",
      "Epoch: 32, Loss: 0.9564, | Train: 0.4688, Val: 0.4900 Test: 0.4900\n",
      "Epoch: 33, Loss: 0.9560, | Train: 0.4762, Val: 0.4600 Test: 0.4900\n",
      "Epoch: 34, Loss: 0.9568, | Train: 0.4900, Val: 0.4400 Test: 0.4900\n",
      "Epoch: 35, Loss: 0.9529, | Train: 0.4937, Val: 0.4400 Test: 0.5000\n",
      "Epoch: 36, Loss: 0.9528, | Train: 0.4963, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 37, Loss: 0.9540, | Train: 0.4850, Val: 0.4700 Test: 0.5000\n",
      "Epoch: 38, Loss: 0.9498, | Train: 0.4850, Val: 0.4800 Test: 0.5200\n",
      "Epoch: 39, Loss: 0.9494, | Train: 0.4812, Val: 0.4600 Test: 0.5000\n",
      "Epoch: 40, Loss: 0.9463, | Train: 0.4887, Val: 0.4600 Test: 0.5000\n",
      "Epoch: 41, Loss: 0.9466, | Train: 0.5125, Val: 0.4600 Test: 0.4900\n",
      "Epoch: 42, Loss: 0.9479, | Train: 0.5225, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 43, Loss: 0.9452, | Train: 0.5275, Val: 0.4600 Test: 0.5200\n",
      "Epoch: 44, Loss: 0.9413, | Train: 0.5275, Val: 0.4600 Test: 0.5400\n",
      "Epoch: 45, Loss: 0.9414, | Train: 0.5325, Val: 0.4600 Test: 0.5400\n",
      "Epoch: 46, Loss: 0.9351, | Train: 0.5300, Val: 0.4500 Test: 0.5300\n",
      "Epoch: 47, Loss: 0.9318, | Train: 0.5250, Val: 0.4400 Test: 0.5300\n",
      "Epoch: 48, Loss: 0.9328, | Train: 0.5312, Val: 0.4400 Test: 0.5000\n",
      "Epoch: 49, Loss: 0.9328, | Train: 0.5312, Val: 0.4400 Test: 0.5200\n",
      "Epoch: 50, Loss: 0.9262, | Train: 0.5375, Val: 0.4300 Test: 0.5100\n",
      "Epoch: 51, Loss: 0.9335, | Train: 0.5387, Val: 0.4400 Test: 0.4900\n",
      "Epoch: 52, Loss: 0.9258, | Train: 0.5462, Val: 0.4300 Test: 0.4800\n",
      "Epoch: 53, Loss: 0.9226, | Train: 0.5537, Val: 0.4200 Test: 0.4700\n",
      "Epoch: 54, Loss: 0.9210, | Train: 0.5550, Val: 0.4400 Test: 0.4500\n",
      "Epoch: 55, Loss: 0.9153, | Train: 0.5550, Val: 0.4500 Test: 0.4500\n",
      "Epoch: 56, Loss: 0.9112, | Train: 0.5600, Val: 0.4300 Test: 0.4500\n",
      "Epoch: 57, Loss: 0.9127, | Train: 0.5612, Val: 0.4500 Test: 0.4600\n",
      "Epoch: 58, Loss: 0.9043, | Train: 0.5650, Val: 0.4600 Test: 0.4700\n",
      "Epoch: 59, Loss: 0.9073, | Train: 0.5625, Val: 0.4300 Test: 0.4700\n",
      "Epoch: 60, Loss: 0.9093, | Train: 0.5675, Val: 0.4400 Test: 0.4900\n",
      "Epoch: 61, Loss: 0.8985, | Train: 0.5800, Val: 0.4300 Test: 0.4600\n",
      "Epoch: 62, Loss: 0.8984, | Train: 0.5825, Val: 0.4300 Test: 0.4500\n",
      "Epoch: 63, Loss: 0.8931, | Train: 0.5788, Val: 0.4200 Test: 0.4700\n",
      "Epoch: 64, Loss: 0.8993, | Train: 0.5775, Val: 0.4200 Test: 0.4800\n",
      "Epoch: 65, Loss: 0.8865, | Train: 0.5825, Val: 0.4200 Test: 0.4800\n",
      "Epoch: 66, Loss: 0.8840, | Train: 0.5800, Val: 0.4400 Test: 0.4400\n",
      "Epoch: 67, Loss: 0.8769, | Train: 0.5863, Val: 0.4100 Test: 0.4600\n",
      "Epoch: 68, Loss: 0.8707, | Train: 0.5850, Val: 0.4200 Test: 0.4800\n",
      "Epoch: 69, Loss: 0.8667, | Train: 0.5962, Val: 0.4100 Test: 0.4700\n",
      "Epoch: 70, Loss: 0.8649, | Train: 0.5913, Val: 0.4100 Test: 0.4900\n",
      "Epoch: 71, Loss: 0.8661, | Train: 0.6050, Val: 0.4000 Test: 0.5000\n",
      "Epoch: 72, Loss: 0.8509, | Train: 0.6050, Val: 0.3900 Test: 0.5100\n",
      "Epoch: 73, Loss: 0.8616, | Train: 0.6100, Val: 0.4000 Test: 0.4900\n",
      "Epoch: 74, Loss: 0.8491, | Train: 0.6225, Val: 0.4000 Test: 0.5000\n",
      "Epoch: 75, Loss: 0.8485, | Train: 0.6300, Val: 0.4500 Test: 0.5200\n",
      "Epoch: 76, Loss: 0.8614, | Train: 0.6325, Val: 0.4100 Test: 0.5100\n",
      "Epoch: 77, Loss: 0.8344, | Train: 0.6375, Val: 0.3800 Test: 0.5200\n",
      "Epoch: 78, Loss: 0.8407, | Train: 0.6337, Val: 0.4000 Test: 0.5000\n",
      "Epoch: 79, Loss: 0.8434, | Train: 0.6375, Val: 0.3900 Test: 0.5100\n",
      "Epoch: 80, Loss: 0.8330, | Train: 0.6513, Val: 0.3800 Test: 0.4600\n",
      "Epoch: 81, Loss: 0.8196, | Train: 0.6600, Val: 0.4000 Test: 0.4800\n",
      "Epoch: 82, Loss: 0.8215, | Train: 0.6525, Val: 0.4100 Test: 0.4700\n",
      "Epoch: 83, Loss: 0.8212, | Train: 0.6662, Val: 0.4200 Test: 0.4900\n",
      "Epoch: 84, Loss: 0.8126, | Train: 0.6662, Val: 0.4000 Test: 0.4700\n",
      "Epoch: 85, Loss: 0.8155, | Train: 0.6637, Val: 0.4000 Test: 0.4900\n",
      "Epoch: 86, Loss: 0.8114, | Train: 0.6637, Val: 0.4000 Test: 0.5000\n",
      "Epoch: 87, Loss: 0.8076, | Train: 0.6675, Val: 0.3700 Test: 0.4900\n",
      "Epoch: 88, Loss: 0.7921, | Train: 0.6525, Val: 0.3700 Test: 0.5200\n",
      "Epoch: 89, Loss: 0.7970, | Train: 0.6562, Val: 0.4000 Test: 0.4900\n",
      "Epoch: 90, Loss: 0.7972, | Train: 0.6600, Val: 0.4100 Test: 0.4800\n",
      "Epoch: 91, Loss: 0.7944, | Train: 0.6737, Val: 0.4100 Test: 0.4600\n",
      "Epoch: 92, Loss: 0.7996, | Train: 0.6675, Val: 0.4000 Test: 0.5000\n",
      "Epoch: 93, Loss: 0.7838, | Train: 0.6750, Val: 0.4200 Test: 0.4700\n",
      "Epoch: 94, Loss: 0.7784, | Train: 0.6712, Val: 0.3800 Test: 0.5000\n",
      "Epoch: 95, Loss: 0.7794, | Train: 0.6725, Val: 0.3600 Test: 0.5000\n",
      "Epoch: 96, Loss: 0.7549, | Train: 0.6737, Val: 0.3900 Test: 0.5000\n",
      "Epoch: 97, Loss: 0.7795, | Train: 0.6800, Val: 0.3900 Test: 0.4900\n",
      "Epoch: 98, Loss: 0.7920, | Train: 0.6825, Val: 0.3800 Test: 0.4700\n",
      "Epoch: 99, Loss: 0.7760, | Train: 0.6812, Val: 0.4000 Test: 0.5000\n",
      "Epoch: 100, Loss: 0.7620, | Train: 0.6762, Val: 0.3900 Test: 0.5000\n",
      "Epoch: 101, Loss: 0.7572, | Train: 0.6875, Val: 0.4100 Test: 0.4800\n",
      "Epoch: 102, Loss: 0.7655, | Train: 0.6875, Val: 0.4000 Test: 0.4700\n",
      "Epoch: 103, Loss: 0.7583, | Train: 0.6912, Val: 0.4400 Test: 0.4500\n",
      "Epoch: 104, Loss: 0.7493, | Train: 0.6812, Val: 0.4400 Test: 0.4600\n",
      "Epoch: 105, Loss: 0.7585, | Train: 0.6900, Val: 0.4100 Test: 0.4600\n",
      "Epoch: 106, Loss: 0.7266, | Train: 0.6900, Val: 0.4100 Test: 0.4600\n",
      "Epoch: 107, Loss: 0.7462, | Train: 0.6837, Val: 0.4100 Test: 0.4700\n",
      "Epoch: 108, Loss: 0.7564, | Train: 0.6912, Val: 0.4400 Test: 0.4400\n",
      "Epoch: 109, Loss: 0.7359, | Train: 0.6962, Val: 0.3800 Test: 0.4600\n",
      "Epoch: 110, Loss: 0.7392, | Train: 0.6975, Val: 0.4400 Test: 0.4500\n",
      "Epoch: 111, Loss: 0.7476, | Train: 0.6987, Val: 0.4200 Test: 0.4300\n",
      "Epoch: 112, Loss: 0.7179, | Train: 0.7000, Val: 0.4200 Test: 0.4600\n",
      "Epoch: 113, Loss: 0.7201, | Train: 0.7025, Val: 0.4000 Test: 0.4700\n",
      "Epoch: 114, Loss: 0.7359, | Train: 0.7075, Val: 0.4100 Test: 0.4900\n",
      "Epoch: 115, Loss: 0.7385, | Train: 0.7012, Val: 0.3800 Test: 0.4800\n",
      "Epoch: 116, Loss: 0.7439, | Train: 0.7138, Val: 0.4500 Test: 0.4500\n",
      "Epoch: 117, Loss: 0.7276, | Train: 0.7125, Val: 0.4500 Test: 0.4200\n",
      "Epoch: 118, Loss: 0.7128, | Train: 0.7163, Val: 0.4200 Test: 0.4600\n",
      "Epoch: 119, Loss: 0.7002, | Train: 0.7075, Val: 0.4400 Test: 0.4600\n",
      "Epoch: 120, Loss: 0.7278, | Train: 0.7175, Val: 0.4200 Test: 0.4300\n",
      "Epoch: 121, Loss: 0.6918, | Train: 0.7163, Val: 0.4300 Test: 0.4400\n",
      "Epoch: 122, Loss: 0.7057, | Train: 0.7125, Val: 0.4200 Test: 0.4500\n",
      "Epoch: 123, Loss: 0.7254, | Train: 0.7163, Val: 0.4200 Test: 0.5000\n",
      "Epoch: 124, Loss: 0.7025, | Train: 0.7200, Val: 0.4300 Test: 0.4900\n",
      "Epoch: 125, Loss: 0.6990, | Train: 0.7200, Val: 0.4300 Test: 0.4600\n",
      "Epoch: 126, Loss: 0.6897, | Train: 0.7237, Val: 0.4100 Test: 0.4600\n",
      "Epoch: 127, Loss: 0.7045, | Train: 0.7262, Val: 0.4100 Test: 0.4300\n",
      "Epoch: 128, Loss: 0.6957, | Train: 0.7300, Val: 0.4200 Test: 0.4400\n",
      "Epoch: 129, Loss: 0.7008, | Train: 0.7312, Val: 0.4300 Test: 0.4600\n",
      "Epoch: 130, Loss: 0.6920, | Train: 0.7312, Val: 0.4300 Test: 0.4700\n",
      "Epoch: 131, Loss: 0.6836, | Train: 0.7287, Val: 0.4200 Test: 0.4600\n",
      "Epoch: 132, Loss: 0.6792, | Train: 0.7312, Val: 0.4300 Test: 0.4700\n",
      "Epoch: 133, Loss: 0.6876, | Train: 0.7287, Val: 0.4100 Test: 0.4800\n",
      "Epoch: 134, Loss: 0.6911, | Train: 0.7312, Val: 0.4400 Test: 0.4600\n",
      "Epoch: 135, Loss: 0.6756, | Train: 0.7387, Val: 0.4300 Test: 0.4600\n",
      "Epoch: 136, Loss: 0.6696, | Train: 0.7437, Val: 0.4200 Test: 0.4600\n",
      "Epoch: 137, Loss: 0.6757, | Train: 0.7475, Val: 0.4400 Test: 0.4500\n",
      "Epoch: 138, Loss: 0.6680, | Train: 0.7550, Val: 0.4600 Test: 0.4600\n",
      "Epoch: 139, Loss: 0.6604, | Train: 0.7575, Val: 0.4600 Test: 0.4600\n",
      "Epoch: 140, Loss: 0.6721, | Train: 0.7500, Val: 0.4400 Test: 0.4600\n",
      "Epoch: 141, Loss: 0.6615, | Train: 0.7487, Val: 0.4300 Test: 0.4600\n",
      "Epoch: 142, Loss: 0.6825, | Train: 0.7512, Val: 0.4400 Test: 0.4600\n",
      "Epoch: 143, Loss: 0.6495, | Train: 0.7412, Val: 0.4500 Test: 0.4700\n",
      "Epoch: 144, Loss: 0.6478, | Train: 0.7562, Val: 0.4600 Test: 0.4600\n",
      "Epoch: 145, Loss: 0.6531, | Train: 0.7512, Val: 0.4800 Test: 0.4700\n",
      "Epoch: 146, Loss: 0.6429, | Train: 0.7525, Val: 0.4800 Test: 0.4600\n",
      "Epoch: 147, Loss: 0.6572, | Train: 0.7525, Val: 0.4500 Test: 0.4400\n",
      "Epoch: 148, Loss: 0.6528, | Train: 0.7525, Val: 0.4400 Test: 0.4300\n",
      "Epoch: 149, Loss: 0.6540, | Train: 0.7575, Val: 0.4500 Test: 0.4500\n",
      "Epoch: 150, Loss: 0.6583, | Train: 0.7600, Val: 0.4300 Test: 0.4200\n",
      "Epoch: 151, Loss: 0.6536, | Train: 0.7562, Val: 0.4400 Test: 0.4400\n",
      "Epoch: 152, Loss: 0.6429, | Train: 0.7587, Val: 0.4600 Test: 0.4800\n",
      "Epoch: 153, Loss: 0.6509, | Train: 0.7537, Val: 0.4600 Test: 0.5000\n",
      "Epoch: 154, Loss: 0.6418, | Train: 0.7500, Val: 0.4300 Test: 0.4600\n",
      "Epoch: 155, Loss: 0.6296, | Train: 0.7662, Val: 0.4700 Test: 0.4600\n",
      "Epoch: 156, Loss: 0.6340, | Train: 0.7600, Val: 0.4700 Test: 0.4600\n",
      "Epoch: 157, Loss: 0.6390, | Train: 0.7650, Val: 0.4500 Test: 0.4300\n",
      "Epoch: 158, Loss: 0.6306, | Train: 0.7637, Val: 0.4500 Test: 0.4300\n",
      "Epoch: 159, Loss: 0.6236, | Train: 0.7763, Val: 0.4600 Test: 0.4400\n",
      "Epoch: 160, Loss: 0.6146, | Train: 0.7700, Val: 0.4500 Test: 0.4300\n",
      "Epoch: 161, Loss: 0.6256, | Train: 0.7650, Val: 0.4500 Test: 0.4500\n",
      "Epoch: 162, Loss: 0.6269, | Train: 0.7738, Val: 0.4500 Test: 0.4500\n",
      "Epoch: 163, Loss: 0.6493, | Train: 0.7763, Val: 0.4400 Test: 0.4200\n",
      "Epoch: 164, Loss: 0.6431, | Train: 0.7763, Val: 0.4400 Test: 0.4000\n",
      "Epoch: 165, Loss: 0.6282, | Train: 0.7850, Val: 0.4600 Test: 0.4000\n",
      "Epoch: 166, Loss: 0.6328, | Train: 0.7788, Val: 0.4700 Test: 0.4100\n",
      "Epoch: 167, Loss: 0.6231, | Train: 0.7800, Val: 0.4700 Test: 0.4100\n",
      "Epoch: 168, Loss: 0.6464, | Train: 0.7937, Val: 0.4600 Test: 0.3900\n",
      "Epoch: 169, Loss: 0.6279, | Train: 0.7837, Val: 0.4600 Test: 0.4000\n",
      "Epoch: 170, Loss: 0.6183, | Train: 0.7812, Val: 0.4500 Test: 0.4100\n",
      "Epoch: 171, Loss: 0.6299, | Train: 0.7812, Val: 0.4400 Test: 0.4200\n",
      "Epoch: 172, Loss: 0.6473, | Train: 0.7887, Val: 0.4500 Test: 0.4300\n",
      "Epoch: 173, Loss: 0.6269, | Train: 0.7912, Val: 0.4300 Test: 0.4100\n",
      "Epoch: 174, Loss: 0.6209, | Train: 0.7837, Val: 0.4300 Test: 0.4000\n",
      "Epoch: 175, Loss: 0.6122, | Train: 0.7900, Val: 0.4100 Test: 0.4100\n",
      "Epoch: 176, Loss: 0.5917, | Train: 0.7912, Val: 0.3800 Test: 0.4200\n",
      "Epoch: 177, Loss: 0.6254, | Train: 0.7862, Val: 0.4200 Test: 0.4300\n",
      "Epoch: 178, Loss: 0.6049, | Train: 0.7850, Val: 0.4000 Test: 0.4100\n",
      "Epoch: 179, Loss: 0.5971, | Train: 0.7788, Val: 0.3900 Test: 0.4600\n",
      "Epoch: 180, Loss: 0.6038, | Train: 0.7850, Val: 0.4000 Test: 0.4600\n",
      "Epoch: 181, Loss: 0.5759, | Train: 0.7900, Val: 0.4500 Test: 0.4200\n",
      "Epoch: 182, Loss: 0.6097, | Train: 0.7900, Val: 0.4600 Test: 0.4300\n",
      "Epoch: 183, Loss: 0.5822, | Train: 0.8062, Val: 0.4300 Test: 0.4300\n",
      "Epoch: 184, Loss: 0.6025, | Train: 0.7962, Val: 0.4300 Test: 0.3900\n",
      "Epoch: 185, Loss: 0.6156, | Train: 0.7900, Val: 0.4200 Test: 0.4000\n",
      "Epoch: 186, Loss: 0.5752, | Train: 0.7900, Val: 0.4300 Test: 0.4100\n",
      "Epoch: 187, Loss: 0.5879, | Train: 0.7937, Val: 0.4200 Test: 0.4100\n",
      "Epoch: 188, Loss: 0.5760, | Train: 0.7987, Val: 0.4100 Test: 0.4300\n",
      "Epoch: 189, Loss: 0.5863, | Train: 0.8075, Val: 0.4300 Test: 0.4600\n",
      "Epoch: 190, Loss: 0.5928, | Train: 0.8075, Val: 0.4500 Test: 0.4300\n",
      "Epoch: 191, Loss: 0.5767, | Train: 0.8087, Val: 0.4400 Test: 0.4200\n",
      "Epoch: 192, Loss: 0.5635, | Train: 0.8112, Val: 0.4700 Test: 0.4100\n",
      "Epoch: 193, Loss: 0.5673, | Train: 0.8175, Val: 0.4700 Test: 0.4000\n",
      "Epoch: 194, Loss: 0.5667, | Train: 0.8187, Val: 0.4000 Test: 0.4000\n",
      "Epoch: 195, Loss: 0.5793, | Train: 0.8137, Val: 0.4200 Test: 0.4500\n",
      "Epoch: 196, Loss: 0.5672, | Train: 0.7987, Val: 0.4000 Test: 0.4600\n",
      "Epoch: 197, Loss: 0.5469, | Train: 0.8025, Val: 0.3900 Test: 0.4600\n",
      "Epoch: 198, Loss: 0.5619, | Train: 0.8075, Val: 0.4200 Test: 0.4300\n",
      "Epoch: 199, Loss: 0.5625, | Train: 0.8200, Val: 0.4000 Test: 0.4100\n",
      "Epoch: 200, Loss: 0.5409, | Train: 0.8100, Val: 0.4300 Test: 0.4500\n",
      "Epoch: 201, Loss: 0.5453, | Train: 0.8237, Val: 0.4200 Test: 0.4100\n",
      "Epoch: 202, Loss: 0.5425, | Train: 0.8187, Val: 0.4600 Test: 0.4200\n",
      "Epoch: 203, Loss: 0.5564, | Train: 0.8137, Val: 0.4300 Test: 0.4400\n",
      "Epoch: 204, Loss: 0.5783, | Train: 0.8312, Val: 0.4000 Test: 0.4400\n",
      "Epoch: 205, Loss: 0.5817, | Train: 0.8262, Val: 0.4000 Test: 0.4300\n",
      "Epoch: 206, Loss: 0.5498, | Train: 0.8300, Val: 0.4400 Test: 0.4300\n",
      "Epoch: 207, Loss: 0.5495, | Train: 0.8225, Val: 0.4500 Test: 0.4400\n",
      "Epoch: 208, Loss: 0.5565, | Train: 0.8287, Val: 0.4300 Test: 0.4200\n",
      "Epoch: 209, Loss: 0.5726, | Train: 0.8200, Val: 0.3800 Test: 0.4400\n",
      "Epoch: 210, Loss: 0.5403, | Train: 0.8388, Val: 0.3900 Test: 0.4400\n",
      "Epoch: 211, Loss: 0.5727, | Train: 0.8300, Val: 0.4000 Test: 0.4400\n",
      "Epoch: 212, Loss: 0.5840, | Train: 0.8212, Val: 0.4000 Test: 0.4200\n",
      "Epoch: 213, Loss: 0.5603, | Train: 0.8237, Val: 0.3900 Test: 0.4100\n",
      "Epoch: 214, Loss: 0.5520, | Train: 0.8275, Val: 0.4000 Test: 0.4100\n",
      "Epoch: 215, Loss: 0.5620, | Train: 0.8325, Val: 0.4100 Test: 0.4200\n",
      "Epoch: 216, Loss: 0.5766, | Train: 0.8413, Val: 0.4200 Test: 0.4300\n",
      "Epoch: 217, Loss: 0.5629, | Train: 0.8338, Val: 0.4200 Test: 0.4300\n",
      "Epoch: 218, Loss: 0.5419, | Train: 0.8350, Val: 0.4400 Test: 0.4300\n",
      "Epoch: 219, Loss: 0.5534, | Train: 0.8300, Val: 0.4200 Test: 0.4400\n",
      "Epoch: 220, Loss: 0.5412, | Train: 0.8500, Val: 0.4200 Test: 0.4300\n",
      "Epoch: 221, Loss: 0.5446, | Train: 0.8413, Val: 0.4100 Test: 0.4200\n",
      "Epoch: 222, Loss: 0.5366, | Train: 0.8400, Val: 0.4000 Test: 0.4400\n",
      "Epoch: 223, Loss: 0.5395, | Train: 0.8312, Val: 0.4000 Test: 0.4400\n",
      "Epoch: 224, Loss: 0.5370, | Train: 0.8325, Val: 0.4100 Test: 0.4400\n",
      "Epoch: 225, Loss: 0.5597, | Train: 0.8413, Val: 0.4400 Test: 0.4400\n",
      "Epoch: 226, Loss: 0.5286, | Train: 0.8512, Val: 0.4200 Test: 0.4300\n",
      "Epoch: 227, Loss: 0.5346, | Train: 0.8562, Val: 0.4200 Test: 0.4400\n",
      "Epoch: 228, Loss: 0.5493, | Train: 0.8500, Val: 0.4100 Test: 0.4600\n",
      "Epoch: 229, Loss: 0.5623, | Train: 0.8375, Val: 0.4000 Test: 0.4600\n",
      "Epoch: 230, Loss: 0.5691, | Train: 0.8487, Val: 0.3900 Test: 0.4500\n",
      "Epoch: 231, Loss: 0.5459, | Train: 0.8400, Val: 0.4000 Test: 0.4500\n",
      "Epoch: 232, Loss: 0.5428, | Train: 0.8475, Val: 0.4100 Test: 0.4400\n",
      "Epoch: 233, Loss: 0.5193, | Train: 0.8438, Val: 0.4300 Test: 0.4300\n",
      "Epoch: 234, Loss: 0.5165, | Train: 0.8287, Val: 0.4000 Test: 0.4200\n",
      "Epoch: 235, Loss: 0.5408, | Train: 0.8475, Val: 0.4200 Test: 0.4400\n",
      "Epoch: 236, Loss: 0.5242, | Train: 0.8550, Val: 0.4100 Test: 0.4500\n",
      "Epoch: 237, Loss: 0.5577, | Train: 0.8487, Val: 0.4000 Test: 0.4300\n",
      "Epoch: 238, Loss: 0.5327, | Train: 0.8512, Val: 0.3900 Test: 0.4400\n",
      "Epoch: 239, Loss: 0.5327, | Train: 0.8425, Val: 0.4000 Test: 0.4400\n",
      "Epoch: 240, Loss: 0.5276, | Train: 0.8425, Val: 0.4200 Test: 0.4400\n",
      "Epoch: 241, Loss: 0.5154, | Train: 0.8537, Val: 0.4100 Test: 0.4300\n",
      "Epoch: 242, Loss: 0.5300, | Train: 0.8687, Val: 0.4200 Test: 0.4400\n",
      "Epoch: 243, Loss: 0.5053, | Train: 0.8575, Val: 0.4100 Test: 0.4500\n",
      "Epoch: 244, Loss: 0.5380, | Train: 0.8475, Val: 0.3900 Test: 0.4400\n",
      "Epoch: 245, Loss: 0.5194, | Train: 0.8375, Val: 0.4000 Test: 0.4700\n",
      "Epoch: 246, Loss: 0.5363, | Train: 0.8512, Val: 0.4100 Test: 0.4500\n",
      "Epoch: 247, Loss: 0.5163, | Train: 0.8550, Val: 0.4100 Test: 0.4300\n",
      "Epoch: 248, Loss: 0.5144, | Train: 0.8525, Val: 0.4000 Test: 0.4500\n",
      "Epoch: 249, Loss: 0.4974, | Train: 0.8575, Val: 0.4000 Test: 0.4400\n",
      "Epoch: 250, Loss: 0.5769, | Train: 0.8425, Val: 0.4200 Test: 0.4300\n",
      "Epoch: 251, Loss: 0.5071, | Train: 0.8525, Val: 0.4300 Test: 0.4200\n",
      "Epoch: 252, Loss: 0.5229, | Train: 0.8600, Val: 0.4100 Test: 0.4500\n",
      "Epoch: 253, Loss: 0.5287, | Train: 0.8512, Val: 0.4000 Test: 0.4600\n",
      "Epoch: 254, Loss: 0.5138, | Train: 0.8537, Val: 0.4100 Test: 0.4700\n",
      "Epoch: 255, Loss: 0.4990, | Train: 0.8587, Val: 0.3900 Test: 0.4600\n",
      "Epoch: 256, Loss: 0.5090, | Train: 0.8525, Val: 0.4000 Test: 0.4600\n",
      "Epoch: 257, Loss: 0.4899, | Train: 0.8562, Val: 0.4000 Test: 0.4600\n",
      "Epoch: 258, Loss: 0.5387, | Train: 0.8575, Val: 0.3900 Test: 0.4600\n",
      "Epoch: 259, Loss: 0.5135, | Train: 0.8712, Val: 0.4100 Test: 0.4700\n",
      "Epoch: 260, Loss: 0.4944, | Train: 0.8662, Val: 0.4100 Test: 0.4600\n",
      "Epoch: 261, Loss: 0.4851, | Train: 0.8712, Val: 0.4000 Test: 0.4500\n",
      "Epoch: 262, Loss: 0.4836, | Train: 0.8700, Val: 0.4000 Test: 0.4500\n",
      "Epoch: 263, Loss: 0.4601, | Train: 0.8662, Val: 0.3800 Test: 0.4500\n",
      "Epoch: 264, Loss: 0.5106, | Train: 0.8600, Val: 0.3900 Test: 0.4400\n",
      "Epoch: 265, Loss: 0.4917, | Train: 0.8662, Val: 0.4000 Test: 0.4500\n",
      "Epoch: 266, Loss: 0.5110, | Train: 0.8687, Val: 0.4000 Test: 0.4500\n",
      "Epoch: 267, Loss: 0.4949, | Train: 0.8737, Val: 0.4000 Test: 0.4500\n",
      "Epoch: 268, Loss: 0.4853, | Train: 0.8762, Val: 0.3900 Test: 0.4400\n",
      "Epoch: 269, Loss: 0.4936, | Train: 0.8712, Val: 0.4100 Test: 0.4300\n",
      "Epoch: 270, Loss: 0.4734, | Train: 0.8662, Val: 0.4100 Test: 0.4400\n",
      "Epoch: 271, Loss: 0.5218, | Train: 0.8712, Val: 0.3800 Test: 0.4300\n",
      "Epoch: 272, Loss: 0.4966, | Train: 0.8787, Val: 0.4100 Test: 0.4400\n",
      "Epoch: 273, Loss: 0.4856, | Train: 0.8762, Val: 0.4000 Test: 0.4500\n",
      "Epoch: 274, Loss: 0.4762, | Train: 0.8812, Val: 0.4100 Test: 0.4500\n",
      "Epoch: 275, Loss: 0.4833, | Train: 0.8712, Val: 0.3900 Test: 0.4500\n",
      "Epoch: 276, Loss: 0.4968, | Train: 0.8650, Val: 0.4000 Test: 0.4600\n",
      "Epoch: 277, Loss: 0.4803, | Train: 0.8637, Val: 0.3900 Test: 0.4400\n",
      "Epoch: 278, Loss: 0.5007, | Train: 0.8750, Val: 0.4100 Test: 0.4400\n",
      "Epoch: 279, Loss: 0.4762, | Train: 0.8712, Val: 0.3900 Test: 0.4500\n",
      "Epoch: 280, Loss: 0.5101, | Train: 0.8762, Val: 0.4100 Test: 0.4300\n",
      "Epoch: 281, Loss: 0.4830, | Train: 0.8712, Val: 0.4200 Test: 0.4400\n",
      "Epoch: 282, Loss: 0.5073, | Train: 0.8750, Val: 0.4200 Test: 0.4400\n",
      "Epoch: 283, Loss: 0.4907, | Train: 0.8625, Val: 0.4100 Test: 0.4400\n",
      "Epoch: 284, Loss: 0.4801, | Train: 0.8775, Val: 0.4300 Test: 0.4500\n",
      "Epoch: 285, Loss: 0.4681, | Train: 0.8737, Val: 0.4300 Test: 0.4400\n",
      "Epoch: 286, Loss: 0.4751, | Train: 0.8737, Val: 0.4300 Test: 0.4400\n",
      "Epoch: 287, Loss: 0.4515, | Train: 0.8737, Val: 0.4100 Test: 0.4300\n",
      "Epoch: 288, Loss: 0.4597, | Train: 0.8750, Val: 0.4200 Test: 0.4300\n",
      "Epoch: 289, Loss: 0.4813, | Train: 0.8762, Val: 0.4300 Test: 0.4100\n",
      "Epoch: 290, Loss: 0.4723, | Train: 0.8775, Val: 0.4400 Test: 0.4100\n",
      "Epoch: 291, Loss: 0.4763, | Train: 0.8775, Val: 0.4200 Test: 0.4300\n",
      "Epoch: 292, Loss: 0.4748, | Train: 0.8687, Val: 0.4300 Test: 0.4300\n",
      "Epoch: 293, Loss: 0.4871, | Train: 0.8762, Val: 0.4100 Test: 0.4200\n",
      "Epoch: 294, Loss: 0.4554, | Train: 0.8862, Val: 0.4100 Test: 0.4300\n",
      "Epoch: 295, Loss: 0.4805, | Train: 0.8887, Val: 0.4000 Test: 0.4400\n",
      "Epoch: 296, Loss: 0.4595, | Train: 0.8912, Val: 0.4100 Test: 0.4400\n",
      "Epoch: 297, Loss: 0.4403, | Train: 0.8825, Val: 0.4400 Test: 0.4200\n",
      "Epoch: 298, Loss: 0.4573, | Train: 0.8775, Val: 0.4300 Test: 0.4300\n",
      "Epoch: 299, Loss: 0.4664, | Train: 0.8687, Val: 0.4200 Test: 0.4300\n",
      "Epoch: 300, Loss: 0.4379, | Train: 0.8787, Val: 0.4200 Test: 0.4500\n",
      "Epoch: 301, Loss: 0.4798, | Train: 0.8837, Val: 0.4000 Test: 0.4200\n",
      "Epoch: 302, Loss: 0.4597, | Train: 0.8812, Val: 0.4000 Test: 0.4100\n",
      "Epoch: 303, Loss: 0.4554, | Train: 0.8800, Val: 0.4300 Test: 0.4300\n",
      "Epoch: 304, Loss: 0.4539, | Train: 0.8775, Val: 0.4500 Test: 0.4500\n",
      "Epoch: 305, Loss: 0.4180, | Train: 0.8887, Val: 0.4400 Test: 0.4400\n",
      "Epoch: 306, Loss: 0.4405, | Train: 0.8925, Val: 0.4400 Test: 0.4100\n",
      "Epoch: 307, Loss: 0.4742, | Train: 0.8950, Val: 0.4400 Test: 0.4300\n",
      "Epoch: 308, Loss: 0.4285, | Train: 0.8912, Val: 0.4500 Test: 0.4300\n",
      "Epoch: 309, Loss: 0.4351, | Train: 0.8887, Val: 0.4600 Test: 0.4300\n",
      "Epoch: 310, Loss: 0.4588, | Train: 0.8875, Val: 0.4600 Test: 0.4400\n",
      "Epoch: 311, Loss: 0.4396, | Train: 0.8975, Val: 0.4600 Test: 0.4200\n",
      "Epoch: 312, Loss: 0.4732, | Train: 0.8925, Val: 0.4300 Test: 0.4400\n",
      "Epoch: 313, Loss: 0.4332, | Train: 0.8963, Val: 0.4200 Test: 0.4200\n",
      "Epoch: 314, Loss: 0.4614, | Train: 0.8912, Val: 0.4200 Test: 0.4200\n",
      "Epoch: 315, Loss: 0.4384, | Train: 0.8937, Val: 0.4300 Test: 0.4100\n",
      "Epoch: 316, Loss: 0.4320, | Train: 0.8912, Val: 0.4300 Test: 0.4400\n",
      "Epoch: 317, Loss: 0.4496, | Train: 0.8950, Val: 0.4400 Test: 0.4400\n",
      "Epoch: 318, Loss: 0.4439, | Train: 0.8937, Val: 0.4500 Test: 0.4200\n",
      "Epoch: 319, Loss: 0.4621, | Train: 0.8975, Val: 0.4300 Test: 0.4200\n",
      "Epoch: 320, Loss: 0.4534, | Train: 0.8875, Val: 0.4200 Test: 0.4300\n",
      "Epoch: 321, Loss: 0.4385, | Train: 0.8837, Val: 0.4100 Test: 0.4400\n",
      "Epoch: 322, Loss: 0.4437, | Train: 0.9000, Val: 0.4200 Test: 0.4100\n",
      "Epoch: 323, Loss: 0.4330, | Train: 0.8925, Val: 0.4300 Test: 0.4200\n",
      "Epoch: 324, Loss: 0.4526, | Train: 0.8950, Val: 0.4500 Test: 0.4600\n",
      "Epoch: 325, Loss: 0.4340, | Train: 0.8837, Val: 0.4400 Test: 0.4600\n",
      "Epoch: 326, Loss: 0.4219, | Train: 0.8963, Val: 0.4500 Test: 0.4300\n",
      "Epoch: 327, Loss: 0.4370, | Train: 0.8950, Val: 0.4200 Test: 0.4200\n",
      "Epoch: 328, Loss: 0.4346, | Train: 0.8862, Val: 0.4400 Test: 0.4300\n",
      "Epoch: 329, Loss: 0.4243, | Train: 0.8875, Val: 0.4400 Test: 0.4100\n",
      "Epoch: 330, Loss: 0.4189, | Train: 0.8912, Val: 0.4500 Test: 0.4200\n",
      "Epoch: 331, Loss: 0.4603, | Train: 0.8975, Val: 0.4400 Test: 0.4200\n",
      "Epoch: 332, Loss: 0.4638, | Train: 0.8887, Val: 0.4300 Test: 0.4200\n",
      "Epoch: 333, Loss: 0.4324, | Train: 0.8862, Val: 0.4500 Test: 0.4200\n",
      "Epoch: 334, Loss: 0.4141, | Train: 0.8887, Val: 0.4400 Test: 0.4400\n",
      "Epoch: 335, Loss: 0.4466, | Train: 0.8937, Val: 0.4400 Test: 0.4400\n",
      "Epoch: 336, Loss: 0.4395, | Train: 0.8963, Val: 0.4300 Test: 0.4400\n",
      "Epoch: 337, Loss: 0.4419, | Train: 0.8988, Val: 0.4400 Test: 0.4400\n",
      "Epoch: 338, Loss: 0.4533, | Train: 0.8988, Val: 0.4600 Test: 0.4300\n",
      "Epoch: 339, Loss: 0.4343, | Train: 0.9013, Val: 0.4500 Test: 0.4400\n",
      "Epoch: 340, Loss: 0.4387, | Train: 0.8988, Val: 0.4300 Test: 0.4300\n",
      "Epoch: 341, Loss: 0.4324, | Train: 0.8963, Val: 0.4300 Test: 0.4400\n",
      "Epoch: 342, Loss: 0.4585, | Train: 0.9125, Val: 0.4300 Test: 0.4500\n",
      "Epoch: 343, Loss: 0.4179, | Train: 0.9087, Val: 0.4400 Test: 0.4400\n",
      "Epoch: 344, Loss: 0.4065, | Train: 0.9000, Val: 0.4500 Test: 0.4500\n",
      "Epoch: 345, Loss: 0.4313, | Train: 0.8988, Val: 0.4400 Test: 0.4500\n",
      "Epoch: 346, Loss: 0.4014, | Train: 0.9038, Val: 0.4300 Test: 0.4500\n",
      "Epoch: 347, Loss: 0.3974, | Train: 0.9062, Val: 0.4200 Test: 0.4500\n",
      "Epoch: 348, Loss: 0.4184, | Train: 0.9087, Val: 0.4200 Test: 0.4200\n",
      "Epoch: 349, Loss: 0.4298, | Train: 0.9075, Val: 0.4300 Test: 0.4400\n",
      "Epoch: 350, Loss: 0.4556, | Train: 0.9087, Val: 0.4100 Test: 0.4100\n",
      "Epoch: 351, Loss: 0.4304, | Train: 0.9112, Val: 0.4200 Test: 0.4300\n",
      "Epoch: 352, Loss: 0.4210, | Train: 0.9112, Val: 0.4400 Test: 0.4300\n",
      "Epoch: 353, Loss: 0.4248, | Train: 0.9100, Val: 0.4400 Test: 0.4300\n",
      "Epoch: 354, Loss: 0.4274, | Train: 0.9100, Val: 0.4200 Test: 0.4300\n",
      "Epoch: 355, Loss: 0.4174, | Train: 0.9013, Val: 0.4300 Test: 0.4500\n",
      "Epoch: 356, Loss: 0.3843, | Train: 0.9112, Val: 0.4400 Test: 0.4300\n",
      "Epoch: 357, Loss: 0.4063, | Train: 0.9025, Val: 0.4300 Test: 0.4300\n",
      "Epoch: 358, Loss: 0.4544, | Train: 0.9075, Val: 0.4300 Test: 0.4400\n",
      "Epoch: 359, Loss: 0.4298, | Train: 0.9062, Val: 0.4200 Test: 0.4300\n",
      "Epoch: 360, Loss: 0.3975, | Train: 0.9150, Val: 0.4100 Test: 0.4400\n",
      "Epoch: 361, Loss: 0.4361, | Train: 0.9137, Val: 0.4400 Test: 0.4600\n",
      "Epoch: 362, Loss: 0.4079, | Train: 0.9125, Val: 0.4300 Test: 0.4600\n",
      "Epoch: 363, Loss: 0.3897, | Train: 0.9013, Val: 0.4200 Test: 0.4400\n",
      "Epoch: 364, Loss: 0.4559, | Train: 0.9087, Val: 0.4300 Test: 0.4400\n",
      "Epoch: 365, Loss: 0.4153, | Train: 0.9125, Val: 0.4300 Test: 0.4400\n",
      "Epoch: 366, Loss: 0.4370, | Train: 0.9162, Val: 0.4100 Test: 0.4500\n",
      "Epoch: 367, Loss: 0.4300, | Train: 0.9062, Val: 0.4100 Test: 0.4300\n",
      "Epoch: 368, Loss: 0.3836, | Train: 0.9125, Val: 0.4300 Test: 0.4200\n",
      "Epoch: 369, Loss: 0.3986, | Train: 0.9137, Val: 0.4200 Test: 0.4300\n",
      "Epoch: 370, Loss: 0.4098, | Train: 0.9125, Val: 0.4400 Test: 0.4400\n",
      "Epoch: 371, Loss: 0.3941, | Train: 0.9112, Val: 0.4400 Test: 0.4600\n",
      "Epoch: 372, Loss: 0.4356, | Train: 0.9125, Val: 0.4400 Test: 0.4800\n",
      "Epoch: 373, Loss: 0.4014, | Train: 0.9175, Val: 0.4100 Test: 0.4900\n",
      "Epoch: 374, Loss: 0.3947, | Train: 0.9187, Val: 0.4500 Test: 0.4700\n",
      "Epoch: 375, Loss: 0.3899, | Train: 0.9237, Val: 0.4300 Test: 0.4200\n",
      "Epoch: 376, Loss: 0.4141, | Train: 0.9150, Val: 0.4400 Test: 0.4200\n",
      "Epoch: 377, Loss: 0.3956, | Train: 0.9162, Val: 0.4500 Test: 0.4200\n",
      "Epoch: 378, Loss: 0.4367, | Train: 0.9112, Val: 0.4400 Test: 0.4300\n",
      "Epoch: 379, Loss: 0.4199, | Train: 0.9150, Val: 0.4400 Test: 0.4400\n",
      "Epoch: 380, Loss: 0.3953, | Train: 0.9125, Val: 0.4500 Test: 0.4100\n",
      "Epoch: 381, Loss: 0.4003, | Train: 0.9112, Val: 0.4400 Test: 0.4400\n",
      "Epoch: 382, Loss: 0.4020, | Train: 0.9050, Val: 0.4400 Test: 0.4300\n",
      "Epoch: 383, Loss: 0.4215, | Train: 0.9137, Val: 0.4400 Test: 0.4500\n",
      "Epoch: 384, Loss: 0.3844, | Train: 0.9237, Val: 0.4500 Test: 0.4400\n",
      "Epoch: 385, Loss: 0.4082, | Train: 0.9212, Val: 0.4600 Test: 0.4500\n",
      "Epoch: 386, Loss: 0.4244, | Train: 0.9237, Val: 0.4400 Test: 0.4600\n",
      "Epoch: 387, Loss: 0.3855, | Train: 0.9200, Val: 0.4200 Test: 0.4400\n",
      "Epoch: 388, Loss: 0.4075, | Train: 0.9137, Val: 0.4200 Test: 0.4400\n",
      "Epoch: 389, Loss: 0.3955, | Train: 0.9212, Val: 0.4400 Test: 0.4300\n",
      "Epoch: 390, Loss: 0.4202, | Train: 0.9162, Val: 0.4500 Test: 0.4400\n",
      "Epoch: 391, Loss: 0.3972, | Train: 0.9212, Val: 0.4300 Test: 0.4300\n",
      "Epoch: 392, Loss: 0.3747, | Train: 0.9112, Val: 0.4200 Test: 0.4500\n",
      "Epoch: 393, Loss: 0.4228, | Train: 0.9200, Val: 0.4500 Test: 0.4500\n",
      "Epoch: 394, Loss: 0.3885, | Train: 0.9200, Val: 0.4500 Test: 0.4600\n",
      "Epoch: 395, Loss: 0.4028, | Train: 0.9150, Val: 0.4400 Test: 0.4700\n",
      "Epoch: 396, Loss: 0.4026, | Train: 0.9137, Val: 0.4200 Test: 0.4700\n",
      "Epoch: 397, Loss: 0.4130, | Train: 0.9000, Val: 0.4400 Test: 0.4600\n",
      "Epoch: 398, Loss: 0.4170, | Train: 0.9112, Val: 0.4600 Test: 0.4400\n",
      "Epoch: 399, Loss: 0.4398, | Train: 0.9075, Val: 0.4200 Test: 0.4200\n",
      "Epoch: 400, Loss: 0.3784, | Train: 0.9175, Val: 0.4400 Test: 0.4300\n",
      "Epoch: 401, Loss: 0.4136, | Train: 0.9062, Val: 0.4400 Test: 0.4500\n",
      "Epoch: 402, Loss: 0.4220, | Train: 0.9050, Val: 0.4300 Test: 0.4300\n",
      "Epoch: 403, Loss: 0.4092, | Train: 0.9225, Val: 0.4500 Test: 0.4400\n",
      "Epoch: 404, Loss: 0.3898, | Train: 0.9112, Val: 0.4400 Test: 0.4300\n",
      "Epoch: 405, Loss: 0.3833, | Train: 0.9200, Val: 0.4400 Test: 0.4600\n",
      "Epoch: 406, Loss: 0.3626, | Train: 0.9250, Val: 0.4400 Test: 0.4600\n",
      "Epoch: 407, Loss: 0.3820, | Train: 0.9212, Val: 0.4400 Test: 0.4500\n",
      "Epoch: 408, Loss: 0.3818, | Train: 0.9162, Val: 0.4600 Test: 0.4400\n",
      "Epoch: 409, Loss: 0.3817, | Train: 0.9237, Val: 0.4500 Test: 0.4500\n",
      "Epoch: 410, Loss: 0.3881, | Train: 0.9237, Val: 0.4200 Test: 0.4500\n",
      "Epoch: 411, Loss: 0.3680, | Train: 0.9175, Val: 0.4100 Test: 0.4500\n",
      "Epoch: 412, Loss: 0.3951, | Train: 0.9162, Val: 0.4100 Test: 0.4500\n",
      "Epoch: 413, Loss: 0.3795, | Train: 0.9200, Val: 0.4100 Test: 0.4500\n",
      "Epoch: 414, Loss: 0.3759, | Train: 0.9312, Val: 0.4300 Test: 0.4500\n",
      "Epoch: 415, Loss: 0.3901, | Train: 0.9262, Val: 0.4400 Test: 0.4400\n",
      "Epoch: 416, Loss: 0.3847, | Train: 0.9300, Val: 0.4100 Test: 0.4500\n",
      "Epoch: 417, Loss: 0.3859, | Train: 0.9300, Val: 0.4100 Test: 0.4500\n",
      "Epoch: 418, Loss: 0.3522, | Train: 0.9200, Val: 0.4300 Test: 0.4400\n",
      "Epoch: 419, Loss: 0.3738, | Train: 0.9187, Val: 0.4400 Test: 0.4400\n",
      "Epoch: 420, Loss: 0.3494, | Train: 0.9200, Val: 0.4600 Test: 0.4500\n",
      "Epoch: 421, Loss: 0.3713, | Train: 0.9200, Val: 0.4400 Test: 0.4400\n",
      "Epoch: 422, Loss: 0.3701, | Train: 0.9175, Val: 0.4400 Test: 0.4500\n",
      "Epoch: 423, Loss: 0.3969, | Train: 0.9262, Val: 0.4500 Test: 0.4400\n",
      "Epoch: 424, Loss: 0.3594, | Train: 0.9325, Val: 0.4100 Test: 0.4400\n",
      "Epoch: 425, Loss: 0.3648, | Train: 0.9300, Val: 0.4200 Test: 0.4300\n",
      "Epoch: 426, Loss: 0.3696, | Train: 0.9175, Val: 0.4200 Test: 0.4600\n",
      "Epoch: 427, Loss: 0.3781, | Train: 0.9200, Val: 0.4300 Test: 0.4400\n",
      "Epoch: 428, Loss: 0.3793, | Train: 0.9225, Val: 0.4400 Test: 0.4400\n",
      "Epoch: 429, Loss: 0.3684, | Train: 0.9237, Val: 0.4500 Test: 0.4500\n",
      "Epoch: 430, Loss: 0.3717, | Train: 0.9262, Val: 0.4500 Test: 0.4700\n",
      "Epoch: 431, Loss: 0.3547, | Train: 0.9275, Val: 0.4600 Test: 0.4500\n",
      "Epoch: 432, Loss: 0.3696, | Train: 0.9350, Val: 0.4300 Test: 0.4700\n",
      "Epoch: 433, Loss: 0.3693, | Train: 0.9337, Val: 0.4400 Test: 0.4600\n",
      "Epoch: 434, Loss: 0.3926, | Train: 0.9287, Val: 0.4500 Test: 0.4300\n",
      "Epoch: 435, Loss: 0.3845, | Train: 0.9087, Val: 0.4500 Test: 0.4300\n",
      "Epoch: 436, Loss: 0.3682, | Train: 0.9200, Val: 0.4400 Test: 0.4300\n",
      "Epoch: 437, Loss: 0.3868, | Train: 0.9362, Val: 0.4500 Test: 0.4500\n",
      "Epoch: 438, Loss: 0.3658, | Train: 0.9250, Val: 0.4400 Test: 0.4500\n",
      "Epoch: 439, Loss: 0.3862, | Train: 0.9300, Val: 0.4500 Test: 0.4500\n",
      "Epoch: 440, Loss: 0.3784, | Train: 0.9175, Val: 0.4300 Test: 0.4500\n",
      "Epoch: 441, Loss: 0.3686, | Train: 0.9187, Val: 0.4300 Test: 0.4500\n",
      "Epoch: 442, Loss: 0.3771, | Train: 0.9312, Val: 0.4400 Test: 0.4400\n",
      "Epoch: 443, Loss: 0.3910, | Train: 0.9325, Val: 0.4400 Test: 0.4500\n",
      "Epoch: 444, Loss: 0.3786, | Train: 0.9312, Val: 0.4200 Test: 0.4500\n",
      "Epoch: 445, Loss: 0.3632, | Train: 0.9212, Val: 0.4300 Test: 0.4400\n",
      "Epoch: 446, Loss: 0.3804, | Train: 0.9250, Val: 0.4500 Test: 0.4400\n",
      "Epoch: 447, Loss: 0.3623, | Train: 0.9337, Val: 0.4400 Test: 0.4600\n",
      "Epoch: 448, Loss: 0.3446, | Train: 0.9300, Val: 0.4400 Test: 0.4500\n",
      "Epoch: 449, Loss: 0.4180, | Train: 0.9337, Val: 0.4300 Test: 0.4600\n",
      "Epoch: 450, Loss: 0.3788, | Train: 0.9312, Val: 0.4400 Test: 0.4500\n",
      "Epoch: 451, Loss: 0.3682, | Train: 0.9250, Val: 0.4200 Test: 0.4200\n",
      "Epoch: 452, Loss: 0.3511, | Train: 0.9250, Val: 0.4000 Test: 0.4300\n",
      "Epoch: 453, Loss: 0.3650, | Train: 0.9337, Val: 0.4500 Test: 0.4600\n",
      "Epoch: 454, Loss: 0.3619, | Train: 0.9325, Val: 0.4400 Test: 0.4600\n",
      "Epoch: 455, Loss: 0.3315, | Train: 0.9375, Val: 0.4500 Test: 0.4500\n",
      "Epoch: 456, Loss: 0.3435, | Train: 0.9300, Val: 0.4200 Test: 0.4500\n",
      "Epoch: 457, Loss: 0.3552, | Train: 0.9300, Val: 0.4100 Test: 0.4500\n",
      "Epoch: 458, Loss: 0.4025, | Train: 0.9387, Val: 0.4300 Test: 0.4400\n",
      "Epoch: 459, Loss: 0.3446, | Train: 0.9425, Val: 0.4500 Test: 0.4400\n",
      "Epoch: 460, Loss: 0.3580, | Train: 0.9387, Val: 0.4400 Test: 0.4300\n",
      "Epoch: 461, Loss: 0.3650, | Train: 0.9350, Val: 0.4300 Test: 0.4400\n",
      "Epoch: 462, Loss: 0.3748, | Train: 0.9362, Val: 0.4100 Test: 0.4400\n",
      "Epoch: 463, Loss: 0.3548, | Train: 0.9312, Val: 0.4400 Test: 0.4300\n",
      "Epoch: 464, Loss: 0.3569, | Train: 0.9300, Val: 0.4300 Test: 0.4200\n",
      "Epoch: 465, Loss: 0.3614, | Train: 0.9375, Val: 0.4500 Test: 0.4500\n",
      "Epoch: 466, Loss: 0.3533, | Train: 0.9350, Val: 0.4500 Test: 0.4600\n",
      "Epoch: 467, Loss: 0.3861, | Train: 0.9287, Val: 0.4600 Test: 0.4500\n",
      "Epoch: 468, Loss: 0.3629, | Train: 0.9362, Val: 0.4500 Test: 0.4300\n",
      "Epoch: 469, Loss: 0.3500, | Train: 0.9337, Val: 0.4300 Test: 0.4200\n",
      "Epoch: 470, Loss: 0.3576, | Train: 0.9375, Val: 0.4000 Test: 0.4300\n",
      "Epoch: 471, Loss: 0.3824, | Train: 0.9300, Val: 0.4100 Test: 0.4400\n",
      "Epoch: 472, Loss: 0.3563, | Train: 0.9375, Val: 0.4300 Test: 0.4400\n",
      "Epoch: 473, Loss: 0.3572, | Train: 0.9425, Val: 0.4300 Test: 0.4500\n",
      "Epoch: 474, Loss: 0.3230, | Train: 0.9387, Val: 0.4200 Test: 0.4300\n",
      "Epoch: 475, Loss: 0.3384, | Train: 0.9412, Val: 0.4400 Test: 0.4300\n",
      "Epoch: 476, Loss: 0.3686, | Train: 0.9300, Val: 0.4100 Test: 0.4300\n",
      "Epoch: 477, Loss: 0.3851, | Train: 0.9375, Val: 0.4200 Test: 0.4300\n",
      "Epoch: 478, Loss: 0.3484, | Train: 0.9412, Val: 0.4100 Test: 0.4300\n",
      "Epoch: 479, Loss: 0.3774, | Train: 0.9387, Val: 0.4300 Test: 0.4500\n",
      "Epoch: 480, Loss: 0.3401, | Train: 0.9387, Val: 0.4400 Test: 0.4600\n",
      "Epoch: 481, Loss: 0.3437, | Train: 0.9425, Val: 0.4300 Test: 0.4600\n",
      "Epoch: 482, Loss: 0.3666, | Train: 0.9375, Val: 0.4300 Test: 0.4600\n",
      "Epoch: 483, Loss: 0.3421, | Train: 0.9437, Val: 0.4100 Test: 0.4700\n",
      "Epoch: 484, Loss: 0.3322, | Train: 0.9425, Val: 0.4000 Test: 0.4700\n",
      "Epoch: 485, Loss: 0.3275, | Train: 0.9425, Val: 0.3900 Test: 0.4500\n",
      "Epoch: 486, Loss: 0.3700, | Train: 0.9412, Val: 0.4000 Test: 0.4500\n",
      "Epoch: 487, Loss: 0.3230, | Train: 0.9412, Val: 0.4000 Test: 0.4500\n",
      "Epoch: 488, Loss: 0.3677, | Train: 0.9337, Val: 0.4000 Test: 0.4500\n",
      "Epoch: 489, Loss: 0.3621, | Train: 0.9412, Val: 0.4000 Test: 0.4500\n",
      "Epoch: 490, Loss: 0.3255, | Train: 0.9462, Val: 0.4000 Test: 0.4600\n",
      "Epoch: 491, Loss: 0.3239, | Train: 0.9450, Val: 0.3900 Test: 0.4600\n",
      "Epoch: 492, Loss: 0.3646, | Train: 0.9400, Val: 0.4000 Test: 0.4500\n",
      "Epoch: 493, Loss: 0.3318, | Train: 0.9362, Val: 0.4100 Test: 0.4500\n",
      "Epoch: 494, Loss: 0.3437, | Train: 0.9362, Val: 0.4000 Test: 0.4400\n",
      "Epoch: 495, Loss: 0.3703, | Train: 0.9375, Val: 0.4000 Test: 0.4600\n",
      "Epoch: 496, Loss: 0.3200, | Train: 0.9362, Val: 0.4200 Test: 0.4700\n",
      "Epoch: 497, Loss: 0.3452, | Train: 0.9325, Val: 0.4200 Test: 0.4600\n",
      "Epoch: 498, Loss: 0.3389, | Train: 0.9387, Val: 0.4100 Test: 0.4500\n",
      "Epoch: 499, Loss: 0.3366, | Train: 0.9412, Val: 0.4100 Test: 0.4500\n",
      "Epoch: 500, Loss: 0.3276, | Train: 0.9400, Val: 0.4100 Test: 0.4700\n",
      "Epoch: 501, Loss: 0.3221, | Train: 0.9437, Val: 0.4300 Test: 0.4800\n",
      "Epoch: 502, Loss: 0.3229, | Train: 0.9475, Val: 0.4400 Test: 0.4700\n",
      "Epoch: 503, Loss: 0.3060, | Train: 0.9400, Val: 0.4500 Test: 0.4500\n",
      "Epoch: 504, Loss: 0.3381, | Train: 0.9375, Val: 0.4400 Test: 0.4400\n",
      "Epoch: 505, Loss: 0.3570, | Train: 0.9437, Val: 0.4300 Test: 0.4700\n",
      "Epoch: 506, Loss: 0.3388, | Train: 0.9500, Val: 0.4200 Test: 0.4700\n",
      "Epoch: 507, Loss: 0.3401, | Train: 0.9462, Val: 0.4200 Test: 0.4700\n",
      "Epoch: 508, Loss: 0.3637, | Train: 0.9400, Val: 0.4100 Test: 0.4600\n",
      "Epoch: 509, Loss: 0.3480, | Train: 0.9437, Val: 0.4200 Test: 0.4700\n",
      "Epoch: 510, Loss: 0.3044, | Train: 0.9475, Val: 0.4200 Test: 0.4800\n",
      "Epoch: 511, Loss: 0.3248, | Train: 0.9462, Val: 0.4100 Test: 0.4700\n",
      "Epoch: 512, Loss: 0.3329, | Train: 0.9462, Val: 0.4300 Test: 0.4700\n",
      "Epoch: 513, Loss: 0.3460, | Train: 0.9437, Val: 0.4200 Test: 0.4600\n",
      "Epoch: 514, Loss: 0.3605, | Train: 0.9412, Val: 0.4200 Test: 0.4600\n",
      "Epoch: 515, Loss: 0.3535, | Train: 0.9475, Val: 0.4100 Test: 0.4600\n",
      "Epoch: 516, Loss: 0.3425, | Train: 0.9450, Val: 0.4000 Test: 0.4700\n",
      "Epoch: 517, Loss: 0.3139, | Train: 0.9437, Val: 0.4300 Test: 0.4700\n",
      "Epoch: 518, Loss: 0.3197, | Train: 0.9437, Val: 0.4400 Test: 0.4800\n",
      "Epoch: 519, Loss: 0.3587, | Train: 0.9462, Val: 0.4300 Test: 0.4700\n",
      "Epoch: 520, Loss: 0.3471, | Train: 0.9462, Val: 0.4200 Test: 0.4700\n",
      "Epoch: 521, Loss: 0.3142, | Train: 0.9425, Val: 0.4300 Test: 0.4600\n",
      "Epoch: 522, Loss: 0.3090, | Train: 0.9425, Val: 0.4300 Test: 0.4700\n",
      "Epoch: 523, Loss: 0.3228, | Train: 0.9387, Val: 0.4300 Test: 0.4700\n",
      "Epoch: 524, Loss: 0.3344, | Train: 0.9437, Val: 0.4300 Test: 0.4600\n",
      "Epoch: 525, Loss: 0.3147, | Train: 0.9437, Val: 0.4400 Test: 0.4600\n",
      "Epoch: 526, Loss: 0.3585, | Train: 0.9462, Val: 0.4300 Test: 0.4600\n",
      "Epoch: 527, Loss: 0.3629, | Train: 0.9462, Val: 0.4200 Test: 0.4700\n",
      "Epoch: 528, Loss: 0.3423, | Train: 0.9537, Val: 0.4200 Test: 0.4700\n",
      "Epoch: 529, Loss: 0.3349, | Train: 0.9487, Val: 0.4300 Test: 0.4700\n",
      "Epoch: 530, Loss: 0.3217, | Train: 0.9437, Val: 0.4300 Test: 0.4600\n",
      "Epoch: 531, Loss: 0.3623, | Train: 0.9462, Val: 0.4200 Test: 0.4600\n",
      "Epoch: 532, Loss: 0.3293, | Train: 0.9500, Val: 0.4300 Test: 0.4700\n",
      "Epoch: 533, Loss: 0.3196, | Train: 0.9487, Val: 0.4000 Test: 0.4700\n",
      "Epoch: 534, Loss: 0.3392, | Train: 0.9475, Val: 0.4100 Test: 0.4600\n",
      "Epoch: 535, Loss: 0.3265, | Train: 0.9425, Val: 0.4000 Test: 0.4400\n",
      "Epoch: 536, Loss: 0.3332, | Train: 0.9375, Val: 0.4100 Test: 0.4400\n",
      "Epoch: 537, Loss: 0.3447, | Train: 0.9425, Val: 0.4300 Test: 0.4800\n",
      "Epoch: 538, Loss: 0.3074, | Train: 0.9437, Val: 0.4300 Test: 0.4900\n",
      "Epoch: 539, Loss: 0.3207, | Train: 0.9350, Val: 0.4300 Test: 0.4700\n",
      "Epoch: 540, Loss: 0.3516, | Train: 0.9312, Val: 0.4400 Test: 0.4900\n",
      "Epoch: 541, Loss: 0.3511, | Train: 0.9450, Val: 0.4400 Test: 0.4700\n",
      "Epoch: 542, Loss: 0.3295, | Train: 0.9450, Val: 0.4300 Test: 0.4700\n",
      "Epoch: 543, Loss: 0.3258, | Train: 0.9337, Val: 0.4300 Test: 0.4600\n",
      "Epoch: 544, Loss: 0.3383, | Train: 0.9337, Val: 0.4300 Test: 0.4500\n",
      "Epoch: 545, Loss: 0.3551, | Train: 0.9325, Val: 0.4300 Test: 0.4800\n",
      "Epoch: 546, Loss: 0.3402, | Train: 0.9487, Val: 0.4500 Test: 0.4900\n",
      "Epoch: 547, Loss: 0.3455, | Train: 0.9512, Val: 0.4300 Test: 0.4700\n",
      "Epoch: 548, Loss: 0.3186, | Train: 0.9462, Val: 0.4300 Test: 0.4700\n",
      "Epoch: 549, Loss: 0.3469, | Train: 0.9412, Val: 0.4100 Test: 0.4700\n",
      "Epoch: 550, Loss: 0.3311, | Train: 0.9362, Val: 0.4200 Test: 0.4800\n",
      "Epoch: 551, Loss: 0.3622, | Train: 0.9387, Val: 0.4100 Test: 0.4900\n",
      "Epoch: 552, Loss: 0.3672, | Train: 0.9437, Val: 0.4300 Test: 0.4700\n",
      "Epoch: 553, Loss: 0.3617, | Train: 0.9487, Val: 0.4100 Test: 0.4600\n",
      "Epoch: 554, Loss: 0.3095, | Train: 0.9500, Val: 0.4000 Test: 0.4800\n",
      "Epoch: 555, Loss: 0.2926, | Train: 0.9475, Val: 0.4100 Test: 0.4600\n",
      "Epoch: 556, Loss: 0.3221, | Train: 0.9525, Val: 0.4200 Test: 0.4600\n",
      "Epoch: 557, Loss: 0.3371, | Train: 0.9525, Val: 0.4200 Test: 0.4500\n",
      "Epoch: 558, Loss: 0.3197, | Train: 0.9475, Val: 0.4000 Test: 0.4800\n",
      "Epoch: 559, Loss: 0.3495, | Train: 0.9400, Val: 0.4100 Test: 0.4800\n",
      "Epoch: 560, Loss: 0.3138, | Train: 0.9425, Val: 0.3900 Test: 0.4800\n",
      "Epoch: 561, Loss: 0.3437, | Train: 0.9487, Val: 0.4100 Test: 0.4900\n",
      "Epoch: 562, Loss: 0.3341, | Train: 0.9462, Val: 0.4000 Test: 0.4800\n",
      "Epoch: 563, Loss: 0.3603, | Train: 0.9487, Val: 0.4200 Test: 0.4600\n",
      "Epoch: 564, Loss: 0.3329, | Train: 0.9512, Val: 0.4100 Test: 0.4800\n",
      "Epoch: 565, Loss: 0.3498, | Train: 0.9487, Val: 0.4400 Test: 0.4800\n",
      "Epoch: 566, Loss: 0.3220, | Train: 0.9450, Val: 0.4200 Test: 0.4700\n",
      "Epoch: 567, Loss: 0.3523, | Train: 0.9500, Val: 0.3900 Test: 0.4800\n",
      "Epoch: 568, Loss: 0.3419, | Train: 0.9512, Val: 0.4000 Test: 0.4800\n",
      "Epoch: 569, Loss: 0.3299, | Train: 0.9462, Val: 0.4200 Test: 0.4700\n",
      "Epoch: 570, Loss: 0.3382, | Train: 0.9512, Val: 0.4200 Test: 0.4900\n",
      "Epoch: 571, Loss: 0.3185, | Train: 0.9487, Val: 0.4200 Test: 0.4700\n",
      "Epoch: 572, Loss: 0.3426, | Train: 0.9512, Val: 0.4100 Test: 0.4800\n",
      "Epoch: 573, Loss: 0.3492, | Train: 0.9487, Val: 0.4000 Test: 0.5000\n",
      "Epoch: 574, Loss: 0.3271, | Train: 0.9525, Val: 0.4000 Test: 0.4900\n",
      "Epoch: 575, Loss: 0.3401, | Train: 0.9475, Val: 0.4100 Test: 0.4800\n",
      "Epoch: 576, Loss: 0.3210, | Train: 0.9437, Val: 0.3900 Test: 0.4800\n",
      "Epoch: 577, Loss: 0.3323, | Train: 0.9562, Val: 0.4100 Test: 0.4900\n",
      "Epoch: 578, Loss: 0.3190, | Train: 0.9500, Val: 0.4200 Test: 0.4700\n",
      "Epoch: 579, Loss: 0.3123, | Train: 0.9562, Val: 0.4200 Test: 0.4400\n",
      "Epoch: 580, Loss: 0.3255, | Train: 0.9525, Val: 0.3800 Test: 0.4500\n",
      "Epoch: 581, Loss: 0.3152, | Train: 0.9450, Val: 0.3900 Test: 0.4600\n",
      "Epoch: 582, Loss: 0.3127, | Train: 0.9462, Val: 0.4100 Test: 0.4500\n",
      "Epoch: 583, Loss: 0.2962, | Train: 0.9487, Val: 0.4200 Test: 0.4800\n",
      "Epoch: 584, Loss: 0.3222, | Train: 0.9487, Val: 0.4200 Test: 0.4700\n",
      "Epoch: 585, Loss: 0.3060, | Train: 0.9500, Val: 0.4200 Test: 0.4700\n",
      "Epoch: 586, Loss: 0.2837, | Train: 0.9462, Val: 0.3900 Test: 0.4500\n",
      "Epoch: 587, Loss: 0.3192, | Train: 0.9375, Val: 0.3900 Test: 0.4700\n",
      "Epoch: 588, Loss: 0.3074, | Train: 0.9437, Val: 0.4000 Test: 0.4700\n",
      "Epoch: 589, Loss: 0.3169, | Train: 0.9500, Val: 0.4000 Test: 0.4700\n",
      "Epoch: 590, Loss: 0.3142, | Train: 0.9537, Val: 0.4200 Test: 0.4900\n",
      "Epoch: 591, Loss: 0.3021, | Train: 0.9525, Val: 0.4200 Test: 0.4600\n",
      "Epoch: 592, Loss: 0.3155, | Train: 0.9437, Val: 0.4400 Test: 0.4600\n",
      "Epoch: 593, Loss: 0.3181, | Train: 0.9412, Val: 0.4100 Test: 0.4600\n",
      "Epoch: 594, Loss: 0.2865, | Train: 0.9512, Val: 0.4100 Test: 0.4800\n",
      "Epoch: 595, Loss: 0.3161, | Train: 0.9425, Val: 0.4000 Test: 0.4800\n",
      "Epoch: 596, Loss: 0.3352, | Train: 0.9487, Val: 0.3900 Test: 0.4400\n",
      "Epoch: 597, Loss: 0.3240, | Train: 0.9462, Val: 0.4000 Test: 0.4400\n",
      "Epoch: 598, Loss: 0.3898, | Train: 0.9550, Val: 0.4200 Test: 0.4300\n",
      "Epoch: 599, Loss: 0.3333, | Train: 0.9475, Val: 0.4200 Test: 0.4300\n",
      "Epoch: 600, Loss: 0.3393, | Train: 0.9500, Val: 0.4100 Test: 0.4600\n",
      "Epoch: 601, Loss: 0.3627, | Train: 0.9537, Val: 0.4100 Test: 0.4600\n",
      "Epoch: 602, Loss: 0.3486, | Train: 0.9537, Val: 0.3900 Test: 0.4700\n",
      "Epoch: 603, Loss: 0.3145, | Train: 0.9512, Val: 0.4100 Test: 0.4600\n",
      "Epoch: 604, Loss: 0.3382, | Train: 0.9613, Val: 0.4200 Test: 0.4800\n",
      "Epoch: 605, Loss: 0.3107, | Train: 0.9613, Val: 0.4400 Test: 0.4800\n",
      "Epoch: 606, Loss: 0.3372, | Train: 0.9537, Val: 0.4400 Test: 0.4700\n",
      "Epoch: 607, Loss: 0.3329, | Train: 0.9462, Val: 0.4400 Test: 0.4500\n",
      "Epoch: 608, Loss: 0.3219, | Train: 0.9512, Val: 0.4300 Test: 0.4300\n",
      "Epoch: 609, Loss: 0.3204, | Train: 0.9550, Val: 0.4200 Test: 0.4700\n",
      "Epoch: 610, Loss: 0.3528, | Train: 0.9525, Val: 0.4000 Test: 0.4300\n",
      "Epoch: 611, Loss: 0.3206, | Train: 0.9575, Val: 0.4000 Test: 0.4500\n",
      "Epoch: 612, Loss: 0.2924, | Train: 0.9512, Val: 0.4100 Test: 0.4500\n",
      "Epoch: 613, Loss: 0.3376, | Train: 0.9475, Val: 0.4200 Test: 0.4500\n",
      "Epoch: 614, Loss: 0.3040, | Train: 0.9462, Val: 0.4200 Test: 0.4400\n",
      "Epoch: 615, Loss: 0.3099, | Train: 0.9475, Val: 0.4400 Test: 0.4500\n",
      "Epoch: 616, Loss: 0.3258, | Train: 0.9550, Val: 0.4300 Test: 0.4500\n",
      "Epoch: 617, Loss: 0.3405, | Train: 0.9475, Val: 0.4100 Test: 0.4400\n",
      "Epoch: 618, Loss: 0.3227, | Train: 0.9500, Val: 0.4100 Test: 0.4600\n",
      "Epoch: 619, Loss: 0.3298, | Train: 0.9500, Val: 0.4200 Test: 0.4600\n",
      "Epoch: 620, Loss: 0.3357, | Train: 0.9437, Val: 0.4300 Test: 0.4500\n",
      "Epoch: 621, Loss: 0.3162, | Train: 0.9450, Val: 0.4200 Test: 0.4700\n",
      "Epoch: 622, Loss: 0.3157, | Train: 0.9475, Val: 0.4100 Test: 0.4500\n",
      "Epoch: 623, Loss: 0.3261, | Train: 0.9562, Val: 0.3800 Test: 0.4700\n",
      "Epoch: 624, Loss: 0.3186, | Train: 0.9613, Val: 0.4000 Test: 0.4700\n",
      "Epoch: 625, Loss: 0.3185, | Train: 0.9562, Val: 0.3900 Test: 0.4900\n",
      "Epoch: 626, Loss: 0.2987, | Train: 0.9525, Val: 0.3800 Test: 0.4700\n",
      "Epoch: 627, Loss: 0.3191, | Train: 0.9613, Val: 0.4100 Test: 0.4800\n",
      "Epoch: 628, Loss: 0.3322, | Train: 0.9625, Val: 0.4000 Test: 0.5000\n",
      "Epoch: 629, Loss: 0.3393, | Train: 0.9487, Val: 0.4000 Test: 0.5000\n",
      "Epoch: 630, Loss: 0.3169, | Train: 0.9562, Val: 0.4100 Test: 0.5000\n",
      "Epoch: 631, Loss: 0.3014, | Train: 0.9663, Val: 0.4100 Test: 0.4900\n",
      "Epoch: 632, Loss: 0.3285, | Train: 0.9663, Val: 0.3900 Test: 0.5000\n",
      "Epoch: 633, Loss: 0.3314, | Train: 0.9675, Val: 0.3900 Test: 0.5000\n",
      "Epoch: 634, Loss: 0.2883, | Train: 0.9650, Val: 0.4000 Test: 0.4800\n",
      "Epoch: 635, Loss: 0.3076, | Train: 0.9587, Val: 0.4000 Test: 0.4600\n",
      "Epoch: 636, Loss: 0.3093, | Train: 0.9537, Val: 0.4000 Test: 0.4700\n",
      "Epoch: 637, Loss: 0.2968, | Train: 0.9613, Val: 0.4000 Test: 0.5000\n",
      "Epoch: 638, Loss: 0.2993, | Train: 0.9487, Val: 0.3900 Test: 0.4900\n",
      "Epoch: 639, Loss: 0.3150, | Train: 0.9537, Val: 0.4000 Test: 0.5000\n",
      "Epoch: 640, Loss: 0.3176, | Train: 0.9613, Val: 0.4100 Test: 0.5000\n",
      "Epoch: 641, Loss: 0.3010, | Train: 0.9587, Val: 0.4000 Test: 0.4900\n",
      "Epoch: 642, Loss: 0.3025, | Train: 0.9575, Val: 0.4000 Test: 0.4900\n",
      "Epoch: 643, Loss: 0.2990, | Train: 0.9650, Val: 0.4100 Test: 0.4800\n",
      "Epoch: 644, Loss: 0.2744, | Train: 0.9638, Val: 0.4000 Test: 0.4800\n",
      "Epoch: 645, Loss: 0.2689, | Train: 0.9650, Val: 0.3900 Test: 0.4800\n",
      "Epoch: 646, Loss: 0.3000, | Train: 0.9600, Val: 0.4000 Test: 0.5100\n",
      "Epoch: 647, Loss: 0.2998, | Train: 0.9550, Val: 0.4100 Test: 0.4900\n",
      "Epoch: 648, Loss: 0.2880, | Train: 0.9600, Val: 0.4200 Test: 0.5000\n",
      "Epoch: 649, Loss: 0.3096, | Train: 0.9600, Val: 0.4100 Test: 0.5100\n",
      "Epoch: 650, Loss: 0.2810, | Train: 0.9638, Val: 0.4100 Test: 0.4900\n",
      "Epoch: 651, Loss: 0.2972, | Train: 0.9663, Val: 0.4300 Test: 0.4800\n",
      "Epoch: 652, Loss: 0.2822, | Train: 0.9613, Val: 0.4300 Test: 0.4700\n",
      "Epoch: 653, Loss: 0.3040, | Train: 0.9575, Val: 0.4100 Test: 0.4700\n",
      "Epoch: 654, Loss: 0.2836, | Train: 0.9575, Val: 0.4300 Test: 0.4700\n",
      "Epoch: 655, Loss: 0.2729, | Train: 0.9587, Val: 0.4200 Test: 0.4900\n",
      "Epoch: 656, Loss: 0.2726, | Train: 0.9625, Val: 0.4400 Test: 0.4900\n",
      "Epoch: 657, Loss: 0.3122, | Train: 0.9638, Val: 0.4200 Test: 0.4700\n",
      "Epoch: 658, Loss: 0.2781, | Train: 0.9475, Val: 0.4200 Test: 0.4500\n",
      "Epoch: 659, Loss: 0.3151, | Train: 0.9562, Val: 0.4400 Test: 0.4600\n",
      "Epoch: 660, Loss: 0.2991, | Train: 0.9613, Val: 0.4100 Test: 0.4700\n",
      "Epoch: 661, Loss: 0.3098, | Train: 0.9537, Val: 0.4000 Test: 0.4700\n",
      "Epoch: 662, Loss: 0.2984, | Train: 0.9575, Val: 0.4000 Test: 0.4500\n",
      "Epoch: 663, Loss: 0.3127, | Train: 0.9625, Val: 0.4000 Test: 0.4600\n",
      "Epoch: 664, Loss: 0.3015, | Train: 0.9562, Val: 0.3800 Test: 0.4700\n",
      "Epoch: 665, Loss: 0.2611, | Train: 0.9587, Val: 0.3900 Test: 0.4800\n",
      "Epoch: 666, Loss: 0.2884, | Train: 0.9600, Val: 0.4200 Test: 0.4900\n",
      "Epoch: 667, Loss: 0.3088, | Train: 0.9575, Val: 0.4300 Test: 0.4800\n",
      "Epoch: 668, Loss: 0.2889, | Train: 0.9575, Val: 0.4000 Test: 0.4600\n",
      "Epoch: 669, Loss: 0.2746, | Train: 0.9575, Val: 0.3600 Test: 0.4700\n",
      "Epoch: 670, Loss: 0.3003, | Train: 0.9625, Val: 0.4000 Test: 0.4600\n",
      "Epoch: 671, Loss: 0.3260, | Train: 0.9613, Val: 0.3800 Test: 0.4800\n",
      "Epoch: 672, Loss: 0.2789, | Train: 0.9587, Val: 0.3900 Test: 0.4600\n",
      "Epoch: 673, Loss: 0.2738, | Train: 0.9638, Val: 0.3900 Test: 0.4800\n",
      "Epoch: 674, Loss: 0.2756, | Train: 0.9562, Val: 0.3800 Test: 0.4600\n",
      "Epoch: 675, Loss: 0.3172, | Train: 0.9550, Val: 0.3900 Test: 0.4700\n",
      "Epoch: 676, Loss: 0.2978, | Train: 0.9600, Val: 0.3600 Test: 0.4700\n",
      "Epoch: 677, Loss: 0.2805, | Train: 0.9638, Val: 0.4100 Test: 0.4400\n",
      "Epoch: 678, Loss: 0.3200, | Train: 0.9712, Val: 0.4000 Test: 0.4600\n",
      "Epoch: 679, Loss: 0.3337, | Train: 0.9600, Val: 0.3700 Test: 0.4600\n",
      "Epoch: 680, Loss: 0.2638, | Train: 0.9600, Val: 0.3800 Test: 0.4600\n",
      "Epoch: 681, Loss: 0.2835, | Train: 0.9688, Val: 0.3900 Test: 0.4800\n",
      "Epoch: 682, Loss: 0.2994, | Train: 0.9575, Val: 0.3800 Test: 0.4600\n",
      "Epoch: 683, Loss: 0.3176, | Train: 0.9638, Val: 0.4300 Test: 0.4600\n",
      "Epoch: 684, Loss: 0.2800, | Train: 0.9587, Val: 0.4300 Test: 0.4700\n",
      "Epoch: 685, Loss: 0.3163, | Train: 0.9600, Val: 0.4000 Test: 0.4800\n",
      "Epoch: 686, Loss: 0.3049, | Train: 0.9675, Val: 0.4000 Test: 0.4700\n",
      "Epoch: 687, Loss: 0.2961, | Train: 0.9675, Val: 0.4300 Test: 0.4800\n",
      "Epoch: 688, Loss: 0.3032, | Train: 0.9638, Val: 0.4200 Test: 0.4800\n",
      "Epoch: 689, Loss: 0.3012, | Train: 0.9638, Val: 0.4000 Test: 0.4600\n",
      "Epoch: 690, Loss: 0.2996, | Train: 0.9650, Val: 0.4000 Test: 0.4600\n",
      "Epoch: 691, Loss: 0.3012, | Train: 0.9613, Val: 0.3900 Test: 0.4400\n",
      "Epoch: 692, Loss: 0.2963, | Train: 0.9688, Val: 0.4200 Test: 0.4500\n",
      "Epoch: 693, Loss: 0.2869, | Train: 0.9712, Val: 0.3900 Test: 0.4600\n",
      "Epoch: 694, Loss: 0.2845, | Train: 0.9650, Val: 0.3800 Test: 0.4600\n",
      "Epoch: 695, Loss: 0.3193, | Train: 0.9650, Val: 0.4100 Test: 0.4800\n",
      "Epoch: 696, Loss: 0.3035, | Train: 0.9650, Val: 0.4000 Test: 0.4800\n",
      "Epoch: 697, Loss: 0.3209, | Train: 0.9613, Val: 0.3800 Test: 0.4800\n",
      "Epoch: 698, Loss: 0.2970, | Train: 0.9525, Val: 0.3800 Test: 0.4800\n",
      "Epoch: 699, Loss: 0.2797, | Train: 0.9650, Val: 0.4000 Test: 0.4700\n",
      "Epoch: 700, Loss: 0.2964, | Train: 0.9650, Val: 0.3800 Test: 0.4800\n",
      "Epoch: 701, Loss: 0.2811, | Train: 0.9663, Val: 0.3700 Test: 0.4600\n",
      "Epoch: 702, Loss: 0.3062, | Train: 0.9663, Val: 0.3800 Test: 0.4500\n",
      "Epoch: 703, Loss: 0.3097, | Train: 0.9650, Val: 0.3700 Test: 0.4700\n",
      "Epoch: 704, Loss: 0.2989, | Train: 0.9525, Val: 0.3700 Test: 0.4700\n",
      "Epoch: 705, Loss: 0.3094, | Train: 0.9638, Val: 0.3800 Test: 0.4800\n",
      "Epoch: 706, Loss: 0.2831, | Train: 0.9675, Val: 0.3700 Test: 0.4800\n",
      "Epoch: 707, Loss: 0.3165, | Train: 0.9675, Val: 0.3900 Test: 0.4700\n",
      "Epoch: 708, Loss: 0.2672, | Train: 0.9638, Val: 0.4100 Test: 0.4800\n",
      "Epoch: 709, Loss: 0.2870, | Train: 0.9575, Val: 0.4100 Test: 0.4800\n",
      "Epoch: 710, Loss: 0.3010, | Train: 0.9587, Val: 0.3900 Test: 0.4800\n",
      "Epoch: 711, Loss: 0.3101, | Train: 0.9613, Val: 0.4000 Test: 0.4900\n",
      "Epoch: 712, Loss: 0.2782, | Train: 0.9613, Val: 0.3800 Test: 0.4800\n",
      "Epoch: 713, Loss: 0.3244, | Train: 0.9675, Val: 0.4000 Test: 0.4600\n",
      "Epoch: 714, Loss: 0.2968, | Train: 0.9650, Val: 0.3800 Test: 0.4700\n",
      "Epoch: 715, Loss: 0.3113, | Train: 0.9663, Val: 0.4000 Test: 0.4700\n",
      "Epoch: 716, Loss: 0.2819, | Train: 0.9663, Val: 0.4200 Test: 0.4800\n",
      "Epoch: 717, Loss: 0.2621, | Train: 0.9638, Val: 0.4100 Test: 0.4800\n",
      "Epoch: 718, Loss: 0.2683, | Train: 0.9638, Val: 0.4200 Test: 0.4900\n",
      "Epoch: 719, Loss: 0.3099, | Train: 0.9613, Val: 0.3900 Test: 0.4700\n",
      "Epoch: 720, Loss: 0.2681, | Train: 0.9625, Val: 0.3500 Test: 0.4600\n",
      "Epoch: 721, Loss: 0.2870, | Train: 0.9625, Val: 0.3600 Test: 0.4600\n",
      "Epoch: 722, Loss: 0.2737, | Train: 0.9613, Val: 0.3900 Test: 0.4500\n",
      "Epoch: 723, Loss: 0.3052, | Train: 0.9663, Val: 0.3700 Test: 0.4600\n",
      "Epoch: 724, Loss: 0.2852, | Train: 0.9712, Val: 0.3700 Test: 0.4500\n",
      "Epoch: 725, Loss: 0.2909, | Train: 0.9650, Val: 0.3900 Test: 0.4300\n",
      "Epoch: 726, Loss: 0.2532, | Train: 0.9625, Val: 0.3700 Test: 0.4600\n",
      "Epoch: 727, Loss: 0.2666, | Train: 0.9688, Val: 0.3900 Test: 0.4700\n",
      "Epoch: 728, Loss: 0.2644, | Train: 0.9663, Val: 0.4000 Test: 0.5000\n",
      "Epoch: 729, Loss: 0.3025, | Train: 0.9663, Val: 0.4300 Test: 0.5000\n",
      "Epoch: 730, Loss: 0.3104, | Train: 0.9688, Val: 0.4100 Test: 0.4700\n",
      "Epoch: 731, Loss: 0.2691, | Train: 0.9650, Val: 0.3900 Test: 0.4700\n",
      "Epoch: 732, Loss: 0.2925, | Train: 0.9650, Val: 0.3800 Test: 0.4700\n",
      "Epoch: 733, Loss: 0.2915, | Train: 0.9650, Val: 0.3900 Test: 0.4700\n",
      "Epoch: 734, Loss: 0.3029, | Train: 0.9663, Val: 0.3800 Test: 0.4800\n",
      "Epoch: 735, Loss: 0.2884, | Train: 0.9688, Val: 0.3800 Test: 0.4700\n",
      "Epoch: 736, Loss: 0.3026, | Train: 0.9737, Val: 0.3800 Test: 0.4900\n",
      "Epoch: 737, Loss: 0.2694, | Train: 0.9700, Val: 0.4100 Test: 0.4800\n",
      "Epoch: 738, Loss: 0.2606, | Train: 0.9700, Val: 0.4000 Test: 0.4900\n",
      "Epoch: 739, Loss: 0.2994, | Train: 0.9625, Val: 0.4100 Test: 0.4900\n",
      "Epoch: 740, Loss: 0.2765, | Train: 0.9650, Val: 0.3700 Test: 0.4800\n",
      "Epoch: 741, Loss: 0.3112, | Train: 0.9575, Val: 0.3700 Test: 0.4700\n",
      "Epoch: 742, Loss: 0.3133, | Train: 0.9650, Val: 0.3800 Test: 0.4600\n",
      "Epoch: 743, Loss: 0.2584, | Train: 0.9688, Val: 0.4000 Test: 0.4900\n",
      "Epoch: 744, Loss: 0.2778, | Train: 0.9675, Val: 0.4000 Test: 0.4900\n",
      "Epoch: 745, Loss: 0.3126, | Train: 0.9663, Val: 0.4100 Test: 0.4800\n",
      "Epoch: 746, Loss: 0.2915, | Train: 0.9638, Val: 0.4100 Test: 0.4900\n",
      "Epoch: 747, Loss: 0.2503, | Train: 0.9562, Val: 0.4000 Test: 0.4800\n",
      "Epoch: 748, Loss: 0.3066, | Train: 0.9675, Val: 0.3900 Test: 0.4800\n",
      "Epoch: 749, Loss: 0.2815, | Train: 0.9638, Val: 0.3800 Test: 0.5100\n",
      "Epoch: 750, Loss: 0.3139, | Train: 0.9688, Val: 0.3700 Test: 0.5000\n",
      "Epoch: 751, Loss: 0.2780, | Train: 0.9638, Val: 0.3600 Test: 0.4700\n",
      "Epoch: 752, Loss: 0.2664, | Train: 0.9600, Val: 0.3800 Test: 0.4700\n",
      "Epoch: 753, Loss: 0.3104, | Train: 0.9663, Val: 0.3500 Test: 0.4600\n",
      "Epoch: 754, Loss: 0.2696, | Train: 0.9663, Val: 0.3700 Test: 0.4600\n",
      "Epoch: 755, Loss: 0.2643, | Train: 0.9650, Val: 0.3500 Test: 0.4800\n",
      "Epoch: 756, Loss: 0.2779, | Train: 0.9663, Val: 0.3800 Test: 0.4600\n",
      "Epoch: 757, Loss: 0.2583, | Train: 0.9650, Val: 0.3700 Test: 0.4800\n",
      "Epoch: 758, Loss: 0.2632, | Train: 0.9675, Val: 0.4000 Test: 0.4800\n",
      "Epoch: 759, Loss: 0.2817, | Train: 0.9700, Val: 0.3900 Test: 0.4800\n",
      "Epoch: 760, Loss: 0.2950, | Train: 0.9638, Val: 0.3900 Test: 0.4800\n",
      "Epoch: 761, Loss: 0.2814, | Train: 0.9638, Val: 0.3900 Test: 0.4900\n",
      "Epoch: 762, Loss: 0.2627, | Train: 0.9600, Val: 0.4000 Test: 0.4900\n",
      "Epoch: 763, Loss: 0.2844, | Train: 0.9638, Val: 0.3900 Test: 0.4800\n",
      "Epoch: 764, Loss: 0.2614, | Train: 0.9688, Val: 0.4200 Test: 0.4900\n",
      "Epoch: 765, Loss: 0.2622, | Train: 0.9663, Val: 0.4200 Test: 0.4900\n",
      "Epoch: 766, Loss: 0.2865, | Train: 0.9675, Val: 0.3800 Test: 0.5000\n",
      "Epoch: 767, Loss: 0.2772, | Train: 0.9700, Val: 0.3800 Test: 0.5000\n",
      "Epoch: 768, Loss: 0.2915, | Train: 0.9725, Val: 0.3900 Test: 0.4900\n",
      "Epoch: 769, Loss: 0.2517, | Train: 0.9688, Val: 0.4300 Test: 0.4800\n",
      "Epoch: 770, Loss: 0.2651, | Train: 0.9650, Val: 0.4300 Test: 0.4800\n",
      "Epoch: 771, Loss: 0.2946, | Train: 0.9650, Val: 0.4200 Test: 0.4800\n",
      "Epoch: 772, Loss: 0.2788, | Train: 0.9700, Val: 0.4000 Test: 0.4900\n",
      "Epoch: 773, Loss: 0.2605, | Train: 0.9663, Val: 0.3700 Test: 0.4900\n",
      "Epoch: 774, Loss: 0.2872, | Train: 0.9700, Val: 0.4000 Test: 0.5000\n",
      "Epoch: 775, Loss: 0.2828, | Train: 0.9675, Val: 0.4100 Test: 0.4900\n",
      "Epoch: 776, Loss: 0.2716, | Train: 0.9675, Val: 0.4100 Test: 0.4900\n",
      "Epoch: 777, Loss: 0.2609, | Train: 0.9675, Val: 0.4200 Test: 0.4600\n",
      "Epoch: 778, Loss: 0.2700, | Train: 0.9650, Val: 0.4000 Test: 0.4700\n",
      "Epoch: 779, Loss: 0.2889, | Train: 0.9725, Val: 0.4000 Test: 0.4700\n",
      "Epoch: 780, Loss: 0.2848, | Train: 0.9638, Val: 0.4200 Test: 0.4800\n",
      "Epoch: 781, Loss: 0.2744, | Train: 0.9650, Val: 0.3800 Test: 0.5000\n",
      "Epoch: 782, Loss: 0.3063, | Train: 0.9688, Val: 0.3900 Test: 0.5000\n",
      "Epoch: 783, Loss: 0.2695, | Train: 0.9688, Val: 0.4000 Test: 0.4800\n",
      "Epoch: 784, Loss: 0.2348, | Train: 0.9712, Val: 0.3900 Test: 0.4800\n",
      "Epoch: 785, Loss: 0.2721, | Train: 0.9712, Val: 0.3900 Test: 0.4900\n",
      "Epoch: 786, Loss: 0.3163, | Train: 0.9737, Val: 0.4100 Test: 0.4800\n",
      "Epoch: 787, Loss: 0.2646, | Train: 0.9675, Val: 0.4100 Test: 0.4800\n",
      "Epoch: 788, Loss: 0.2631, | Train: 0.9650, Val: 0.4000 Test: 0.4700\n",
      "Epoch: 789, Loss: 0.2807, | Train: 0.9700, Val: 0.3900 Test: 0.4800\n",
      "Epoch: 790, Loss: 0.2661, | Train: 0.9712, Val: 0.3800 Test: 0.4700\n",
      "Epoch: 791, Loss: 0.2810, | Train: 0.9700, Val: 0.3800 Test: 0.4800\n",
      "Epoch: 792, Loss: 0.2657, | Train: 0.9700, Val: 0.3900 Test: 0.4700\n",
      "Epoch: 793, Loss: 0.2625, | Train: 0.9712, Val: 0.3900 Test: 0.4600\n",
      "Epoch: 794, Loss: 0.2521, | Train: 0.9712, Val: 0.3900 Test: 0.4600\n",
      "Epoch: 795, Loss: 0.2771, | Train: 0.9725, Val: 0.3900 Test: 0.4500\n",
      "Epoch: 796, Loss: 0.2495, | Train: 0.9663, Val: 0.4000 Test: 0.4700\n",
      "Epoch: 797, Loss: 0.2808, | Train: 0.9688, Val: 0.4200 Test: 0.4700\n",
      "Epoch: 798, Loss: 0.2734, | Train: 0.9712, Val: 0.4200 Test: 0.4700\n",
      "Epoch: 799, Loss: 0.2698, | Train: 0.9700, Val: 0.3800 Test: 0.4700\n",
      "Epoch: 800, Loss: 0.2614, | Train: 0.9712, Val: 0.3800 Test: 0.4700\n",
      "Epoch: 801, Loss: 0.2961, | Train: 0.9675, Val: 0.3900 Test: 0.4900\n",
      "Epoch: 802, Loss: 0.2900, | Train: 0.9712, Val: 0.4100 Test: 0.4900\n",
      "Epoch: 803, Loss: 0.2542, | Train: 0.9737, Val: 0.3900 Test: 0.4700\n",
      "Epoch: 804, Loss: 0.2642, | Train: 0.9712, Val: 0.3900 Test: 0.4700\n",
      "Epoch: 805, Loss: 0.2493, | Train: 0.9725, Val: 0.3800 Test: 0.4700\n",
      "Epoch: 806, Loss: 0.2860, | Train: 0.9675, Val: 0.3900 Test: 0.4700\n",
      "Epoch: 807, Loss: 0.2547, | Train: 0.9737, Val: 0.3800 Test: 0.4700\n",
      "Epoch: 808, Loss: 0.2788, | Train: 0.9725, Val: 0.3700 Test: 0.4700\n",
      "Epoch: 809, Loss: 0.2701, | Train: 0.9750, Val: 0.3600 Test: 0.4700\n",
      "Epoch: 810, Loss: 0.2556, | Train: 0.9762, Val: 0.3900 Test: 0.4600\n",
      "Epoch: 811, Loss: 0.2816, | Train: 0.9737, Val: 0.3900 Test: 0.4700\n",
      "Epoch: 812, Loss: 0.2654, | Train: 0.9663, Val: 0.3700 Test: 0.4700\n",
      "Epoch: 813, Loss: 0.2512, | Train: 0.9712, Val: 0.3900 Test: 0.4800\n",
      "Epoch: 814, Loss: 0.2873, | Train: 0.9750, Val: 0.3800 Test: 0.4800\n",
      "Epoch: 815, Loss: 0.2792, | Train: 0.9775, Val: 0.3700 Test: 0.4700\n",
      "Epoch: 816, Loss: 0.2452, | Train: 0.9775, Val: 0.4000 Test: 0.4700\n",
      "Epoch: 817, Loss: 0.2696, | Train: 0.9775, Val: 0.4100 Test: 0.4600\n",
      "Epoch: 818, Loss: 0.2580, | Train: 0.9737, Val: 0.4000 Test: 0.4800\n",
      "Epoch: 819, Loss: 0.2297, | Train: 0.9750, Val: 0.4000 Test: 0.4700\n",
      "Epoch: 820, Loss: 0.2621, | Train: 0.9700, Val: 0.4200 Test: 0.4900\n",
      "Epoch: 821, Loss: 0.2974, | Train: 0.9688, Val: 0.4100 Test: 0.4900\n",
      "Epoch: 822, Loss: 0.2778, | Train: 0.9700, Val: 0.3800 Test: 0.5100\n",
      "Epoch: 823, Loss: 0.2614, | Train: 0.9675, Val: 0.3800 Test: 0.4700\n",
      "Epoch: 824, Loss: 0.2384, | Train: 0.9700, Val: 0.3800 Test: 0.4800\n",
      "Epoch: 825, Loss: 0.2567, | Train: 0.9688, Val: 0.3900 Test: 0.4700\n",
      "Epoch: 826, Loss: 0.2483, | Train: 0.9688, Val: 0.3800 Test: 0.4700\n",
      "Epoch: 827, Loss: 0.2630, | Train: 0.9725, Val: 0.3800 Test: 0.4700\n",
      "Epoch: 828, Loss: 0.2636, | Train: 0.9775, Val: 0.4100 Test: 0.4800\n",
      "Epoch: 829, Loss: 0.2327, | Train: 0.9750, Val: 0.4000 Test: 0.4800\n",
      "Epoch: 830, Loss: 0.2591, | Train: 0.9725, Val: 0.4000 Test: 0.4800\n",
      "Epoch: 831, Loss: 0.2294, | Train: 0.9750, Val: 0.4100 Test: 0.4800\n",
      "Epoch: 832, Loss: 0.2526, | Train: 0.9762, Val: 0.4100 Test: 0.5000\n",
      "Epoch: 833, Loss: 0.2286, | Train: 0.9750, Val: 0.4000 Test: 0.4800\n",
      "Epoch: 834, Loss: 0.2571, | Train: 0.9800, Val: 0.3900 Test: 0.4800\n",
      "Epoch: 835, Loss: 0.2674, | Train: 0.9775, Val: 0.3900 Test: 0.4900\n",
      "Epoch: 836, Loss: 0.2290, | Train: 0.9737, Val: 0.3800 Test: 0.4900\n",
      "Epoch: 837, Loss: 0.2552, | Train: 0.9725, Val: 0.3600 Test: 0.4900\n",
      "Epoch: 838, Loss: 0.2358, | Train: 0.9700, Val: 0.3700 Test: 0.4900\n",
      "Epoch: 839, Loss: 0.2773, | Train: 0.9737, Val: 0.3800 Test: 0.4900\n",
      "Epoch: 840, Loss: 0.2998, | Train: 0.9712, Val: 0.3900 Test: 0.4800\n",
      "Epoch: 841, Loss: 0.2326, | Train: 0.9700, Val: 0.3900 Test: 0.4700\n",
      "Epoch: 842, Loss: 0.2603, | Train: 0.9700, Val: 0.3800 Test: 0.4800\n",
      "Epoch: 843, Loss: 0.2913, | Train: 0.9737, Val: 0.3700 Test: 0.4900\n",
      "Epoch: 844, Loss: 0.2371, | Train: 0.9650, Val: 0.3600 Test: 0.4800\n",
      "Epoch: 845, Loss: 0.2589, | Train: 0.9712, Val: 0.3700 Test: 0.4800\n",
      "Epoch: 846, Loss: 0.2465, | Train: 0.9688, Val: 0.3900 Test: 0.4800\n",
      "Epoch: 847, Loss: 0.2919, | Train: 0.9725, Val: 0.3900 Test: 0.4800\n",
      "Epoch: 848, Loss: 0.2591, | Train: 0.9712, Val: 0.4100 Test: 0.4900\n",
      "Epoch: 849, Loss: 0.2559, | Train: 0.9688, Val: 0.3900 Test: 0.5100\n",
      "Epoch: 850, Loss: 0.2710, | Train: 0.9737, Val: 0.4000 Test: 0.5000\n",
      "Epoch: 851, Loss: 0.2566, | Train: 0.9787, Val: 0.4100 Test: 0.4800\n",
      "Epoch: 852, Loss: 0.2426, | Train: 0.9725, Val: 0.4000 Test: 0.4800\n",
      "Epoch: 853, Loss: 0.2357, | Train: 0.9675, Val: 0.4000 Test: 0.4600\n",
      "Epoch: 854, Loss: 0.2771, | Train: 0.9750, Val: 0.3900 Test: 0.4700\n",
      "Epoch: 855, Loss: 0.2707, | Train: 0.9750, Val: 0.4100 Test: 0.4900\n",
      "Epoch: 856, Loss: 0.2397, | Train: 0.9787, Val: 0.3800 Test: 0.5000\n",
      "Epoch: 857, Loss: 0.2637, | Train: 0.9688, Val: 0.3800 Test: 0.4700\n",
      "Epoch: 858, Loss: 0.2728, | Train: 0.9663, Val: 0.3600 Test: 0.4600\n",
      "Epoch: 859, Loss: 0.2628, | Train: 0.9750, Val: 0.3800 Test: 0.4800\n",
      "Epoch: 860, Loss: 0.2679, | Train: 0.9688, Val: 0.3700 Test: 0.4900\n",
      "Epoch: 861, Loss: 0.2825, | Train: 0.9663, Val: 0.4000 Test: 0.4800\n",
      "Epoch: 862, Loss: 0.2689, | Train: 0.9712, Val: 0.3900 Test: 0.4800\n",
      "Epoch: 863, Loss: 0.2555, | Train: 0.9712, Val: 0.3900 Test: 0.4800\n",
      "Epoch: 864, Loss: 0.2375, | Train: 0.9762, Val: 0.3700 Test: 0.5000\n",
      "Epoch: 865, Loss: 0.2672, | Train: 0.9800, Val: 0.3700 Test: 0.5000\n",
      "Epoch: 866, Loss: 0.2706, | Train: 0.9775, Val: 0.3700 Test: 0.5000\n",
      "Epoch: 867, Loss: 0.2545, | Train: 0.9800, Val: 0.3800 Test: 0.5000\n",
      "Epoch: 868, Loss: 0.2383, | Train: 0.9775, Val: 0.3800 Test: 0.4900\n",
      "Epoch: 869, Loss: 0.2463, | Train: 0.9688, Val: 0.4000 Test: 0.4800\n",
      "Epoch: 870, Loss: 0.2364, | Train: 0.9750, Val: 0.4100 Test: 0.4700\n",
      "Epoch: 871, Loss: 0.2662, | Train: 0.9787, Val: 0.3900 Test: 0.4900\n",
      "Epoch: 872, Loss: 0.2309, | Train: 0.9737, Val: 0.3800 Test: 0.5000\n",
      "Epoch: 873, Loss: 0.2565, | Train: 0.9737, Val: 0.4000 Test: 0.4700\n",
      "Epoch: 874, Loss: 0.2532, | Train: 0.9750, Val: 0.3900 Test: 0.4700\n",
      "Epoch: 875, Loss: 0.2161, | Train: 0.9750, Val: 0.3800 Test: 0.4900\n",
      "Epoch: 876, Loss: 0.2602, | Train: 0.9725, Val: 0.3900 Test: 0.4900\n",
      "Epoch: 877, Loss: 0.2155, | Train: 0.9750, Val: 0.3800 Test: 0.5000\n",
      "Epoch: 878, Loss: 0.2300, | Train: 0.9725, Val: 0.3800 Test: 0.4900\n",
      "Epoch: 879, Loss: 0.2555, | Train: 0.9750, Val: 0.3600 Test: 0.4800\n",
      "Epoch: 880, Loss: 0.2421, | Train: 0.9712, Val: 0.3600 Test: 0.4800\n",
      "Epoch: 881, Loss: 0.2619, | Train: 0.9750, Val: 0.3700 Test: 0.4800\n",
      "Epoch: 882, Loss: 0.2455, | Train: 0.9737, Val: 0.3500 Test: 0.4800\n",
      "Epoch: 883, Loss: 0.2424, | Train: 0.9750, Val: 0.3600 Test: 0.4900\n",
      "Epoch: 884, Loss: 0.2538, | Train: 0.9812, Val: 0.3700 Test: 0.4800\n",
      "Epoch: 885, Loss: 0.2602, | Train: 0.9750, Val: 0.3600 Test: 0.4900\n",
      "Epoch: 886, Loss: 0.2422, | Train: 0.9762, Val: 0.3700 Test: 0.5000\n",
      "Epoch: 887, Loss: 0.2436, | Train: 0.9787, Val: 0.4100 Test: 0.4900\n",
      "Epoch: 888, Loss: 0.2360, | Train: 0.9775, Val: 0.3700 Test: 0.5000\n",
      "Epoch: 889, Loss: 0.2792, | Train: 0.9800, Val: 0.3700 Test: 0.4800\n",
      "Epoch: 890, Loss: 0.2439, | Train: 0.9787, Val: 0.3700 Test: 0.4800\n",
      "Epoch: 891, Loss: 0.2513, | Train: 0.9725, Val: 0.3700 Test: 0.5000\n",
      "Epoch: 892, Loss: 0.2694, | Train: 0.9663, Val: 0.3800 Test: 0.5000\n",
      "Epoch: 893, Loss: 0.2706, | Train: 0.9725, Val: 0.3700 Test: 0.5100\n",
      "Epoch: 894, Loss: 0.2354, | Train: 0.9750, Val: 0.3600 Test: 0.5000\n",
      "Epoch: 895, Loss: 0.2649, | Train: 0.9750, Val: 0.3700 Test: 0.5000\n",
      "Epoch: 896, Loss: 0.2274, | Train: 0.9762, Val: 0.3800 Test: 0.5100\n",
      "Epoch: 897, Loss: 0.2795, | Train: 0.9787, Val: 0.3800 Test: 0.5100\n",
      "Epoch: 898, Loss: 0.2084, | Train: 0.9762, Val: 0.3800 Test: 0.4600\n",
      "Epoch: 899, Loss: 0.2398, | Train: 0.9762, Val: 0.4000 Test: 0.4700\n",
      "Epoch: 900, Loss: 0.2485, | Train: 0.9762, Val: 0.3900 Test: 0.4800\n",
      "Epoch: 901, Loss: 0.2420, | Train: 0.9762, Val: 0.3900 Test: 0.4800\n",
      "Epoch: 902, Loss: 0.2492, | Train: 0.9750, Val: 0.3800 Test: 0.4800\n",
      "Epoch: 903, Loss: 0.2463, | Train: 0.9762, Val: 0.3900 Test: 0.5000\n",
      "Epoch: 904, Loss: 0.2534, | Train: 0.9787, Val: 0.3900 Test: 0.5000\n",
      "Epoch: 905, Loss: 0.2451, | Train: 0.9750, Val: 0.3800 Test: 0.4900\n",
      "Epoch: 906, Loss: 0.2627, | Train: 0.9750, Val: 0.3700 Test: 0.5100\n",
      "Epoch: 907, Loss: 0.2553, | Train: 0.9737, Val: 0.3600 Test: 0.4900\n",
      "Epoch: 908, Loss: 0.2761, | Train: 0.9762, Val: 0.3500 Test: 0.4800\n",
      "Epoch: 909, Loss: 0.2347, | Train: 0.9737, Val: 0.3500 Test: 0.4800\n",
      "Epoch: 910, Loss: 0.2368, | Train: 0.9712, Val: 0.3600 Test: 0.4800\n",
      "Epoch: 911, Loss: 0.2433, | Train: 0.9737, Val: 0.3700 Test: 0.4800\n",
      "Epoch: 912, Loss: 0.2313, | Train: 0.9787, Val: 0.4200 Test: 0.4900\n",
      "Epoch: 913, Loss: 0.2595, | Train: 0.9737, Val: 0.4200 Test: 0.5000\n",
      "Epoch: 914, Loss: 0.2576, | Train: 0.9750, Val: 0.4100 Test: 0.5000\n",
      "Epoch: 915, Loss: 0.2473, | Train: 0.9775, Val: 0.3900 Test: 0.5000\n",
      "Epoch: 916, Loss: 0.2408, | Train: 0.9762, Val: 0.3900 Test: 0.5000\n",
      "Epoch: 917, Loss: 0.2645, | Train: 0.9750, Val: 0.3800 Test: 0.4900\n",
      "Epoch: 918, Loss: 0.2432, | Train: 0.9712, Val: 0.3600 Test: 0.4900\n",
      "Epoch: 919, Loss: 0.2644, | Train: 0.9737, Val: 0.3400 Test: 0.5000\n",
      "Epoch: 920, Loss: 0.2213, | Train: 0.9787, Val: 0.3600 Test: 0.5000\n",
      "Epoch: 921, Loss: 0.2804, | Train: 0.9787, Val: 0.3700 Test: 0.4900\n",
      "Epoch: 922, Loss: 0.2511, | Train: 0.9800, Val: 0.3700 Test: 0.4800\n",
      "Epoch: 923, Loss: 0.2382, | Train: 0.9812, Val: 0.3900 Test: 0.4500\n",
      "Epoch: 924, Loss: 0.2577, | Train: 0.9800, Val: 0.4100 Test: 0.4600\n",
      "Epoch: 925, Loss: 0.2625, | Train: 0.9787, Val: 0.4000 Test: 0.4700\n",
      "Epoch: 926, Loss: 0.2296, | Train: 0.9800, Val: 0.3800 Test: 0.4800\n",
      "Epoch: 927, Loss: 0.2503, | Train: 0.9762, Val: 0.3700 Test: 0.4900\n",
      "Epoch: 928, Loss: 0.2360, | Train: 0.9762, Val: 0.3700 Test: 0.4900\n",
      "Epoch: 929, Loss: 0.2430, | Train: 0.9787, Val: 0.3700 Test: 0.4900\n",
      "Epoch: 930, Loss: 0.2682, | Train: 0.9762, Val: 0.3600 Test: 0.4600\n",
      "Epoch: 931, Loss: 0.2124, | Train: 0.9725, Val: 0.3700 Test: 0.4500\n",
      "Epoch: 932, Loss: 0.2349, | Train: 0.9737, Val: 0.3700 Test: 0.4700\n",
      "Epoch: 933, Loss: 0.2363, | Train: 0.9775, Val: 0.3700 Test: 0.4700\n",
      "Epoch: 934, Loss: 0.2450, | Train: 0.9850, Val: 0.3800 Test: 0.4600\n",
      "Epoch: 935, Loss: 0.2129, | Train: 0.9812, Val: 0.3400 Test: 0.4600\n",
      "Epoch: 936, Loss: 0.2572, | Train: 0.9800, Val: 0.3500 Test: 0.4700\n",
      "Epoch: 937, Loss: 0.2473, | Train: 0.9800, Val: 0.3500 Test: 0.4600\n",
      "Epoch: 938, Loss: 0.2180, | Train: 0.9825, Val: 0.3900 Test: 0.4400\n",
      "Epoch: 939, Loss: 0.2319, | Train: 0.9750, Val: 0.3800 Test: 0.4700\n",
      "Epoch: 940, Loss: 0.2574, | Train: 0.9750, Val: 0.3900 Test: 0.4800\n",
      "Epoch: 941, Loss: 0.2482, | Train: 0.9762, Val: 0.3800 Test: 0.4600\n",
      "Epoch: 942, Loss: 0.2004, | Train: 0.9712, Val: 0.4100 Test: 0.4600\n",
      "Epoch: 943, Loss: 0.2248, | Train: 0.9750, Val: 0.3700 Test: 0.4600\n",
      "Epoch: 944, Loss: 0.2295, | Train: 0.9800, Val: 0.3900 Test: 0.4700\n",
      "Epoch: 945, Loss: 0.2744, | Train: 0.9775, Val: 0.3900 Test: 0.4600\n",
      "Epoch: 946, Loss: 0.2323, | Train: 0.9775, Val: 0.4000 Test: 0.4800\n",
      "Epoch: 947, Loss: 0.2536, | Train: 0.9787, Val: 0.3900 Test: 0.4700\n",
      "Epoch: 948, Loss: 0.2282, | Train: 0.9775, Val: 0.3800 Test: 0.4700\n",
      "Epoch: 949, Loss: 0.2440, | Train: 0.9737, Val: 0.3700 Test: 0.4700\n",
      "Epoch: 950, Loss: 0.2442, | Train: 0.9787, Val: 0.3800 Test: 0.4600\n",
      "Epoch: 951, Loss: 0.2591, | Train: 0.9762, Val: 0.3800 Test: 0.4800\n",
      "Epoch: 952, Loss: 0.2422, | Train: 0.9837, Val: 0.3800 Test: 0.4900\n",
      "Epoch: 953, Loss: 0.2304, | Train: 0.9787, Val: 0.3800 Test: 0.4800\n",
      "Epoch: 954, Loss: 0.2391, | Train: 0.9712, Val: 0.3900 Test: 0.4900\n",
      "Epoch: 955, Loss: 0.2429, | Train: 0.9750, Val: 0.3900 Test: 0.4800\n",
      "Epoch: 956, Loss: 0.2433, | Train: 0.9800, Val: 0.3700 Test: 0.4900\n",
      "Epoch: 957, Loss: 0.2623, | Train: 0.9775, Val: 0.3600 Test: 0.4800\n",
      "Epoch: 958, Loss: 0.2223, | Train: 0.9775, Val: 0.3500 Test: 0.4800\n",
      "Epoch: 959, Loss: 0.2458, | Train: 0.9750, Val: 0.3500 Test: 0.5000\n",
      "Epoch: 960, Loss: 0.2364, | Train: 0.9762, Val: 0.3600 Test: 0.5000\n",
      "Epoch: 961, Loss: 0.2277, | Train: 0.9787, Val: 0.3400 Test: 0.5100\n",
      "Epoch: 962, Loss: 0.2521, | Train: 0.9750, Val: 0.3500 Test: 0.4900\n",
      "Epoch: 963, Loss: 0.2676, | Train: 0.9787, Val: 0.3800 Test: 0.4900\n",
      "Epoch: 964, Loss: 0.2431, | Train: 0.9787, Val: 0.3700 Test: 0.4400\n",
      "Epoch: 965, Loss: 0.2063, | Train: 0.9712, Val: 0.3700 Test: 0.4500\n",
      "Epoch: 966, Loss: 0.2597, | Train: 0.9700, Val: 0.3500 Test: 0.4800\n",
      "Epoch: 967, Loss: 0.2442, | Train: 0.9650, Val: 0.3500 Test: 0.4700\n",
      "Epoch: 968, Loss: 0.2566, | Train: 0.9750, Val: 0.3500 Test: 0.4800\n",
      "Epoch: 969, Loss: 0.2651, | Train: 0.9787, Val: 0.3200 Test: 0.4800\n",
      "Epoch: 970, Loss: 0.2249, | Train: 0.9800, Val: 0.3700 Test: 0.4700\n",
      "Epoch: 971, Loss: 0.2529, | Train: 0.9737, Val: 0.3800 Test: 0.4500\n",
      "Epoch: 972, Loss: 0.2248, | Train: 0.9750, Val: 0.3900 Test: 0.4500\n",
      "Epoch: 973, Loss: 0.2455, | Train: 0.9725, Val: 0.3600 Test: 0.4400\n",
      "Epoch: 974, Loss: 0.2646, | Train: 0.9712, Val: 0.3800 Test: 0.4600\n",
      "Epoch: 975, Loss: 0.2469, | Train: 0.9762, Val: 0.3600 Test: 0.4700\n",
      "Epoch: 976, Loss: 0.2403, | Train: 0.9725, Val: 0.3700 Test: 0.4700\n",
      "Epoch: 977, Loss: 0.2550, | Train: 0.9762, Val: 0.3800 Test: 0.4600\n",
      "Epoch: 978, Loss: 0.2186, | Train: 0.9775, Val: 0.3800 Test: 0.4600\n",
      "Epoch: 979, Loss: 0.2884, | Train: 0.9688, Val: 0.4000 Test: 0.4700\n",
      "Epoch: 980, Loss: 0.2768, | Train: 0.9725, Val: 0.3900 Test: 0.4700\n",
      "Epoch: 981, Loss: 0.2812, | Train: 0.9762, Val: 0.4000 Test: 0.4700\n",
      "Epoch: 982, Loss: 0.2310, | Train: 0.9775, Val: 0.3900 Test: 0.5000\n",
      "Epoch: 983, Loss: 0.2521, | Train: 0.9737, Val: 0.3800 Test: 0.5100\n",
      "Epoch: 984, Loss: 0.2382, | Train: 0.9725, Val: 0.3700 Test: 0.4800\n",
      "Epoch: 985, Loss: 0.2390, | Train: 0.9762, Val: 0.3900 Test: 0.4700\n",
      "Epoch: 986, Loss: 0.2517, | Train: 0.9775, Val: 0.3900 Test: 0.4700\n",
      "Epoch: 987, Loss: 0.2663, | Train: 0.9787, Val: 0.4000 Test: 0.4700\n",
      "Epoch: 988, Loss: 0.2224, | Train: 0.9737, Val: 0.3800 Test: 0.4400\n",
      "Epoch: 989, Loss: 0.2789, | Train: 0.9750, Val: 0.4000 Test: 0.4700\n",
      "Epoch: 990, Loss: 0.2195, | Train: 0.9837, Val: 0.3900 Test: 0.4700\n",
      "Epoch: 991, Loss: 0.2372, | Train: 0.9800, Val: 0.3900 Test: 0.4700\n",
      "Epoch: 992, Loss: 0.2379, | Train: 0.9737, Val: 0.4000 Test: 0.4900\n",
      "Epoch: 993, Loss: 0.2532, | Train: 0.9737, Val: 0.4000 Test: 0.4900\n",
      "Epoch: 994, Loss: 0.2509, | Train: 0.9725, Val: 0.4200 Test: 0.5000\n",
      "Epoch: 995, Loss: 0.2405, | Train: 0.9800, Val: 0.4100 Test: 0.4900\n",
      "Epoch: 996, Loss: 0.2281, | Train: 0.9850, Val: 0.4000 Test: 0.4900\n",
      "Epoch: 997, Loss: 0.2528, | Train: 0.9775, Val: 0.3900 Test: 0.4900\n",
      "Epoch: 998, Loss: 0.2215, | Train: 0.9775, Val: 0.3900 Test: 0.5000\n",
      "Epoch: 999, Loss: 0.2398, | Train: 0.9837, Val: 0.4100 Test: 0.4800\n",
      "Epoch: 1000, Loss: 0.2397, | Train: 0.9787, Val: 0.3700 Test: 0.4700\n",
      "Median time per epoch: 0.0015s\n"
     ]
    }
   ],
   "source": [
    "# MLP classifier\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(p=0.2),\n",
    "            nn.Linear(32, 3),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_relu_stack(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "model = NeuralNetwork().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-5)\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(train_x)\n",
    "    loss = F.nll_loss(out, train_y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return float(loss)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    pred = model(patient_embedding).argmax(dim=-1)\n",
    "    train_acc = float((pred[:int(num_patients*0.8)] == train_y).float().mean())\n",
    "    val_acc = float((pred[int(num_patients*0.8):int(num_patients*0.9)] == val_y).float().mean())\n",
    "    test_acc = float((pred[int(num_patients*0.9):] == test_y).float().mean())\n",
    "    return train_acc, val_acc, test_acc\n",
    "\n",
    "import time\n",
    "\n",
    "times = []\n",
    "best_acc = float(0.)\n",
    "\n",
    "for epoch in range(1, 1001):\n",
    "    start = time.time()\n",
    "    loss = train()\n",
    "    train_acc, val_acc, test_acc = test()\n",
    "    print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}, | Train: {train_acc:.4f}, Val: {val_acc:.4f} '\n",
    "        f'Test: {test_acc:.4f}')\n",
    "    # # Early stopping\n",
    "    # if val_acc >= best_acc:\n",
    "    #     best_acc = val_acc\n",
    "    #     patience = 50  # Reset patience counter\n",
    "    # else:\n",
    "    #     patience -= 1\n",
    "    #     if patience == 0:\n",
    "    #         print(\"Early stopping...\")\n",
    "    #         break\n",
    "    times.append(time.time() - start)\n",
    "print(f\"Median time per epoch: {torch.tensor(times).median():.4f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC score: 0.5825\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(patient_embedding).cpu()\n",
    "    prob = F.softmax(out, dim=1)\n",
    "auc = roc_auc_score(test_y.cpu(), prob[int(num_patients*0.9):], multi_class='ovr')\n",
    "print(f'ROC AUC score: {auc:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neurovasc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
