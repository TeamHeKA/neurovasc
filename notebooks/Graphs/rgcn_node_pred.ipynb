{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outcome Prediction with RGCN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data from \"processed_data\" folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.logging import log\n",
    "from torch_geometric.nn import RGCNConv\n",
    "from torch_geometric.utils import to_undirected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(edge_index=[2, 221870], edge_type=[221870], train_idx=[700], train_y=[700], valid_idx=[100], valid_y=[100], test_idx=[200], test_y=[200], num_nodes=33563, x=[33563, 100], num_relations=10, num_classes=3)\n"
     ]
    }
   ],
   "source": [
    "num_patients = 1000\n",
    "inverse_triples=True\n",
    "embed_dim = 100\n",
    "\n",
    "entity = pd.read_csv(f'processed_data/sphn_entities_{num_patients}_noOutcome.tsv', sep='\\t', index_col=0, header=None)\n",
    "entity = entity.to_dict()[1]\n",
    "\n",
    "indices = []\n",
    "for i in range(num_patients):\n",
    "    idx = f'<http://nvasc.org/synth_patient_{i}>'\n",
    "    indices.append(entity[idx])\n",
    "\n",
    "events = pd.read_csv(f'processed_data/sphn_events_{num_patients}_noOutcome.tsv', sep='\\t', header=None)\n",
    "y = joblib.load(f'../Data Generation/outcomes_{num_patients}_0.joblib')\n",
    "\n",
    "non_test_X, test_X, non_test_y, test_y_ = train_test_split(indices, y, stratify=y, test_size=0.2)\n",
    "train_X, valid_X, train_y_, valid_y_ = train_test_split(non_test_X, non_test_y, stratify=non_test_y, test_size=1./8)\n",
    "\n",
    "edge_index = torch.vstack((torch.Tensor(events[0]).long(),torch.Tensor(events[2]).long()))\n",
    "edge_type = torch.Tensor(events[1]).long()\n",
    "train_idx = torch.Tensor(train_X).long()\n",
    "train_y = torch.Tensor(train_y_).long()\n",
    "valid_idx = torch.Tensor(valid_X).long()\n",
    "valid_y = torch.Tensor(valid_y_).long()\n",
    "test_idx = torch.Tensor(test_X).long()\n",
    "test_y = torch.Tensor(test_y_).long()\n",
    "num_nodes = len(entity)\n",
    "\n",
    "if inverse_triples == True:\n",
    "    edge_index = to_undirected(edge_index)\n",
    "    edge_type = torch.cat((edge_type, edge_type))\n",
    "\n",
    "data = Data(\n",
    "    edge_index=edge_index,\n",
    "    edge_type=edge_type,\n",
    "    train_idx=train_idx,\n",
    "    train_y=train_y,\n",
    "    valid_idx=valid_idx,\n",
    "    valid_y=valid_y,\n",
    "    test_idx=test_idx,\n",
    "    test_y=test_y,\n",
    "    num_nodes=num_nodes,\n",
    ")\n",
    "embedding = torch.nn.Parameter(torch.empty(num_nodes, embed_dim))\n",
    "torch.nn.init.xavier_uniform_(embedding, gain=math.sqrt(2.0))\n",
    "data.x = embedding\n",
    "data.num_relations = data.num_edge_types\n",
    "data.num_classes = 3\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = RGCNConv(embed_dim, 16, data.num_relations,\n",
    "                          num_bases=4)\n",
    "        self.conv2 = RGCNConv(16, data.num_classes, data.num_relations,\n",
    "                          num_bases=4)\n",
    "\n",
    "    def forward(self, edge_index, edge_type):\n",
    "        x = F.elu(self.conv1(data.x, edge_index, edge_type))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index, edge_type)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "model, data = Net().to(device), data.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.edge_index, data.edge_type)\n",
    "    loss = F.nll_loss(out[data.train_idx], data.train_y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return float(loss)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    pred = model(data.edge_index, data.edge_type).argmax(dim=-1)\n",
    "    train_acc = float((pred[data.train_idx] == data.train_y).float().mean())\n",
    "    val_acc = float((pred[data.valid_idx] == data.valid_y).float().mean())\n",
    "    test_acc = float((pred[data.test_idx] == data.test_y).float().mean())\n",
    "    return train_acc, val_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 1.1040, Train: 0.4800, Val: 0.4700, Test: 0.4350\n",
      "Epoch: 002, Loss: 1.0682, Train: 0.5000, Val: 0.4600, Test: 0.4350\n",
      "Epoch: 003, Loss: 1.0386, Train: 0.5086, Val: 0.4500, Test: 0.4350\n",
      "Epoch: 004, Loss: 1.0132, Train: 0.5157, Val: 0.4600, Test: 0.4350\n",
      "Epoch: 005, Loss: 0.9898, Train: 0.5257, Val: 0.4700, Test: 0.4350\n",
      "Epoch: 006, Loss: 0.9681, Train: 0.5357, Val: 0.4600, Test: 0.4350\n",
      "Epoch: 007, Loss: 0.9520, Train: 0.5871, Val: 0.4300, Test: 0.4350\n",
      "Epoch: 008, Loss: 0.9457, Train: 0.6000, Val: 0.4400, Test: 0.4350\n",
      "Epoch: 009, Loss: 0.9444, Train: 0.5629, Val: 0.4800, Test: 0.4350\n",
      "Epoch: 010, Loss: 0.9401, Train: 0.5529, Val: 0.4700, Test: 0.4350\n",
      "Epoch: 011, Loss: 0.9457, Train: 0.5457, Val: 0.4600, Test: 0.4350\n",
      "Epoch: 012, Loss: 0.9350, Train: 0.5714, Val: 0.4500, Test: 0.4350\n",
      "Epoch: 013, Loss: 0.9360, Train: 0.6186, Val: 0.4600, Test: 0.4350\n",
      "Epoch: 014, Loss: 0.9288, Train: 0.6457, Val: 0.4900, Test: 0.4450\n",
      "Epoch: 015, Loss: 0.9097, Train: 0.6786, Val: 0.4900, Test: 0.4450\n",
      "Epoch: 016, Loss: 0.9127, Train: 0.6943, Val: 0.5200, Test: 0.4400\n",
      "Epoch: 017, Loss: 0.9060, Train: 0.6971, Val: 0.5300, Test: 0.4450\n",
      "Epoch: 018, Loss: 0.8909, Train: 0.7000, Val: 0.5100, Test: 0.4450\n",
      "Epoch: 019, Loss: 0.8870, Train: 0.7043, Val: 0.4800, Test: 0.4450\n",
      "Epoch: 020, Loss: 0.8791, Train: 0.7071, Val: 0.4800, Test: 0.4450\n",
      "Epoch: 021, Loss: 0.8765, Train: 0.7129, Val: 0.4600, Test: 0.4450\n",
      "Epoch: 022, Loss: 0.8656, Train: 0.7171, Val: 0.4500, Test: 0.4450\n",
      "Epoch: 023, Loss: 0.8590, Train: 0.7143, Val: 0.4500, Test: 0.4450\n",
      "Epoch: 024, Loss: 0.8482, Train: 0.7200, Val: 0.4600, Test: 0.4450\n",
      "Epoch: 025, Loss: 0.8300, Train: 0.7200, Val: 0.4400, Test: 0.4450\n",
      "Epoch: 026, Loss: 0.8280, Train: 0.7286, Val: 0.4600, Test: 0.4450\n",
      "Epoch: 027, Loss: 0.8103, Train: 0.7314, Val: 0.4500, Test: 0.4450\n",
      "Epoch: 028, Loss: 0.8199, Train: 0.7343, Val: 0.4700, Test: 0.4450\n",
      "Epoch: 029, Loss: 0.7968, Train: 0.7371, Val: 0.4600, Test: 0.4450\n",
      "Epoch: 030, Loss: 0.7858, Train: 0.7386, Val: 0.4600, Test: 0.4450\n",
      "Epoch: 031, Loss: 0.7742, Train: 0.7500, Val: 0.4700, Test: 0.4450\n",
      "Epoch: 032, Loss: 0.7647, Train: 0.7514, Val: 0.4600, Test: 0.4450\n",
      "Epoch: 033, Loss: 0.7419, Train: 0.7543, Val: 0.4600, Test: 0.4450\n",
      "Epoch: 034, Loss: 0.7310, Train: 0.7614, Val: 0.4800, Test: 0.4450\n",
      "Epoch: 035, Loss: 0.7204, Train: 0.7714, Val: 0.4700, Test: 0.4450\n",
      "Epoch: 036, Loss: 0.7109, Train: 0.7714, Val: 0.4700, Test: 0.4450\n",
      "Epoch: 037, Loss: 0.6954, Train: 0.7771, Val: 0.4700, Test: 0.4450\n",
      "Epoch: 038, Loss: 0.6897, Train: 0.7886, Val: 0.4700, Test: 0.4450\n",
      "Epoch: 039, Loss: 0.6713, Train: 0.8100, Val: 0.4900, Test: 0.4450\n",
      "Epoch: 040, Loss: 0.6594, Train: 0.8243, Val: 0.4900, Test: 0.4450\n",
      "Epoch: 041, Loss: 0.6369, Train: 0.8271, Val: 0.5000, Test: 0.4450\n",
      "Epoch: 042, Loss: 0.6232, Train: 0.8400, Val: 0.4900, Test: 0.4450\n",
      "Epoch: 043, Loss: 0.6009, Train: 0.8429, Val: 0.5000, Test: 0.4450\n",
      "Epoch: 044, Loss: 0.6046, Train: 0.8500, Val: 0.5000, Test: 0.4450\n",
      "Epoch: 045, Loss: 0.5977, Train: 0.8443, Val: 0.4900, Test: 0.4450\n",
      "Epoch: 046, Loss: 0.5663, Train: 0.8543, Val: 0.4900, Test: 0.4450\n",
      "Epoch: 047, Loss: 0.5488, Train: 0.8686, Val: 0.4900, Test: 0.4450\n",
      "Epoch: 048, Loss: 0.5303, Train: 0.8757, Val: 0.4900, Test: 0.4450\n",
      "Epoch: 049, Loss: 0.5431, Train: 0.8800, Val: 0.4700, Test: 0.4450\n",
      "Epoch: 050, Loss: 0.4880, Train: 0.8786, Val: 0.4700, Test: 0.4450\n",
      "Epoch: 051, Loss: 0.5036, Train: 0.8843, Val: 0.4600, Test: 0.4450\n",
      "Epoch: 052, Loss: 0.4743, Train: 0.8914, Val: 0.4600, Test: 0.4450\n",
      "Epoch: 053, Loss: 0.4617, Train: 0.9000, Val: 0.4500, Test: 0.4450\n",
      "Epoch: 054, Loss: 0.4600, Train: 0.9071, Val: 0.4300, Test: 0.4450\n",
      "Epoch: 055, Loss: 0.4469, Train: 0.9171, Val: 0.4400, Test: 0.4450\n",
      "Epoch: 056, Loss: 0.4304, Train: 0.9243, Val: 0.4400, Test: 0.4450\n",
      "Epoch: 057, Loss: 0.4289, Train: 0.9229, Val: 0.4400, Test: 0.4450\n",
      "Epoch: 058, Loss: 0.3960, Train: 0.9243, Val: 0.4400, Test: 0.4450\n",
      "Epoch: 059, Loss: 0.4158, Train: 0.9286, Val: 0.4300, Test: 0.4450\n",
      "Epoch: 060, Loss: 0.3829, Train: 0.9429, Val: 0.4300, Test: 0.4450\n",
      "Epoch: 061, Loss: 0.3611, Train: 0.9471, Val: 0.4300, Test: 0.4450\n",
      "Epoch: 062, Loss: 0.3568, Train: 0.9543, Val: 0.4200, Test: 0.4450\n",
      "Epoch: 063, Loss: 0.3570, Train: 0.9571, Val: 0.4300, Test: 0.4450\n",
      "Epoch: 064, Loss: 0.3558, Train: 0.9586, Val: 0.4300, Test: 0.4450\n",
      "Epoch: 065, Loss: 0.3503, Train: 0.9629, Val: 0.4300, Test: 0.4450\n",
      "Epoch: 066, Loss: 0.3041, Train: 0.9714, Val: 0.4300, Test: 0.4450\n",
      "Epoch: 067, Loss: 0.3223, Train: 0.9729, Val: 0.4300, Test: 0.4450\n",
      "Epoch: 068, Loss: 0.3044, Train: 0.9729, Val: 0.4300, Test: 0.4450\n",
      "Epoch: 069, Loss: 0.2790, Train: 0.9743, Val: 0.4400, Test: 0.4450\n",
      "Epoch: 070, Loss: 0.2873, Train: 0.9757, Val: 0.4300, Test: 0.4450\n",
      "Epoch: 071, Loss: 0.2901, Train: 0.9771, Val: 0.4400, Test: 0.4450\n",
      "Epoch: 072, Loss: 0.2848, Train: 0.9771, Val: 0.4500, Test: 0.4450\n",
      "Epoch: 073, Loss: 0.2560, Train: 0.9843, Val: 0.4500, Test: 0.4450\n",
      "Epoch: 074, Loss: 0.2500, Train: 0.9843, Val: 0.4300, Test: 0.4450\n",
      "Epoch: 075, Loss: 0.2606, Train: 0.9871, Val: 0.4300, Test: 0.4450\n",
      "Epoch: 076, Loss: 0.2370, Train: 0.9871, Val: 0.4300, Test: 0.4450\n",
      "Epoch: 077, Loss: 0.2518, Train: 0.9871, Val: 0.4300, Test: 0.4450\n",
      "Epoch: 078, Loss: 0.2127, Train: 0.9886, Val: 0.4100, Test: 0.4450\n",
      "Epoch: 079, Loss: 0.2215, Train: 0.9886, Val: 0.4100, Test: 0.4450\n",
      "Epoch: 080, Loss: 0.2244, Train: 0.9886, Val: 0.4200, Test: 0.4450\n",
      "Epoch: 081, Loss: 0.2072, Train: 0.9914, Val: 0.4200, Test: 0.4450\n",
      "Epoch: 082, Loss: 0.1960, Train: 0.9929, Val: 0.4200, Test: 0.4450\n",
      "Epoch: 083, Loss: 0.1978, Train: 0.9943, Val: 0.4200, Test: 0.4450\n",
      "Epoch: 084, Loss: 0.1952, Train: 0.9943, Val: 0.4200, Test: 0.4450\n",
      "Epoch: 085, Loss: 0.1902, Train: 0.9929, Val: 0.4100, Test: 0.4450\n",
      "Epoch: 086, Loss: 0.1858, Train: 0.9929, Val: 0.4100, Test: 0.4450\n",
      "Epoch: 087, Loss: 0.1815, Train: 0.9943, Val: 0.4100, Test: 0.4450\n",
      "Epoch: 088, Loss: 0.1788, Train: 0.9957, Val: 0.3900, Test: 0.4450\n",
      "Epoch: 089, Loss: 0.1562, Train: 0.9957, Val: 0.3900, Test: 0.4450\n",
      "Epoch: 090, Loss: 0.1805, Train: 0.9986, Val: 0.3900, Test: 0.4450\n",
      "Epoch: 091, Loss: 0.1791, Train: 1.0000, Val: 0.4200, Test: 0.4450\n",
      "Epoch: 092, Loss: 0.1657, Train: 1.0000, Val: 0.4100, Test: 0.4450\n",
      "Epoch: 093, Loss: 0.1582, Train: 1.0000, Val: 0.4000, Test: 0.4450\n",
      "Epoch: 094, Loss: 0.1550, Train: 0.9986, Val: 0.4100, Test: 0.4450\n",
      "Epoch: 095, Loss: 0.1614, Train: 0.9986, Val: 0.3900, Test: 0.4450\n",
      "Epoch: 096, Loss: 0.1553, Train: 1.0000, Val: 0.3900, Test: 0.4450\n",
      "Epoch: 097, Loss: 0.1641, Train: 1.0000, Val: 0.3800, Test: 0.4450\n",
      "Epoch: 098, Loss: 0.1557, Train: 1.0000, Val: 0.4100, Test: 0.4450\n",
      "Epoch: 099, Loss: 0.1451, Train: 1.0000, Val: 0.4100, Test: 0.4450\n",
      "Epoch: 100, Loss: 0.1391, Train: 1.0000, Val: 0.3900, Test: 0.4450\n",
      "Epoch: 101, Loss: 0.1243, Train: 1.0000, Val: 0.3800, Test: 0.4450\n",
      "Epoch: 102, Loss: 0.1464, Train: 1.0000, Val: 0.3900, Test: 0.4450\n",
      "Epoch: 103, Loss: 0.1372, Train: 1.0000, Val: 0.3900, Test: 0.4450\n",
      "Epoch: 104, Loss: 0.1466, Train: 1.0000, Val: 0.4000, Test: 0.4450\n",
      "Epoch: 105, Loss: 0.1255, Train: 1.0000, Val: 0.4000, Test: 0.4450\n",
      "Epoch: 106, Loss: 0.1229, Train: 1.0000, Val: 0.3900, Test: 0.4450\n",
      "Epoch: 107, Loss: 0.1302, Train: 1.0000, Val: 0.4000, Test: 0.4450\n",
      "Epoch: 108, Loss: 0.1180, Train: 1.0000, Val: 0.4200, Test: 0.4450\n",
      "Epoch: 109, Loss: 0.1298, Train: 1.0000, Val: 0.4300, Test: 0.4450\n",
      "Epoch: 110, Loss: 0.1129, Train: 1.0000, Val: 0.4300, Test: 0.4450\n",
      "Epoch: 111, Loss: 0.1160, Train: 1.0000, Val: 0.4300, Test: 0.4450\n",
      "Epoch: 112, Loss: 0.1295, Train: 1.0000, Val: 0.4200, Test: 0.4450\n",
      "Epoch: 113, Loss: 0.1123, Train: 1.0000, Val: 0.4300, Test: 0.4450\n",
      "Epoch: 114, Loss: 0.1144, Train: 1.0000, Val: 0.4300, Test: 0.4450\n",
      "Epoch: 115, Loss: 0.1184, Train: 1.0000, Val: 0.4300, Test: 0.4450\n",
      "Epoch: 116, Loss: 0.1077, Train: 1.0000, Val: 0.4100, Test: 0.4450\n",
      "Epoch: 117, Loss: 0.1210, Train: 1.0000, Val: 0.4200, Test: 0.4450\n",
      "Epoch: 118, Loss: 0.0980, Train: 1.0000, Val: 0.4200, Test: 0.4450\n",
      "Epoch: 119, Loss: 0.1238, Train: 1.0000, Val: 0.4100, Test: 0.4450\n",
      "Epoch: 120, Loss: 0.1053, Train: 1.0000, Val: 0.4200, Test: 0.4450\n",
      "Epoch: 121, Loss: 0.1075, Train: 1.0000, Val: 0.4200, Test: 0.4450\n",
      "Epoch: 122, Loss: 0.1160, Train: 1.0000, Val: 0.4000, Test: 0.4450\n",
      "Epoch: 123, Loss: 0.0927, Train: 1.0000, Val: 0.3800, Test: 0.4450\n",
      "Epoch: 124, Loss: 0.1205, Train: 1.0000, Val: 0.4400, Test: 0.4450\n",
      "Epoch: 125, Loss: 0.1041, Train: 1.0000, Val: 0.4200, Test: 0.4450\n",
      "Epoch: 126, Loss: 0.1128, Train: 1.0000, Val: 0.4300, Test: 0.4450\n",
      "Epoch: 127, Loss: 0.1148, Train: 1.0000, Val: 0.3800, Test: 0.4450\n",
      "Epoch: 128, Loss: 0.1057, Train: 1.0000, Val: 0.4000, Test: 0.4450\n",
      "Epoch: 129, Loss: 0.1062, Train: 1.0000, Val: 0.4100, Test: 0.4450\n",
      "Epoch: 130, Loss: 0.0985, Train: 1.0000, Val: 0.4400, Test: 0.4450\n",
      "Epoch: 131, Loss: 0.1049, Train: 1.0000, Val: 0.4400, Test: 0.4450\n",
      "Epoch: 132, Loss: 0.1072, Train: 1.0000, Val: 0.4000, Test: 0.4450\n",
      "Epoch: 133, Loss: 0.0953, Train: 1.0000, Val: 0.3900, Test: 0.4450\n",
      "Epoch: 134, Loss: 0.1204, Train: 1.0000, Val: 0.4000, Test: 0.4450\n",
      "Epoch: 135, Loss: 0.0996, Train: 1.0000, Val: 0.4400, Test: 0.4450\n",
      "Epoch: 136, Loss: 0.0997, Train: 1.0000, Val: 0.4300, Test: 0.4450\n",
      "Epoch: 137, Loss: 0.1016, Train: 1.0000, Val: 0.4300, Test: 0.4450\n",
      "Epoch: 138, Loss: 0.1042, Train: 1.0000, Val: 0.4300, Test: 0.4450\n",
      "Epoch: 139, Loss: 0.0940, Train: 1.0000, Val: 0.4100, Test: 0.4450\n",
      "Epoch: 140, Loss: 0.1032, Train: 1.0000, Val: 0.4000, Test: 0.4450\n",
      "Epoch: 141, Loss: 0.0929, Train: 1.0000, Val: 0.4100, Test: 0.4450\n",
      "Epoch: 142, Loss: 0.1000, Train: 1.0000, Val: 0.4300, Test: 0.4450\n",
      "Epoch: 143, Loss: 0.0918, Train: 1.0000, Val: 0.4100, Test: 0.4450\n",
      "Epoch: 144, Loss: 0.0924, Train: 1.0000, Val: 0.4000, Test: 0.4450\n",
      "Epoch: 145, Loss: 0.1014, Train: 1.0000, Val: 0.4000, Test: 0.4450\n",
      "Epoch: 146, Loss: 0.0984, Train: 1.0000, Val: 0.3900, Test: 0.4450\n",
      "Epoch: 147, Loss: 0.0975, Train: 1.0000, Val: 0.3700, Test: 0.4450\n",
      "Epoch: 148, Loss: 0.0875, Train: 1.0000, Val: 0.4100, Test: 0.4450\n",
      "Epoch: 149, Loss: 0.0916, Train: 1.0000, Val: 0.4000, Test: 0.4450\n",
      "Epoch: 150, Loss: 0.0949, Train: 1.0000, Val: 0.3900, Test: 0.4450\n",
      "Epoch: 151, Loss: 0.0969, Train: 1.0000, Val: 0.3700, Test: 0.4450\n",
      "Epoch: 152, Loss: 0.0823, Train: 1.0000, Val: 0.4100, Test: 0.4450\n",
      "Epoch: 153, Loss: 0.0948, Train: 1.0000, Val: 0.4100, Test: 0.4450\n",
      "Epoch: 154, Loss: 0.1007, Train: 1.0000, Val: 0.3900, Test: 0.4450\n",
      "Epoch: 155, Loss: 0.0832, Train: 1.0000, Val: 0.4300, Test: 0.4450\n",
      "Epoch: 156, Loss: 0.0925, Train: 1.0000, Val: 0.4300, Test: 0.4450\n",
      "Epoch: 157, Loss: 0.0872, Train: 1.0000, Val: 0.4100, Test: 0.4450\n",
      "Epoch: 158, Loss: 0.0838, Train: 1.0000, Val: 0.4100, Test: 0.4450\n",
      "Epoch: 159, Loss: 0.0925, Train: 1.0000, Val: 0.4100, Test: 0.4450\n",
      "Epoch: 160, Loss: 0.0933, Train: 1.0000, Val: 0.4300, Test: 0.4450\n",
      "Epoch: 161, Loss: 0.0999, Train: 1.0000, Val: 0.4200, Test: 0.4450\n",
      "Epoch: 162, Loss: 0.0914, Train: 1.0000, Val: 0.4200, Test: 0.4450\n",
      "Epoch: 163, Loss: 0.0890, Train: 1.0000, Val: 0.4200, Test: 0.4450\n",
      "Epoch: 164, Loss: 0.0953, Train: 1.0000, Val: 0.4300, Test: 0.4450\n",
      "Epoch: 165, Loss: 0.0766, Train: 1.0000, Val: 0.4100, Test: 0.4450\n",
      "Epoch: 166, Loss: 0.0957, Train: 1.0000, Val: 0.4200, Test: 0.4450\n",
      "Epoch: 167, Loss: 0.0952, Train: 1.0000, Val: 0.4400, Test: 0.4450\n",
      "Epoch: 168, Loss: 0.0850, Train: 1.0000, Val: 0.4300, Test: 0.4450\n",
      "Epoch: 169, Loss: 0.0770, Train: 1.0000, Val: 0.4400, Test: 0.4450\n",
      "Epoch: 170, Loss: 0.0841, Train: 1.0000, Val: 0.4400, Test: 0.4450\n",
      "Epoch: 171, Loss: 0.0892, Train: 1.0000, Val: 0.4200, Test: 0.4450\n",
      "Epoch: 172, Loss: 0.0803, Train: 1.0000, Val: 0.4200, Test: 0.4450\n",
      "Epoch: 173, Loss: 0.0817, Train: 1.0000, Val: 0.4200, Test: 0.4450\n",
      "Epoch: 174, Loss: 0.0850, Train: 1.0000, Val: 0.4100, Test: 0.4450\n",
      "Epoch: 175, Loss: 0.0860, Train: 1.0000, Val: 0.4100, Test: 0.4450\n",
      "Epoch: 176, Loss: 0.0733, Train: 1.0000, Val: 0.4000, Test: 0.4450\n",
      "Epoch: 177, Loss: 0.0839, Train: 1.0000, Val: 0.4100, Test: 0.4450\n",
      "Epoch: 178, Loss: 0.0747, Train: 1.0000, Val: 0.4200, Test: 0.4450\n",
      "Epoch: 179, Loss: 0.0731, Train: 1.0000, Val: 0.4200, Test: 0.4450\n",
      "Epoch: 180, Loss: 0.0890, Train: 1.0000, Val: 0.4200, Test: 0.4450\n",
      "Epoch: 181, Loss: 0.0782, Train: 1.0000, Val: 0.4200, Test: 0.4450\n",
      "Epoch: 182, Loss: 0.0722, Train: 1.0000, Val: 0.4100, Test: 0.4450\n",
      "Epoch: 183, Loss: 0.0859, Train: 1.0000, Val: 0.4000, Test: 0.4450\n",
      "Epoch: 184, Loss: 0.0863, Train: 1.0000, Val: 0.4100, Test: 0.4450\n",
      "Epoch: 185, Loss: 0.0869, Train: 1.0000, Val: 0.4100, Test: 0.4450\n",
      "Epoch: 186, Loss: 0.0716, Train: 1.0000, Val: 0.4000, Test: 0.4450\n",
      "Epoch: 187, Loss: 0.0713, Train: 1.0000, Val: 0.4200, Test: 0.4450\n",
      "Epoch: 188, Loss: 0.0773, Train: 1.0000, Val: 0.4200, Test: 0.4450\n",
      "Epoch: 189, Loss: 0.0769, Train: 1.0000, Val: 0.4200, Test: 0.4450\n",
      "Epoch: 190, Loss: 0.0744, Train: 1.0000, Val: 0.4100, Test: 0.4450\n",
      "Epoch: 191, Loss: 0.0758, Train: 1.0000, Val: 0.4100, Test: 0.4450\n",
      "Epoch: 192, Loss: 0.0690, Train: 1.0000, Val: 0.3900, Test: 0.4450\n",
      "Epoch: 193, Loss: 0.0775, Train: 1.0000, Val: 0.4200, Test: 0.4450\n",
      "Epoch: 194, Loss: 0.0617, Train: 1.0000, Val: 0.4000, Test: 0.4450\n",
      "Epoch: 195, Loss: 0.0681, Train: 1.0000, Val: 0.4000, Test: 0.4450\n",
      "Epoch: 196, Loss: 0.0767, Train: 1.0000, Val: 0.4200, Test: 0.4450\n",
      "Epoch: 197, Loss: 0.0793, Train: 1.0000, Val: 0.4100, Test: 0.4450\n",
      "Epoch: 198, Loss: 0.0779, Train: 1.0000, Val: 0.4200, Test: 0.4450\n",
      "Epoch: 199, Loss: 0.0696, Train: 1.0000, Val: 0.4200, Test: 0.4450\n",
      "Epoch: 200, Loss: 0.0703, Train: 1.0000, Val: 0.4200, Test: 0.4450\n",
      "Epoch: 201, Loss: 0.0636, Train: 1.0000, Val: 0.4100, Test: 0.4450\n",
      "Epoch: 202, Loss: 0.0812, Train: 1.0000, Val: 0.4200, Test: 0.4450\n",
      "Epoch: 203, Loss: 0.0931, Train: 1.0000, Val: 0.4100, Test: 0.4450\n",
      "Epoch: 204, Loss: 0.0747, Train: 1.0000, Val: 0.4400, Test: 0.4450\n",
      "Epoch: 205, Loss: 0.0679, Train: 1.0000, Val: 0.4400, Test: 0.4450\n",
      "Epoch: 206, Loss: 0.0666, Train: 1.0000, Val: 0.4100, Test: 0.4450\n",
      "Epoch: 207, Loss: 0.0816, Train: 1.0000, Val: 0.4400, Test: 0.4450\n",
      "Epoch: 208, Loss: 0.0814, Train: 1.0000, Val: 0.4600, Test: 0.4450\n",
      "Epoch: 209, Loss: 0.0799, Train: 1.0000, Val: 0.4400, Test: 0.4450\n",
      "Epoch: 210, Loss: 0.0690, Train: 1.0000, Val: 0.4300, Test: 0.4450\n",
      "Epoch: 211, Loss: 0.0719, Train: 1.0000, Val: 0.4400, Test: 0.4450\n",
      "Epoch: 212, Loss: 0.0707, Train: 1.0000, Val: 0.4500, Test: 0.4450\n",
      "Epoch: 213, Loss: 0.0696, Train: 1.0000, Val: 0.4400, Test: 0.4450\n",
      "Epoch: 214, Loss: 0.0695, Train: 1.0000, Val: 0.4500, Test: 0.4450\n",
      "Epoch: 215, Loss: 0.0575, Train: 1.0000, Val: 0.4400, Test: 0.4450\n",
      "Epoch: 216, Loss: 0.0701, Train: 1.0000, Val: 0.4400, Test: 0.4450\n",
      "Epoch: 217, Loss: 0.0627, Train: 1.0000, Val: 0.4300, Test: 0.4450\n",
      "Epoch: 218, Loss: 0.0803, Train: 1.0000, Val: 0.4300, Test: 0.4450\n",
      "Epoch: 219, Loss: 0.0643, Train: 1.0000, Val: 0.4500, Test: 0.4450\n",
      "Epoch: 220, Loss: 0.0634, Train: 1.0000, Val: 0.4500, Test: 0.4450\n",
      "Epoch: 221, Loss: 0.0705, Train: 1.0000, Val: 0.4600, Test: 0.4450\n",
      "Epoch: 222, Loss: 0.0704, Train: 1.0000, Val: 0.4300, Test: 0.4450\n",
      "Epoch: 223, Loss: 0.0726, Train: 1.0000, Val: 0.4300, Test: 0.4450\n",
      "Epoch: 224, Loss: 0.0639, Train: 1.0000, Val: 0.4300, Test: 0.4450\n",
      "Epoch: 225, Loss: 0.0731, Train: 1.0000, Val: 0.4400, Test: 0.4450\n",
      "Epoch: 226, Loss: 0.0583, Train: 1.0000, Val: 0.4400, Test: 0.4450\n",
      "Epoch: 227, Loss: 0.0656, Train: 1.0000, Val: 0.4500, Test: 0.4450\n",
      "Epoch: 228, Loss: 0.0620, Train: 1.0000, Val: 0.4400, Test: 0.4450\n",
      "Epoch: 229, Loss: 0.0697, Train: 1.0000, Val: 0.4400, Test: 0.4450\n",
      "Epoch: 230, Loss: 0.0623, Train: 1.0000, Val: 0.4400, Test: 0.4450\n",
      "Epoch: 231, Loss: 0.0781, Train: 1.0000, Val: 0.4100, Test: 0.4450\n",
      "Epoch: 232, Loss: 0.0590, Train: 1.0000, Val: 0.4300, Test: 0.4450\n",
      "Epoch: 233, Loss: 0.0735, Train: 1.0000, Val: 0.4300, Test: 0.4450\n",
      "Epoch: 234, Loss: 0.0726, Train: 1.0000, Val: 0.4500, Test: 0.4450\n",
      "Epoch: 235, Loss: 0.0752, Train: 1.0000, Val: 0.4400, Test: 0.4450\n",
      "Epoch: 236, Loss: 0.0640, Train: 1.0000, Val: 0.4300, Test: 0.4450\n",
      "Epoch: 237, Loss: 0.0755, Train: 1.0000, Val: 0.4100, Test: 0.4450\n",
      "Epoch: 238, Loss: 0.0677, Train: 1.0000, Val: 0.4300, Test: 0.4450\n",
      "Epoch: 239, Loss: 0.0666, Train: 1.0000, Val: 0.4500, Test: 0.4450\n",
      "Epoch: 240, Loss: 0.0718, Train: 1.0000, Val: 0.4500, Test: 0.4450\n",
      "Epoch: 241, Loss: 0.0633, Train: 1.0000, Val: 0.4500, Test: 0.4450\n",
      "Epoch: 242, Loss: 0.0710, Train: 1.0000, Val: 0.4400, Test: 0.4450\n",
      "Epoch: 243, Loss: 0.0642, Train: 1.0000, Val: 0.4400, Test: 0.4450\n",
      "Epoch: 244, Loss: 0.0540, Train: 1.0000, Val: 0.4400, Test: 0.4450\n",
      "Epoch: 245, Loss: 0.0636, Train: 1.0000, Val: 0.4600, Test: 0.4450\n",
      "Epoch: 246, Loss: 0.0708, Train: 1.0000, Val: 0.4500, Test: 0.4450\n",
      "Epoch: 247, Loss: 0.0792, Train: 1.0000, Val: 0.4500, Test: 0.4450\n",
      "Epoch: 248, Loss: 0.0638, Train: 1.0000, Val: 0.4500, Test: 0.4450\n",
      "Epoch: 249, Loss: 0.0679, Train: 1.0000, Val: 0.4700, Test: 0.4450\n",
      "Epoch: 250, Loss: 0.0726, Train: 1.0000, Val: 0.4600, Test: 0.4450\n",
      "Epoch: 251, Loss: 0.0738, Train: 1.0000, Val: 0.4600, Test: 0.4450\n",
      "Epoch: 252, Loss: 0.0700, Train: 1.0000, Val: 0.4500, Test: 0.4450\n",
      "Epoch: 253, Loss: 0.0638, Train: 1.0000, Val: 0.4400, Test: 0.4450\n",
      "Epoch: 254, Loss: 0.0668, Train: 1.0000, Val: 0.4500, Test: 0.4450\n",
      "Epoch: 255, Loss: 0.0734, Train: 1.0000, Val: 0.4600, Test: 0.4450\n",
      "Epoch: 256, Loss: 0.0695, Train: 1.0000, Val: 0.4600, Test: 0.4450\n",
      "Epoch: 257, Loss: 0.0676, Train: 1.0000, Val: 0.4200, Test: 0.4450\n",
      "Epoch: 258, Loss: 0.0737, Train: 1.0000, Val: 0.4300, Test: 0.4450\n",
      "Epoch: 259, Loss: 0.0598, Train: 1.0000, Val: 0.4500, Test: 0.4450\n",
      "Epoch: 260, Loss: 0.0607, Train: 1.0000, Val: 0.4600, Test: 0.4450\n",
      "Epoch: 261, Loss: 0.0662, Train: 1.0000, Val: 0.4800, Test: 0.4450\n",
      "Epoch: 262, Loss: 0.0680, Train: 1.0000, Val: 0.4400, Test: 0.4450\n",
      "Epoch: 263, Loss: 0.0663, Train: 1.0000, Val: 0.4300, Test: 0.4450\n",
      "Epoch: 264, Loss: 0.0613, Train: 1.0000, Val: 0.4300, Test: 0.4450\n",
      "Epoch: 265, Loss: 0.0748, Train: 1.0000, Val: 0.4200, Test: 0.4450\n",
      "Epoch: 266, Loss: 0.0685, Train: 1.0000, Val: 0.4500, Test: 0.4450\n",
      "Epoch: 267, Loss: 0.0657, Train: 1.0000, Val: 0.4400, Test: 0.4450\n",
      "Epoch: 268, Loss: 0.0598, Train: 1.0000, Val: 0.4300, Test: 0.4450\n",
      "Epoch: 269, Loss: 0.0651, Train: 1.0000, Val: 0.4400, Test: 0.4450\n",
      "Epoch: 270, Loss: 0.0585, Train: 1.0000, Val: 0.4400, Test: 0.4450\n",
      "Epoch: 271, Loss: 0.0607, Train: 1.0000, Val: 0.4600, Test: 0.4450\n",
      "Epoch: 272, Loss: 0.0661, Train: 1.0000, Val: 0.4600, Test: 0.4450\n",
      "Epoch: 273, Loss: 0.0574, Train: 1.0000, Val: 0.4600, Test: 0.4450\n",
      "Epoch: 274, Loss: 0.0673, Train: 1.0000, Val: 0.4500, Test: 0.4450\n",
      "Epoch: 275, Loss: 0.0555, Train: 1.0000, Val: 0.4500, Test: 0.4450\n",
      "Epoch: 276, Loss: 0.0645, Train: 1.0000, Val: 0.4500, Test: 0.4450\n",
      "Epoch: 277, Loss: 0.0610, Train: 1.0000, Val: 0.4600, Test: 0.4450\n",
      "Epoch: 278, Loss: 0.0612, Train: 1.0000, Val: 0.4600, Test: 0.4450\n",
      "Epoch: 279, Loss: 0.0525, Train: 1.0000, Val: 0.4600, Test: 0.4450\n",
      "Epoch: 280, Loss: 0.0582, Train: 1.0000, Val: 0.4400, Test: 0.4450\n",
      "Epoch: 281, Loss: 0.0556, Train: 1.0000, Val: 0.4400, Test: 0.4450\n",
      "Epoch: 282, Loss: 0.0557, Train: 1.0000, Val: 0.4200, Test: 0.4450\n",
      "Epoch: 283, Loss: 0.0811, Train: 1.0000, Val: 0.4400, Test: 0.4450\n",
      "Epoch: 284, Loss: 0.0680, Train: 1.0000, Val: 0.4500, Test: 0.4450\n",
      "Epoch: 285, Loss: 0.0471, Train: 1.0000, Val: 0.4400, Test: 0.4450\n",
      "Epoch: 286, Loss: 0.0690, Train: 1.0000, Val: 0.4300, Test: 0.4450\n",
      "Epoch: 287, Loss: 0.0720, Train: 1.0000, Val: 0.4300, Test: 0.4450\n",
      "Epoch: 288, Loss: 0.0714, Train: 1.0000, Val: 0.4300, Test: 0.4450\n",
      "Epoch: 289, Loss: 0.0730, Train: 1.0000, Val: 0.4400, Test: 0.4450\n",
      "Epoch: 290, Loss: 0.0629, Train: 1.0000, Val: 0.4500, Test: 0.4450\n",
      "Epoch: 291, Loss: 0.0561, Train: 1.0000, Val: 0.4600, Test: 0.4450\n",
      "Epoch: 292, Loss: 0.0583, Train: 1.0000, Val: 0.4600, Test: 0.4450\n",
      "Epoch: 293, Loss: 0.0660, Train: 1.0000, Val: 0.4600, Test: 0.4450\n",
      "Epoch: 294, Loss: 0.0595, Train: 1.0000, Val: 0.4600, Test: 0.4450\n",
      "Epoch: 295, Loss: 0.0565, Train: 1.0000, Val: 0.4700, Test: 0.4450\n",
      "Epoch: 296, Loss: 0.0509, Train: 1.0000, Val: 0.4800, Test: 0.4450\n",
      "Epoch: 297, Loss: 0.0521, Train: 1.0000, Val: 0.4800, Test: 0.4450\n",
      "Epoch: 298, Loss: 0.0641, Train: 1.0000, Val: 0.4800, Test: 0.4450\n",
      "Epoch: 299, Loss: 0.0572, Train: 1.0000, Val: 0.4900, Test: 0.4450\n",
      "Epoch: 300, Loss: 0.0675, Train: 1.0000, Val: 0.4700, Test: 0.4450\n",
      "Epoch: 301, Loss: 0.0541, Train: 1.0000, Val: 0.4700, Test: 0.4450\n",
      "Epoch: 302, Loss: 0.0641, Train: 1.0000, Val: 0.4800, Test: 0.4450\n",
      "Epoch: 303, Loss: 0.0797, Train: 1.0000, Val: 0.4800, Test: 0.4450\n",
      "Epoch: 304, Loss: 0.0591, Train: 1.0000, Val: 0.4600, Test: 0.4450\n",
      "Epoch: 305, Loss: 0.0521, Train: 1.0000, Val: 0.4600, Test: 0.4450\n",
      "Epoch: 306, Loss: 0.0680, Train: 1.0000, Val: 0.5200, Test: 0.4450\n",
      "Epoch: 307, Loss: 0.0693, Train: 1.0000, Val: 0.4900, Test: 0.4450\n",
      "Epoch: 308, Loss: 0.0733, Train: 1.0000, Val: 0.4800, Test: 0.4450\n",
      "Epoch: 309, Loss: 0.0675, Train: 1.0000, Val: 0.4800, Test: 0.4450\n",
      "Epoch: 310, Loss: 0.0534, Train: 1.0000, Val: 0.4900, Test: 0.4450\n",
      "Epoch: 311, Loss: 0.0626, Train: 1.0000, Val: 0.4900, Test: 0.4450\n",
      "Epoch: 312, Loss: 0.0582, Train: 1.0000, Val: 0.4800, Test: 0.4450\n",
      "Epoch: 313, Loss: 0.0544, Train: 1.0000, Val: 0.4800, Test: 0.4450\n",
      "Epoch: 314, Loss: 0.0574, Train: 1.0000, Val: 0.4800, Test: 0.4450\n",
      "Epoch: 315, Loss: 0.0523, Train: 1.0000, Val: 0.4600, Test: 0.4450\n",
      "Epoch: 316, Loss: 0.0622, Train: 1.0000, Val: 0.4800, Test: 0.4450\n",
      "Epoch: 317, Loss: 0.0550, Train: 1.0000, Val: 0.5100, Test: 0.4450\n",
      "Epoch: 318, Loss: 0.0597, Train: 1.0000, Val: 0.4900, Test: 0.4450\n",
      "Epoch: 319, Loss: 0.0550, Train: 1.0000, Val: 0.4600, Test: 0.4450\n",
      "Epoch: 320, Loss: 0.0580, Train: 1.0000, Val: 0.4400, Test: 0.4450\n",
      "Epoch: 321, Loss: 0.0608, Train: 1.0000, Val: 0.4500, Test: 0.4450\n",
      "Epoch: 322, Loss: 0.0614, Train: 1.0000, Val: 0.4500, Test: 0.4450\n",
      "Epoch: 323, Loss: 0.0587, Train: 1.0000, Val: 0.4800, Test: 0.4450\n",
      "Epoch: 324, Loss: 0.0576, Train: 1.0000, Val: 0.4700, Test: 0.4450\n",
      "Epoch: 325, Loss: 0.0533, Train: 1.0000, Val: 0.4600, Test: 0.4450\n",
      "Epoch: 326, Loss: 0.0478, Train: 1.0000, Val: 0.4500, Test: 0.4450\n",
      "Epoch: 327, Loss: 0.0630, Train: 1.0000, Val: 0.4400, Test: 0.4450\n",
      "Epoch: 328, Loss: 0.0582, Train: 1.0000, Val: 0.4700, Test: 0.4450\n",
      "Epoch: 329, Loss: 0.0593, Train: 1.0000, Val: 0.4900, Test: 0.4450\n",
      "Epoch: 330, Loss: 0.0562, Train: 1.0000, Val: 0.4900, Test: 0.4450\n",
      "Epoch: 331, Loss: 0.0543, Train: 1.0000, Val: 0.4800, Test: 0.4450\n",
      "Epoch: 332, Loss: 0.0576, Train: 1.0000, Val: 0.4700, Test: 0.4450\n",
      "Epoch: 333, Loss: 0.0578, Train: 1.0000, Val: 0.4900, Test: 0.4450\n",
      "Epoch: 334, Loss: 0.0606, Train: 1.0000, Val: 0.4700, Test: 0.4450\n",
      "Epoch: 335, Loss: 0.0574, Train: 1.0000, Val: 0.4700, Test: 0.4450\n",
      "Epoch: 336, Loss: 0.0661, Train: 1.0000, Val: 0.4700, Test: 0.4450\n",
      "Epoch: 337, Loss: 0.0573, Train: 1.0000, Val: 0.4700, Test: 0.4450\n",
      "Epoch: 338, Loss: 0.0652, Train: 1.0000, Val: 0.4600, Test: 0.4450\n",
      "Epoch: 339, Loss: 0.0590, Train: 1.0000, Val: 0.4800, Test: 0.4450\n",
      "Epoch: 340, Loss: 0.0563, Train: 1.0000, Val: 0.5000, Test: 0.4450\n",
      "Epoch: 341, Loss: 0.0532, Train: 1.0000, Val: 0.4900, Test: 0.4450\n",
      "Epoch: 342, Loss: 0.0497, Train: 1.0000, Val: 0.4900, Test: 0.4450\n",
      "Epoch: 343, Loss: 0.0528, Train: 1.0000, Val: 0.4800, Test: 0.4450\n",
      "Epoch: 344, Loss: 0.0491, Train: 1.0000, Val: 0.4700, Test: 0.4450\n",
      "Epoch: 345, Loss: 0.0579, Train: 1.0000, Val: 0.4900, Test: 0.4450\n",
      "Epoch: 346, Loss: 0.0506, Train: 1.0000, Val: 0.5000, Test: 0.4450\n",
      "Epoch: 347, Loss: 0.0564, Train: 1.0000, Val: 0.5000, Test: 0.4450\n",
      "Epoch: 348, Loss: 0.0538, Train: 1.0000, Val: 0.5300, Test: 0.4450\n",
      "Epoch: 349, Loss: 0.0546, Train: 1.0000, Val: 0.5200, Test: 0.4450\n",
      "Epoch: 350, Loss: 0.0700, Train: 1.0000, Val: 0.5100, Test: 0.4450\n",
      "Epoch: 351, Loss: 0.0632, Train: 1.0000, Val: 0.5100, Test: 0.4450\n",
      "Epoch: 352, Loss: 0.0586, Train: 1.0000, Val: 0.4900, Test: 0.4450\n",
      "Epoch: 353, Loss: 0.0638, Train: 1.0000, Val: 0.5000, Test: 0.4450\n",
      "Epoch: 354, Loss: 0.0638, Train: 1.0000, Val: 0.5100, Test: 0.4450\n",
      "Epoch: 355, Loss: 0.0564, Train: 1.0000, Val: 0.5100, Test: 0.4450\n",
      "Epoch: 356, Loss: 0.0618, Train: 1.0000, Val: 0.5200, Test: 0.4450\n",
      "Epoch: 357, Loss: 0.0441, Train: 1.0000, Val: 0.5200, Test: 0.4450\n",
      "Epoch: 358, Loss: 0.0470, Train: 1.0000, Val: 0.5200, Test: 0.4450\n",
      "Epoch: 359, Loss: 0.0554, Train: 1.0000, Val: 0.5100, Test: 0.4450\n",
      "Epoch: 360, Loss: 0.0526, Train: 1.0000, Val: 0.5300, Test: 0.4450\n",
      "Epoch: 361, Loss: 0.0492, Train: 1.0000, Val: 0.5200, Test: 0.4450\n",
      "Epoch: 362, Loss: 0.0537, Train: 1.0000, Val: 0.5100, Test: 0.4450\n",
      "Epoch: 363, Loss: 0.0611, Train: 1.0000, Val: 0.5200, Test: 0.4450\n",
      "Epoch: 364, Loss: 0.0552, Train: 1.0000, Val: 0.5200, Test: 0.4450\n",
      "Epoch: 365, Loss: 0.0532, Train: 1.0000, Val: 0.4900, Test: 0.4450\n",
      "Epoch: 366, Loss: 0.0549, Train: 1.0000, Val: 0.5100, Test: 0.4450\n",
      "Epoch: 367, Loss: 0.0472, Train: 1.0000, Val: 0.5000, Test: 0.4450\n",
      "Epoch: 368, Loss: 0.0464, Train: 1.0000, Val: 0.5100, Test: 0.4450\n",
      "Epoch: 369, Loss: 0.0650, Train: 1.0000, Val: 0.5200, Test: 0.4450\n",
      "Epoch: 370, Loss: 0.0641, Train: 1.0000, Val: 0.5000, Test: 0.4450\n",
      "Epoch: 371, Loss: 0.0522, Train: 1.0000, Val: 0.5000, Test: 0.4450\n",
      "Epoch: 372, Loss: 0.0558, Train: 1.0000, Val: 0.5000, Test: 0.4450\n",
      "Epoch: 373, Loss: 0.0564, Train: 1.0000, Val: 0.5100, Test: 0.4450\n",
      "Epoch: 374, Loss: 0.0502, Train: 1.0000, Val: 0.5000, Test: 0.4450\n",
      "Epoch: 375, Loss: 0.0566, Train: 1.0000, Val: 0.5100, Test: 0.4450\n",
      "Epoch: 376, Loss: 0.0665, Train: 1.0000, Val: 0.5200, Test: 0.4450\n",
      "Epoch: 377, Loss: 0.0511, Train: 1.0000, Val: 0.5100, Test: 0.4450\n",
      "Epoch: 378, Loss: 0.0521, Train: 1.0000, Val: 0.5000, Test: 0.4450\n",
      "Epoch: 379, Loss: 0.0567, Train: 1.0000, Val: 0.4800, Test: 0.4450\n",
      "Epoch: 380, Loss: 0.0492, Train: 1.0000, Val: 0.4800, Test: 0.4450\n",
      "Epoch: 381, Loss: 0.0510, Train: 1.0000, Val: 0.4900, Test: 0.4450\n",
      "Epoch: 382, Loss: 0.0515, Train: 1.0000, Val: 0.5100, Test: 0.4450\n",
      "Epoch: 383, Loss: 0.0503, Train: 1.0000, Val: 0.5200, Test: 0.4450\n",
      "Epoch: 384, Loss: 0.0495, Train: 1.0000, Val: 0.5100, Test: 0.4450\n",
      "Epoch: 385, Loss: 0.0539, Train: 1.0000, Val: 0.4700, Test: 0.4450\n",
      "Epoch: 386, Loss: 0.0646, Train: 1.0000, Val: 0.4600, Test: 0.4450\n",
      "Epoch: 387, Loss: 0.0564, Train: 1.0000, Val: 0.4700, Test: 0.4450\n",
      "Epoch: 388, Loss: 0.0514, Train: 1.0000, Val: 0.5200, Test: 0.4450\n",
      "Epoch: 389, Loss: 0.0619, Train: 1.0000, Val: 0.4800, Test: 0.4450\n",
      "Epoch: 390, Loss: 0.0621, Train: 1.0000, Val: 0.4600, Test: 0.4450\n",
      "Epoch: 391, Loss: 0.0463, Train: 1.0000, Val: 0.4700, Test: 0.4450\n",
      "Epoch: 392, Loss: 0.0503, Train: 1.0000, Val: 0.4700, Test: 0.4450\n",
      "Epoch: 393, Loss: 0.0465, Train: 1.0000, Val: 0.4700, Test: 0.4450\n",
      "Epoch: 394, Loss: 0.0633, Train: 1.0000, Val: 0.4900, Test: 0.4450\n",
      "Epoch: 395, Loss: 0.0652, Train: 1.0000, Val: 0.4700, Test: 0.4450\n",
      "Epoch: 396, Loss: 0.0578, Train: 1.0000, Val: 0.4400, Test: 0.4450\n",
      "Epoch: 397, Loss: 0.0547, Train: 1.0000, Val: 0.4500, Test: 0.4450\n",
      "Epoch: 398, Loss: 0.0668, Train: 1.0000, Val: 0.4400, Test: 0.4450\n",
      "Epoch: 399, Loss: 0.0498, Train: 1.0000, Val: 0.4600, Test: 0.4450\n",
      "Epoch: 400, Loss: 0.0511, Train: 1.0000, Val: 0.4600, Test: 0.4450\n",
      "Epoch: 401, Loss: 0.0599, Train: 1.0000, Val: 0.4700, Test: 0.4450\n",
      "Epoch: 402, Loss: 0.0478, Train: 1.0000, Val: 0.4600, Test: 0.4450\n",
      "Epoch: 403, Loss: 0.0520, Train: 1.0000, Val: 0.4700, Test: 0.4450\n",
      "Epoch: 404, Loss: 0.0455, Train: 1.0000, Val: 0.4800, Test: 0.4450\n",
      "Epoch: 405, Loss: 0.0572, Train: 1.0000, Val: 0.4900, Test: 0.4450\n",
      "Epoch: 406, Loss: 0.0544, Train: 1.0000, Val: 0.5000, Test: 0.4450\n",
      "Epoch: 407, Loss: 0.0497, Train: 1.0000, Val: 0.5100, Test: 0.4450\n",
      "Epoch: 408, Loss: 0.0477, Train: 1.0000, Val: 0.4800, Test: 0.4450\n",
      "Epoch: 409, Loss: 0.0391, Train: 1.0000, Val: 0.4700, Test: 0.4450\n",
      "Epoch: 410, Loss: 0.0433, Train: 1.0000, Val: 0.4900, Test: 0.4450\n",
      "Epoch: 411, Loss: 0.0553, Train: 1.0000, Val: 0.4900, Test: 0.4450\n",
      "Epoch: 412, Loss: 0.0639, Train: 1.0000, Val: 0.4900, Test: 0.4450\n",
      "Epoch: 413, Loss: 0.0522, Train: 1.0000, Val: 0.5000, Test: 0.4450\n",
      "Epoch: 414, Loss: 0.0584, Train: 1.0000, Val: 0.5000, Test: 0.4450\n",
      "Epoch: 415, Loss: 0.0531, Train: 1.0000, Val: 0.4900, Test: 0.4450\n",
      "Epoch: 416, Loss: 0.0489, Train: 1.0000, Val: 0.4800, Test: 0.4450\n",
      "Epoch: 417, Loss: 0.0475, Train: 1.0000, Val: 0.4900, Test: 0.4450\n",
      "Epoch: 418, Loss: 0.0500, Train: 1.0000, Val: 0.4900, Test: 0.4450\n",
      "Epoch: 419, Loss: 0.0588, Train: 1.0000, Val: 0.4900, Test: 0.4450\n",
      "Epoch: 420, Loss: 0.0570, Train: 1.0000, Val: 0.4600, Test: 0.4450\n",
      "Epoch: 421, Loss: 0.0627, Train: 1.0000, Val: 0.4800, Test: 0.4450\n",
      "Epoch: 422, Loss: 0.0594, Train: 1.0000, Val: 0.4700, Test: 0.4450\n",
      "Epoch: 423, Loss: 0.0511, Train: 1.0000, Val: 0.5100, Test: 0.4450\n",
      "Epoch: 424, Loss: 0.0505, Train: 1.0000, Val: 0.5100, Test: 0.4450\n",
      "Epoch: 425, Loss: 0.0490, Train: 1.0000, Val: 0.4800, Test: 0.4450\n",
      "Epoch: 426, Loss: 0.0483, Train: 1.0000, Val: 0.4700, Test: 0.4450\n",
      "Epoch: 427, Loss: 0.0661, Train: 1.0000, Val: 0.4700, Test: 0.4450\n",
      "Epoch: 428, Loss: 0.0468, Train: 1.0000, Val: 0.4600, Test: 0.4450\n",
      "Epoch: 429, Loss: 0.0457, Train: 1.0000, Val: 0.4600, Test: 0.4450\n",
      "Epoch: 430, Loss: 0.0459, Train: 1.0000, Val: 0.4600, Test: 0.4450\n",
      "Epoch: 431, Loss: 0.0498, Train: 1.0000, Val: 0.4700, Test: 0.4450\n",
      "Epoch: 432, Loss: 0.0483, Train: 1.0000, Val: 0.4900, Test: 0.4450\n",
      "Epoch: 433, Loss: 0.0483, Train: 1.0000, Val: 0.4800, Test: 0.4450\n",
      "Epoch: 434, Loss: 0.0487, Train: 1.0000, Val: 0.4800, Test: 0.4450\n",
      "Epoch: 435, Loss: 0.0575, Train: 1.0000, Val: 0.5000, Test: 0.4450\n",
      "Epoch: 436, Loss: 0.0457, Train: 1.0000, Val: 0.5200, Test: 0.4450\n",
      "Epoch: 437, Loss: 0.0657, Train: 1.0000, Val: 0.5100, Test: 0.4450\n",
      "Epoch: 438, Loss: 0.0491, Train: 1.0000, Val: 0.4900, Test: 0.4450\n",
      "Epoch: 439, Loss: 0.0475, Train: 1.0000, Val: 0.4900, Test: 0.4450\n",
      "Epoch: 440, Loss: 0.0541, Train: 1.0000, Val: 0.4800, Test: 0.4450\n",
      "Epoch: 441, Loss: 0.0476, Train: 1.0000, Val: 0.5000, Test: 0.4450\n",
      "Epoch: 442, Loss: 0.0491, Train: 1.0000, Val: 0.5000, Test: 0.4450\n",
      "Epoch: 443, Loss: 0.0642, Train: 1.0000, Val: 0.4900, Test: 0.4450\n",
      "Epoch: 444, Loss: 0.0487, Train: 1.0000, Val: 0.4900, Test: 0.4450\n",
      "Epoch: 445, Loss: 0.0617, Train: 1.0000, Val: 0.5100, Test: 0.4450\n",
      "Epoch: 446, Loss: 0.0513, Train: 1.0000, Val: 0.5100, Test: 0.4450\n",
      "Epoch: 447, Loss: 0.0644, Train: 1.0000, Val: 0.4700, Test: 0.4450\n",
      "Epoch: 448, Loss: 0.0450, Train: 1.0000, Val: 0.4600, Test: 0.4450\n",
      "Epoch: 449, Loss: 0.0540, Train: 1.0000, Val: 0.4800, Test: 0.4450\n",
      "Epoch: 450, Loss: 0.0566, Train: 1.0000, Val: 0.4800, Test: 0.4450\n",
      "Epoch: 451, Loss: 0.0532, Train: 1.0000, Val: 0.4800, Test: 0.4450\n",
      "Epoch: 452, Loss: 0.0562, Train: 1.0000, Val: 0.5100, Test: 0.4450\n",
      "Epoch: 453, Loss: 0.0518, Train: 1.0000, Val: 0.4800, Test: 0.4450\n",
      "Epoch: 454, Loss: 0.0523, Train: 1.0000, Val: 0.5100, Test: 0.4450\n",
      "Epoch: 455, Loss: 0.0413, Train: 1.0000, Val: 0.4900, Test: 0.4450\n",
      "Epoch: 456, Loss: 0.0501, Train: 1.0000, Val: 0.4700, Test: 0.4450\n",
      "Epoch: 457, Loss: 0.0577, Train: 1.0000, Val: 0.4700, Test: 0.4450\n",
      "Epoch: 458, Loss: 0.0573, Train: 1.0000, Val: 0.4700, Test: 0.4450\n",
      "Epoch: 459, Loss: 0.0422, Train: 1.0000, Val: 0.4800, Test: 0.4450\n",
      "Epoch: 460, Loss: 0.0610, Train: 1.0000, Val: 0.4600, Test: 0.4450\n",
      "Epoch: 461, Loss: 0.0523, Train: 1.0000, Val: 0.4800, Test: 0.4450\n",
      "Epoch: 462, Loss: 0.0478, Train: 1.0000, Val: 0.4800, Test: 0.4450\n",
      "Epoch: 463, Loss: 0.0573, Train: 1.0000, Val: 0.4900, Test: 0.4450\n",
      "Epoch: 464, Loss: 0.0590, Train: 1.0000, Val: 0.4900, Test: 0.4450\n",
      "Epoch: 465, Loss: 0.0590, Train: 1.0000, Val: 0.4900, Test: 0.4450\n",
      "Epoch: 466, Loss: 0.0540, Train: 1.0000, Val: 0.4700, Test: 0.4450\n",
      "Epoch: 467, Loss: 0.0489, Train: 1.0000, Val: 0.4800, Test: 0.4450\n",
      "Epoch: 468, Loss: 0.0423, Train: 1.0000, Val: 0.4700, Test: 0.4450\n",
      "Epoch: 469, Loss: 0.0530, Train: 1.0000, Val: 0.4600, Test: 0.4450\n",
      "Epoch: 470, Loss: 0.0513, Train: 1.0000, Val: 0.4700, Test: 0.4450\n",
      "Epoch: 471, Loss: 0.0526, Train: 1.0000, Val: 0.4700, Test: 0.4450\n",
      "Epoch: 472, Loss: 0.0496, Train: 1.0000, Val: 0.4500, Test: 0.4450\n",
      "Epoch: 473, Loss: 0.0645, Train: 1.0000, Val: 0.4500, Test: 0.4450\n",
      "Epoch: 474, Loss: 0.0413, Train: 1.0000, Val: 0.4800, Test: 0.4450\n",
      "Epoch: 475, Loss: 0.0586, Train: 1.0000, Val: 0.4600, Test: 0.4450\n",
      "Epoch: 476, Loss: 0.0495, Train: 1.0000, Val: 0.4700, Test: 0.4450\n",
      "Epoch: 477, Loss: 0.0504, Train: 1.0000, Val: 0.4900, Test: 0.4450\n",
      "Epoch: 478, Loss: 0.0545, Train: 1.0000, Val: 0.4800, Test: 0.4450\n",
      "Epoch: 479, Loss: 0.0540, Train: 1.0000, Val: 0.4600, Test: 0.4450\n",
      "Epoch: 480, Loss: 0.0404, Train: 1.0000, Val: 0.4600, Test: 0.4450\n",
      "Epoch: 481, Loss: 0.0422, Train: 1.0000, Val: 0.4500, Test: 0.4450\n",
      "Epoch: 482, Loss: 0.0535, Train: 1.0000, Val: 0.4700, Test: 0.4450\n",
      "Epoch: 483, Loss: 0.0533, Train: 1.0000, Val: 0.4600, Test: 0.4450\n",
      "Epoch: 484, Loss: 0.0525, Train: 1.0000, Val: 0.4700, Test: 0.4450\n",
      "Epoch: 485, Loss: 0.0467, Train: 1.0000, Val: 0.4600, Test: 0.4450\n",
      "Epoch: 486, Loss: 0.0490, Train: 1.0000, Val: 0.4800, Test: 0.4450\n",
      "Epoch: 487, Loss: 0.0518, Train: 1.0000, Val: 0.4500, Test: 0.4450\n",
      "Epoch: 488, Loss: 0.0505, Train: 1.0000, Val: 0.4500, Test: 0.4450\n",
      "Epoch: 489, Loss: 0.0550, Train: 1.0000, Val: 0.4800, Test: 0.4450\n",
      "Epoch: 490, Loss: 0.0517, Train: 1.0000, Val: 0.4600, Test: 0.4450\n",
      "Epoch: 491, Loss: 0.0516, Train: 1.0000, Val: 0.4600, Test: 0.4450\n",
      "Epoch: 492, Loss: 0.0468, Train: 1.0000, Val: 0.4800, Test: 0.4450\n",
      "Epoch: 493, Loss: 0.0508, Train: 1.0000, Val: 0.4900, Test: 0.4450\n",
      "Epoch: 494, Loss: 0.0574, Train: 1.0000, Val: 0.4900, Test: 0.4450\n",
      "Epoch: 495, Loss: 0.0473, Train: 1.0000, Val: 0.4900, Test: 0.4450\n",
      "Epoch: 496, Loss: 0.0467, Train: 1.0000, Val: 0.4500, Test: 0.4450\n",
      "Epoch: 497, Loss: 0.0589, Train: 1.0000, Val: 0.4800, Test: 0.4450\n",
      "Epoch: 498, Loss: 0.0513, Train: 1.0000, Val: 0.5000, Test: 0.4450\n",
      "Epoch: 499, Loss: 0.0436, Train: 1.0000, Val: 0.4900, Test: 0.4450\n",
      "Epoch: 500, Loss: 0.0530, Train: 1.0000, Val: 0.4700, Test: 0.4450\n",
      "Median time per epoch: 0.0589s\n"
     ]
    }
   ],
   "source": [
    "times = []\n",
    "best_val_acc = final_test_acc = 0\n",
    "for epoch in range(1, 501):\n",
    "    start = time.time()\n",
    "    loss = train()\n",
    "    train_acc, val_acc, tmp_test_acc = test()\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        test_acc = tmp_test_acc\n",
    "        torch.save(model.state_dict(), f'processed_data/model_weights_rgcn_{num_patients}.pth')\n",
    "    log(Epoch=epoch, Loss=loss, Train=train_acc, Val=val_acc, Test=test_acc)\n",
    "    times.append(time.time() - start)\n",
    "print(f\"Median time per epoch: {torch.tensor(times).median():.4f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC score: 0.5396\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "model = Net().to(device)\n",
    "model.load_state_dict(torch.load(f'processed_data/model_weights_rgcn_{num_patients}.pth', weights_only=True))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(data.edge_index, data.edge_type).cpu()\n",
    "    prob = F.softmax(out, dim=1)\n",
    "auc = roc_auc_score(data.test_y.cpu(), prob[data.test_idx.cpu()], multi_class='ovr')\n",
    "print(f'ROC AUC score: {auc:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neurovasc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
