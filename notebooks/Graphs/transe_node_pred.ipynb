{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outcome Prediction (Node Classification) with TransE Model + MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pykeen.triples import TriplesFactory\n",
    "from pykeen.predict import predict_target\n",
    "from pykeen.pipeline import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load KG and remove outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<http://nvasc.org/f0ebdaad-cbff-4146-9f6c-cd36ca2fa6a9> <http://sphn.org/hasSubjectPseudoIdentifier> <http://nvasc.org/synth_patient_655> .\n",
      "\n",
      "                                                   s  \\\n",
      "0  <http://nvasc.org/5520b794-ce4d-429b-989b-f50a...   \n",
      "1  <http://nvasc.org/e7e35917-43e4-43b0-ae9b-03ef...   \n",
      "2  <http://nvasc.org/e4ee5cff-28ac-4166-8061-e702...   \n",
      "3  <http://nvasc.org/ff28cf13-ce6f-4ee1-9bb7-1e27...   \n",
      "4              _:n666407211fed47599e420f9104257e72b1   \n",
      "\n",
      "                                                   r  \\\n",
      "0  <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>   \n",
      "1                      <http://sphn.org/hasQuantity>   \n",
      "2                          <http://sphn.org/hasCode>   \n",
      "3                      <http://sphn.org/hasQuantity>   \n",
      "4                         <http://sphn.org/hasValue>   \n",
      "\n",
      "                                       d  \n",
      "0            <http://sphn.org/Diagnosis>  \n",
      "1  _:n05ec718cd7f94fe28f83910c54f4011bb1  \n",
      "2        <http://nvasc.org/code_hct_0.0>  \n",
      "3  _:na54f2ce404eb4f4498dec936c73c132cb1  \n",
      "4                                   \"32\"  \n"
     ]
    }
   ],
   "source": [
    "dataset = '../Data Generation/sphn_transductive_1000_0.nt'\n",
    "num_patients = 1000\n",
    "\n",
    "def preprocess(data_name):\n",
    "    s_list, r_list, d_list = [], [], []\n",
    "    \n",
    "    with open(data_name) as f:\n",
    "        s = next(f)\n",
    "        print(s)\n",
    "        for idx, line in enumerate(f):\n",
    "            e = line.strip().split(' ')\n",
    "            s = e[0]\n",
    "            r = e[1]\n",
    "            d = e[2]            \n",
    "            \n",
    "            s_list.append(s)\n",
    "            r_list.append(r)\n",
    "            d_list.append(d)            \n",
    "            \n",
    "    return pd.DataFrame({\n",
    "        's':s_list, \n",
    "        'r':r_list, \n",
    "        'd':d_list,                          \n",
    "        })\n",
    "\n",
    "df = preprocess(dataset)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove outcomes from KG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome = df['d'].str.contains('outcome_0.0|outcome_1.0|outcome_2.0')\n",
    "node_df = df[~outcome]\n",
    "node_df = node_df.reset_index(drop=True)\n",
    "outcome = node_df['s'].str.contains('outcome_0.0|outcome_1.0|outcome_2.0')\n",
    "node_df = node_df[~outcome]\n",
    "node_df = node_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_to_id = {k: v for v, k in enumerate(set(node_df['s']).union(set(node_df['d'])), start=0)}\n",
    "rel_to_id = {k: v for v, k in enumerate(set(node_df['r']), start=0)}\n",
    "\n",
    "patients = [f\"<http://nvasc.org/synth_patient_{i}>\" for i in range(num_patients)]\n",
    "patient_id = []\n",
    "for patient in patients:\n",
    "    patient_id.append(ent_to_id[patient])\n",
    "\n",
    "num_nodes = max(ent_to_id.values()) + 1\n",
    "num_rels = max(rel_to_id.values()) + 1\n",
    "\n",
    "events = node_df.copy()\n",
    "events[\"s\"] = node_df.s.map(ent_to_id)\n",
    "events[\"d\"] = node_df.d.map(ent_to_id)\n",
    "events[\"r\"] = node_df.r.map(rel_to_id)\n",
    "\n",
    "ent_to_id = pd.DataFrame.from_dict(ent_to_id, orient='index')\n",
    "rel_to_id = pd.DataFrame.from_dict(rel_to_id, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save events, entities and relations to 'processed_data' folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'processed_data'\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "events.to_csv(f'{path}/sphn_events_noOutcome.tsv', sep='\\t', index=False, header=None)\n",
    "ent_to_id.to_csv(f'{path}/sphn_entities_noOutcome.tsv', sep='\\t', header=None)\n",
    "rel_to_id.to_csv(f'{path}/sphn_relations_noOutcome.tsv', sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TriplesFactory(num_entities=32564, num_relations=16, create_inverse_triples=True, num_triples=111934, path=\"/home/baical77/projects/neurovasc/notebooks/Graphs/processed_data/sphn_events_noOutcome.tsv\")\n"
     ]
    }
   ],
   "source": [
    "path = 'processed_data'\n",
    "tf = TriplesFactory.from_path(f'{path}/sphn_events_noOutcome.tsv', create_inverse_triples=True)\n",
    "print(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No random seed is specified. Setting to 898004030.\n",
      "INFO:pykeen.triples.triples_factory:Creating inverse triples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4519c61e1339403c9de35b6a8e8dcc19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epochs on cuda:0:   0%|          | 0/50 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pykeen.triples.triples_factory:Creating inverse triples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51cf4c5b5b3348cba92fc6e4df2920cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating on cuda:0:   0%|          | 0.00/112k [00:00<?, ?triple/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pykeen.evaluation.evaluator:Evaluation took 192.25s seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = 'TransE'\n",
    "embedding_dim = 32\n",
    "epochs = 50\n",
    "\n",
    "result = pipeline(\n",
    "    training=tf,\n",
    "    testing=tf,\n",
    "    model=model_name,\n",
    "    model_kwargs=dict(\n",
    "        embedding_dim=embedding_dim,\n",
    "        loss=\"softplus\",\n",
    "    ),\n",
    "    training_kwargs=dict(\n",
    "        num_epochs=epochs,\n",
    "        label_smoothing=0.2,\n",
    "        use_tqdm_batch=False,\n",
    "    ),\n",
    "    optimizer_kwargs=dict(\n",
    "        lr=0.001,\n",
    "        weight_decay=1e-5,\n",
    "    ),\n",
    "    training_loop='sLCWA',\n",
    "    negative_sampler='basic',\n",
    "    device='gpu',\n",
    "    use_tqdm=True,\n",
    ")\n",
    "\n",
    "#plot loss\n",
    "loss_plot = result.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient embedding size: torch.Size([1000, 32])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "num_patients = 1000\n",
    "entity = pd.read_csv('processed_data/sphn_entities_noOutcome.tsv', sep='\\t', index_col=0, header=None)\n",
    "entity = entity.to_dict()[1]\n",
    "patient_id = []\n",
    "for i in range(num_patients):\n",
    "    idx = f'<http://nvasc.org/synth_patient_{i}>'\n",
    "    patient_id.append(entity[idx])\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = result.model\n",
    "entity_embedding = model.entity_representations[0](indices=None).detach().cpu()\n",
    "patient_embedding = entity_embedding[patient_id]\n",
    "print(f'Patient embedding size: {patient_embedding.shape}')\n",
    "\n",
    "patient_embedding = patient_embedding.to(device)\n",
    "y = joblib.load('../Data Generation/outcomes_1000_0.joblib')\n",
    "y = torch.Tensor(y).long().to(device)\n",
    "\n",
    "train_x, train_y = patient_embedding[:int(num_patients*0.8)], y[:int(num_patients*0.8)]\n",
    "val_x, val_y = patient_embedding[int(num_patients*0.8):int(num_patients*0.9)], y[int(num_patients*0.8):int(num_patients*0.9)]\n",
    "test_x, test_y = patient_embedding[int(num_patients*0.9):], y[int(num_patients*0.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Loss: 1.1102, Val_Loss: 1.1125, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 02, Loss: 1.1128, Val_Loss: 1.1122, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 03, Loss: 1.1105, Val_Loss: 1.1120, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 04, Loss: 1.1095, Val_Loss: 1.1117, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 05, Loss: 1.1105, Val_Loss: 1.1114, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 06, Loss: 1.1104, Val_Loss: 1.1112, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 07, Loss: 1.1110, Val_Loss: 1.1109, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 08, Loss: 1.1082, Val_Loss: 1.1107, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 09, Loss: 1.1120, Val_Loss: 1.1104, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 10, Loss: 1.1114, Val_Loss: 1.1102, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 11, Loss: 1.1076, Val_Loss: 1.1099, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 12, Loss: 1.1111, Val_Loss: 1.1097, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 13, Loss: 1.1054, Val_Loss: 1.1094, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 14, Loss: 1.1053, Val_Loss: 1.1092, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 15, Loss: 1.1071, Val_Loss: 1.1089, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 16, Loss: 1.1050, Val_Loss: 1.1087, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 17, Loss: 1.1057, Val_Loss: 1.1084, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 18, Loss: 1.1090, Val_Loss: 1.1082, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 19, Loss: 1.1058, Val_Loss: 1.1079, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 20, Loss: 1.1097, Val_Loss: 1.1077, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 21, Loss: 1.1052, Val_Loss: 1.1074, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 22, Loss: 1.1045, Val_Loss: 1.1072, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 23, Loss: 1.1052, Val_Loss: 1.1070, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 24, Loss: 1.1079, Val_Loss: 1.1067, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 25, Loss: 1.1078, Val_Loss: 1.1065, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 26, Loss: 1.1061, Val_Loss: 1.1062, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 27, Loss: 1.1034, Val_Loss: 1.1060, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 28, Loss: 1.1052, Val_Loss: 1.1058, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 29, Loss: 1.1049, Val_Loss: 1.1055, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 30, Loss: 1.1054, Val_Loss: 1.1053, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 31, Loss: 1.1017, Val_Loss: 1.1050, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 32, Loss: 1.1064, Val_Loss: 1.1048, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 33, Loss: 1.1030, Val_Loss: 1.1046, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 34, Loss: 1.1049, Val_Loss: 1.1043, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 35, Loss: 1.1005, Val_Loss: 1.1041, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 36, Loss: 1.1056, Val_Loss: 1.1039, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 37, Loss: 1.1046, Val_Loss: 1.1036, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 38, Loss: 1.0996, Val_Loss: 1.1034, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 39, Loss: 1.1027, Val_Loss: 1.1031, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 40, Loss: 1.1016, Val_Loss: 1.1029, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 41, Loss: 1.0989, Val_Loss: 1.1027, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 42, Loss: 1.0973, Val_Loss: 1.1024, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 43, Loss: 1.1021, Val_Loss: 1.1022, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 44, Loss: 1.1026, Val_Loss: 1.1020, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 45, Loss: 1.1021, Val_Loss: 1.1017, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 46, Loss: 1.1024, Val_Loss: 1.1015, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 47, Loss: 1.1012, Val_Loss: 1.1013, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 48, Loss: 1.1017, Val_Loss: 1.1010, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 49, Loss: 1.1007, Val_Loss: 1.1008, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 50, Loss: 1.0999, Val_Loss: 1.1006, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 51, Loss: 1.0984, Val_Loss: 1.1003, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 52, Loss: 1.0991, Val_Loss: 1.1001, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 53, Loss: 1.0994, Val_Loss: 1.0999, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 54, Loss: 1.0963, Val_Loss: 1.0996, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 55, Loss: 1.0967, Val_Loss: 1.0994, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 56, Loss: 1.0972, Val_Loss: 1.0992, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 57, Loss: 1.0990, Val_Loss: 1.0989, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 58, Loss: 1.0979, Val_Loss: 1.0987, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 59, Loss: 1.0956, Val_Loss: 1.0984, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 60, Loss: 1.0955, Val_Loss: 1.0982, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 61, Loss: 1.0961, Val_Loss: 1.0980, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 62, Loss: 1.0962, Val_Loss: 1.0977, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 63, Loss: 1.0974, Val_Loss: 1.0975, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 64, Loss: 1.0982, Val_Loss: 1.0973, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 65, Loss: 1.0981, Val_Loss: 1.0970, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 66, Loss: 1.0960, Val_Loss: 1.0968, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 67, Loss: 1.0955, Val_Loss: 1.0966, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 68, Loss: 1.0952, Val_Loss: 1.0963, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 69, Loss: 1.0958, Val_Loss: 1.0961, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 70, Loss: 1.0929, Val_Loss: 1.0959, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 71, Loss: 1.0971, Val_Loss: 1.0956, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 72, Loss: 1.0927, Val_Loss: 1.0954, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 73, Loss: 1.0919, Val_Loss: 1.0952, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 74, Loss: 1.0968, Val_Loss: 1.0949, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 75, Loss: 1.0940, Val_Loss: 1.0947, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 76, Loss: 1.0938, Val_Loss: 1.0945, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 77, Loss: 1.0926, Val_Loss: 1.0942, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 78, Loss: 1.0928, Val_Loss: 1.0940, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 79, Loss: 1.0930, Val_Loss: 1.0938, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 80, Loss: 1.0923, Val_Loss: 1.0935, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 81, Loss: 1.0916, Val_Loss: 1.0933, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 82, Loss: 1.0923, Val_Loss: 1.0930, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 83, Loss: 1.0921, Val_Loss: 1.0928, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 84, Loss: 1.0924, Val_Loss: 1.0926, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 85, Loss: 1.0899, Val_Loss: 1.0923, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 86, Loss: 1.0918, Val_Loss: 1.0921, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 87, Loss: 1.0899, Val_Loss: 1.0919, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 88, Loss: 1.0892, Val_Loss: 1.0916, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 89, Loss: 1.0920, Val_Loss: 1.0914, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 90, Loss: 1.0909, Val_Loss: 1.0911, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 91, Loss: 1.0891, Val_Loss: 1.0909, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 92, Loss: 1.0907, Val_Loss: 1.0907, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 93, Loss: 1.0889, Val_Loss: 1.0904, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 94, Loss: 1.0918, Val_Loss: 1.0902, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 95, Loss: 1.0899, Val_Loss: 1.0899, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 96, Loss: 1.0893, Val_Loss: 1.0897, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 97, Loss: 1.0891, Val_Loss: 1.0894, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 98, Loss: 1.0872, Val_Loss: 1.0891, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 99, Loss: 1.0892, Val_Loss: 1.0889, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 100, Loss: 1.0882, Val_Loss: 1.0886, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 101, Loss: 1.0878, Val_Loss: 1.0884, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 102, Loss: 1.0912, Val_Loss: 1.0881, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 103, Loss: 1.0856, Val_Loss: 1.0878, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 104, Loss: 1.0868, Val_Loss: 1.0876, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 105, Loss: 1.0841, Val_Loss: 1.0873, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 106, Loss: 1.0887, Val_Loss: 1.0870, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 107, Loss: 1.0883, Val_Loss: 1.0867, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 108, Loss: 1.0852, Val_Loss: 1.0865, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 109, Loss: 1.0890, Val_Loss: 1.0862, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 110, Loss: 1.0875, Val_Loss: 1.0859, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 111, Loss: 1.0874, Val_Loss: 1.0856, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 112, Loss: 1.0836, Val_Loss: 1.0854, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 113, Loss: 1.0832, Val_Loss: 1.0851, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 114, Loss: 1.0897, Val_Loss: 1.0848, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 115, Loss: 1.0831, Val_Loss: 1.0845, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 116, Loss: 1.0833, Val_Loss: 1.0842, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 117, Loss: 1.0848, Val_Loss: 1.0839, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 118, Loss: 1.0839, Val_Loss: 1.0836, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 119, Loss: 1.0807, Val_Loss: 1.0833, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 120, Loss: 1.0837, Val_Loss: 1.0830, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 121, Loss: 1.0846, Val_Loss: 1.0827, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 122, Loss: 1.0824, Val_Loss: 1.0824, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 123, Loss: 1.0807, Val_Loss: 1.0821, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 124, Loss: 1.0829, Val_Loss: 1.0818, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 125, Loss: 1.0831, Val_Loss: 1.0815, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 126, Loss: 1.0810, Val_Loss: 1.0812, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 127, Loss: 1.0797, Val_Loss: 1.0809, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 128, Loss: 1.0820, Val_Loss: 1.0806, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 129, Loss: 1.0810, Val_Loss: 1.0803, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 130, Loss: 1.0798, Val_Loss: 1.0800, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 131, Loss: 1.0809, Val_Loss: 1.0797, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 132, Loss: 1.0814, Val_Loss: 1.0794, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 133, Loss: 1.0786, Val_Loss: 1.0791, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 134, Loss: 1.0826, Val_Loss: 1.0788, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 135, Loss: 1.0800, Val_Loss: 1.0785, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 136, Loss: 1.0800, Val_Loss: 1.0782, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 137, Loss: 1.0791, Val_Loss: 1.0779, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 138, Loss: 1.0806, Val_Loss: 1.0776, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 139, Loss: 1.0813, Val_Loss: 1.0773, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 140, Loss: 1.0800, Val_Loss: 1.0770, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 141, Loss: 1.0752, Val_Loss: 1.0767, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 142, Loss: 1.0789, Val_Loss: 1.0764, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 143, Loss: 1.0770, Val_Loss: 1.0761, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 144, Loss: 1.0754, Val_Loss: 1.0758, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 145, Loss: 1.0767, Val_Loss: 1.0754, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 146, Loss: 1.0753, Val_Loss: 1.0751, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 147, Loss: 1.0759, Val_Loss: 1.0748, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 148, Loss: 1.0759, Val_Loss: 1.0745, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 149, Loss: 1.0763, Val_Loss: 1.0742, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 150, Loss: 1.0730, Val_Loss: 1.0739, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 151, Loss: 1.0755, Val_Loss: 1.0736, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 152, Loss: 1.0736, Val_Loss: 1.0733, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 153, Loss: 1.0740, Val_Loss: 1.0729, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 154, Loss: 1.0721, Val_Loss: 1.0726, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 155, Loss: 1.0724, Val_Loss: 1.0723, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 156, Loss: 1.0727, Val_Loss: 1.0720, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 157, Loss: 1.0730, Val_Loss: 1.0717, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 158, Loss: 1.0716, Val_Loss: 1.0714, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 159, Loss: 1.0702, Val_Loss: 1.0710, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 160, Loss: 1.0730, Val_Loss: 1.0707, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 161, Loss: 1.0728, Val_Loss: 1.0704, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 162, Loss: 1.0750, Val_Loss: 1.0701, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 163, Loss: 1.0720, Val_Loss: 1.0698, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 164, Loss: 1.0700, Val_Loss: 1.0694, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 165, Loss: 1.0730, Val_Loss: 1.0691, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 166, Loss: 1.0730, Val_Loss: 1.0688, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 167, Loss: 1.0723, Val_Loss: 1.0685, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 168, Loss: 1.0681, Val_Loss: 1.0681, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 169, Loss: 1.0702, Val_Loss: 1.0678, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 170, Loss: 1.0681, Val_Loss: 1.0675, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 171, Loss: 1.0726, Val_Loss: 1.0672, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 172, Loss: 1.0731, Val_Loss: 1.0668, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 173, Loss: 1.0680, Val_Loss: 1.0665, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 174, Loss: 1.0696, Val_Loss: 1.0662, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 175, Loss: 1.0702, Val_Loss: 1.0659, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 176, Loss: 1.0684, Val_Loss: 1.0656, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 177, Loss: 1.0671, Val_Loss: 1.0652, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 178, Loss: 1.0672, Val_Loss: 1.0649, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 179, Loss: 1.0652, Val_Loss: 1.0646, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 180, Loss: 1.0671, Val_Loss: 1.0642, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 181, Loss: 1.0688, Val_Loss: 1.0639, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 182, Loss: 1.0666, Val_Loss: 1.0636, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 183, Loss: 1.0701, Val_Loss: 1.0633, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 184, Loss: 1.0653, Val_Loss: 1.0629, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 185, Loss: 1.0636, Val_Loss: 1.0626, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 186, Loss: 1.0577, Val_Loss: 1.0623, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 187, Loss: 1.0640, Val_Loss: 1.0619, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 188, Loss: 1.0664, Val_Loss: 1.0616, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 189, Loss: 1.0634, Val_Loss: 1.0613, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 190, Loss: 1.0606, Val_Loss: 1.0609, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 191, Loss: 1.0611, Val_Loss: 1.0606, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 192, Loss: 1.0614, Val_Loss: 1.0602, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 193, Loss: 1.0640, Val_Loss: 1.0599, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 194, Loss: 1.0615, Val_Loss: 1.0596, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 195, Loss: 1.0597, Val_Loss: 1.0592, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 196, Loss: 1.0597, Val_Loss: 1.0589, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 197, Loss: 1.0631, Val_Loss: 1.0586, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 198, Loss: 1.0613, Val_Loss: 1.0582, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 199, Loss: 1.0592, Val_Loss: 1.0579, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 200, Loss: 1.0686, Val_Loss: 1.0576, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 201, Loss: 1.0635, Val_Loss: 1.0572, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 202, Loss: 1.0626, Val_Loss: 1.0569, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 203, Loss: 1.0611, Val_Loss: 1.0565, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 204, Loss: 1.0548, Val_Loss: 1.0562, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 205, Loss: 1.0646, Val_Loss: 1.0559, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 206, Loss: 1.0638, Val_Loss: 1.0555, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 207, Loss: 1.0616, Val_Loss: 1.0552, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 208, Loss: 1.0594, Val_Loss: 1.0548, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 209, Loss: 1.0598, Val_Loss: 1.0545, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 210, Loss: 1.0573, Val_Loss: 1.0541, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 211, Loss: 1.0608, Val_Loss: 1.0537, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 212, Loss: 1.0609, Val_Loss: 1.0534, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 213, Loss: 1.0563, Val_Loss: 1.0530, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 214, Loss: 1.0552, Val_Loss: 1.0526, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 215, Loss: 1.0594, Val_Loss: 1.0523, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 216, Loss: 1.0601, Val_Loss: 1.0519, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 217, Loss: 1.0512, Val_Loss: 1.0515, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 218, Loss: 1.0565, Val_Loss: 1.0511, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 219, Loss: 1.0536, Val_Loss: 1.0508, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 220, Loss: 1.0553, Val_Loss: 1.0504, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 221, Loss: 1.0563, Val_Loss: 1.0500, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 222, Loss: 1.0537, Val_Loss: 1.0496, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 223, Loss: 1.0550, Val_Loss: 1.0492, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 224, Loss: 1.0564, Val_Loss: 1.0488, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 225, Loss: 1.0537, Val_Loss: 1.0485, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 226, Loss: 1.0536, Val_Loss: 1.0481, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 227, Loss: 1.0549, Val_Loss: 1.0477, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 228, Loss: 1.0516, Val_Loss: 1.0473, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 229, Loss: 1.0535, Val_Loss: 1.0469, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 230, Loss: 1.0527, Val_Loss: 1.0465, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 231, Loss: 1.0547, Val_Loss: 1.0461, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 232, Loss: 1.0522, Val_Loss: 1.0457, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 233, Loss: 1.0506, Val_Loss: 1.0453, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 234, Loss: 1.0528, Val_Loss: 1.0449, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 235, Loss: 1.0501, Val_Loss: 1.0445, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 236, Loss: 1.0529, Val_Loss: 1.0441, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 237, Loss: 1.0523, Val_Loss: 1.0437, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 238, Loss: 1.0504, Val_Loss: 1.0433, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 239, Loss: 1.0479, Val_Loss: 1.0429, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 240, Loss: 1.0504, Val_Loss: 1.0425, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 241, Loss: 1.0480, Val_Loss: 1.0421, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 242, Loss: 1.0509, Val_Loss: 1.0417, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 243, Loss: 1.0463, Val_Loss: 1.0413, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 244, Loss: 1.0466, Val_Loss: 1.0409, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 245, Loss: 1.0475, Val_Loss: 1.0405, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 246, Loss: 1.0462, Val_Loss: 1.0401, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 247, Loss: 1.0450, Val_Loss: 1.0397, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 248, Loss: 1.0468, Val_Loss: 1.0392, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 249, Loss: 1.0433, Val_Loss: 1.0388, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 250, Loss: 1.0462, Val_Loss: 1.0384, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 251, Loss: 1.0480, Val_Loss: 1.0380, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 252, Loss: 1.0414, Val_Loss: 1.0376, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 253, Loss: 1.0499, Val_Loss: 1.0372, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 254, Loss: 1.0441, Val_Loss: 1.0367, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 255, Loss: 1.0496, Val_Loss: 1.0363, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 256, Loss: 1.0477, Val_Loss: 1.0359, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 257, Loss: 1.0412, Val_Loss: 1.0355, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 258, Loss: 1.0434, Val_Loss: 1.0351, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 259, Loss: 1.0407, Val_Loss: 1.0347, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 260, Loss: 1.0411, Val_Loss: 1.0342, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 261, Loss: 1.0417, Val_Loss: 1.0338, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 262, Loss: 1.0444, Val_Loss: 1.0334, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 263, Loss: 1.0368, Val_Loss: 1.0330, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 264, Loss: 1.0393, Val_Loss: 1.0325, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 265, Loss: 1.0462, Val_Loss: 1.0321, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 266, Loss: 1.0408, Val_Loss: 1.0317, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 267, Loss: 1.0387, Val_Loss: 1.0313, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 268, Loss: 1.0413, Val_Loss: 1.0309, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 269, Loss: 1.0429, Val_Loss: 1.0304, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 270, Loss: 1.0378, Val_Loss: 1.0300, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 271, Loss: 1.0376, Val_Loss: 1.0296, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 272, Loss: 1.0360, Val_Loss: 1.0292, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 273, Loss: 1.0364, Val_Loss: 1.0287, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 274, Loss: 1.0367, Val_Loss: 1.0283, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 275, Loss: 1.0334, Val_Loss: 1.0279, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 276, Loss: 1.0384, Val_Loss: 1.0275, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 277, Loss: 1.0376, Val_Loss: 1.0270, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 278, Loss: 1.0452, Val_Loss: 1.0266, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 279, Loss: 1.0406, Val_Loss: 1.0262, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 280, Loss: 1.0399, Val_Loss: 1.0258, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 281, Loss: 1.0355, Val_Loss: 1.0254, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 282, Loss: 1.0381, Val_Loss: 1.0250, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 283, Loss: 1.0371, Val_Loss: 1.0245, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 284, Loss: 1.0313, Val_Loss: 1.0241, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 285, Loss: 1.0377, Val_Loss: 1.0237, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 286, Loss: 1.0375, Val_Loss: 1.0233, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 287, Loss: 1.0373, Val_Loss: 1.0229, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 288, Loss: 1.0320, Val_Loss: 1.0225, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 289, Loss: 1.0353, Val_Loss: 1.0221, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 290, Loss: 1.0345, Val_Loss: 1.0217, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 291, Loss: 1.0301, Val_Loss: 1.0213, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 292, Loss: 1.0282, Val_Loss: 1.0209, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 293, Loss: 1.0327, Val_Loss: 1.0205, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 294, Loss: 1.0313, Val_Loss: 1.0201, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 295, Loss: 1.0299, Val_Loss: 1.0196, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 296, Loss: 1.0309, Val_Loss: 1.0192, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 297, Loss: 1.0322, Val_Loss: 1.0188, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 298, Loss: 1.0337, Val_Loss: 1.0184, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 299, Loss: 1.0343, Val_Loss: 1.0180, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 300, Loss: 1.0288, Val_Loss: 1.0176, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 301, Loss: 1.0297, Val_Loss: 1.0172, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 302, Loss: 1.0328, Val_Loss: 1.0168, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 303, Loss: 1.0291, Val_Loss: 1.0164, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 304, Loss: 1.0287, Val_Loss: 1.0160, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 305, Loss: 1.0257, Val_Loss: 1.0156, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 306, Loss: 1.0297, Val_Loss: 1.0152, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 307, Loss: 1.0260, Val_Loss: 1.0148, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 308, Loss: 1.0301, Val_Loss: 1.0144, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 309, Loss: 1.0266, Val_Loss: 1.0140, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 310, Loss: 1.0276, Val_Loss: 1.0136, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 311, Loss: 1.0291, Val_Loss: 1.0132, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 312, Loss: 1.0226, Val_Loss: 1.0128, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 313, Loss: 1.0255, Val_Loss: 1.0124, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 314, Loss: 1.0261, Val_Loss: 1.0120, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 315, Loss: 1.0240, Val_Loss: 1.0116, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 316, Loss: 1.0266, Val_Loss: 1.0112, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 317, Loss: 1.0275, Val_Loss: 1.0108, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 318, Loss: 1.0265, Val_Loss: 1.0104, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 319, Loss: 1.0290, Val_Loss: 1.0100, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 320, Loss: 1.0208, Val_Loss: 1.0096, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 321, Loss: 1.0252, Val_Loss: 1.0092, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 322, Loss: 1.0251, Val_Loss: 1.0088, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 323, Loss: 1.0200, Val_Loss: 1.0085, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 324, Loss: 1.0201, Val_Loss: 1.0081, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 325, Loss: 1.0220, Val_Loss: 1.0077, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 326, Loss: 1.0191, Val_Loss: 1.0073, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 327, Loss: 1.0259, Val_Loss: 1.0069, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 328, Loss: 1.0274, Val_Loss: 1.0065, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 329, Loss: 1.0286, Val_Loss: 1.0061, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 330, Loss: 1.0282, Val_Loss: 1.0057, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 331, Loss: 1.0171, Val_Loss: 1.0054, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 332, Loss: 1.0262, Val_Loss: 1.0050, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 333, Loss: 1.0247, Val_Loss: 1.0046, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 334, Loss: 1.0233, Val_Loss: 1.0043, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 335, Loss: 1.0228, Val_Loss: 1.0039, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 336, Loss: 1.0196, Val_Loss: 1.0035, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 337, Loss: 1.0140, Val_Loss: 1.0032, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 338, Loss: 1.0209, Val_Loss: 1.0028, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 339, Loss: 1.0169, Val_Loss: 1.0024, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 340, Loss: 1.0215, Val_Loss: 1.0021, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 341, Loss: 1.0280, Val_Loss: 1.0017, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 342, Loss: 1.0114, Val_Loss: 1.0013, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 343, Loss: 1.0189, Val_Loss: 1.0010, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 344, Loss: 1.0123, Val_Loss: 1.0006, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 345, Loss: 1.0133, Val_Loss: 1.0003, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 346, Loss: 1.0140, Val_Loss: 0.9999, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 347, Loss: 1.0183, Val_Loss: 0.9995, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 348, Loss: 1.0205, Val_Loss: 0.9992, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 349, Loss: 1.0160, Val_Loss: 0.9988, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 350, Loss: 1.0240, Val_Loss: 0.9985, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 351, Loss: 1.0133, Val_Loss: 0.9981, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 352, Loss: 1.0107, Val_Loss: 0.9978, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 353, Loss: 1.0146, Val_Loss: 0.9974, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 354, Loss: 1.0128, Val_Loss: 0.9971, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 355, Loss: 1.0187, Val_Loss: 0.9967, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 356, Loss: 1.0132, Val_Loss: 0.9964, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 357, Loss: 1.0180, Val_Loss: 0.9960, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 358, Loss: 1.0148, Val_Loss: 0.9957, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 359, Loss: 1.0139, Val_Loss: 0.9953, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 360, Loss: 1.0066, Val_Loss: 0.9950, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 361, Loss: 1.0097, Val_Loss: 0.9946, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 362, Loss: 1.0129, Val_Loss: 0.9943, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 363, Loss: 1.0142, Val_Loss: 0.9940, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 364, Loss: 1.0196, Val_Loss: 0.9936, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 365, Loss: 1.0188, Val_Loss: 0.9933, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 366, Loss: 1.0149, Val_Loss: 0.9929, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 367, Loss: 1.0192, Val_Loss: 0.9926, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 368, Loss: 1.0057, Val_Loss: 0.9923, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 369, Loss: 1.0153, Val_Loss: 0.9920, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 370, Loss: 1.0106, Val_Loss: 0.9916, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 371, Loss: 1.0188, Val_Loss: 0.9913, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 372, Loss: 1.0124, Val_Loss: 0.9910, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 373, Loss: 1.0118, Val_Loss: 0.9907, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 374, Loss: 1.0166, Val_Loss: 0.9904, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 375, Loss: 1.0105, Val_Loss: 0.9901, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 376, Loss: 1.0049, Val_Loss: 0.9898, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 377, Loss: 1.0068, Val_Loss: 0.9895, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 378, Loss: 1.0108, Val_Loss: 0.9891, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 379, Loss: 1.0183, Val_Loss: 0.9888, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 380, Loss: 1.0083, Val_Loss: 0.9885, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 381, Loss: 1.0088, Val_Loss: 0.9882, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 382, Loss: 1.0064, Val_Loss: 0.9879, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 383, Loss: 1.0034, Val_Loss: 0.9876, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 384, Loss: 1.0155, Val_Loss: 0.9873, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 385, Loss: 1.0050, Val_Loss: 0.9870, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 386, Loss: 1.0117, Val_Loss: 0.9867, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 387, Loss: 1.0092, Val_Loss: 0.9864, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 388, Loss: 1.0072, Val_Loss: 0.9861, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 389, Loss: 1.0089, Val_Loss: 0.9858, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 390, Loss: 1.0066, Val_Loss: 0.9855, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 391, Loss: 1.0133, Val_Loss: 0.9852, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 392, Loss: 1.0077, Val_Loss: 0.9849, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 393, Loss: 1.0106, Val_Loss: 0.9846, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 394, Loss: 1.0086, Val_Loss: 0.9843, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 395, Loss: 1.0021, Val_Loss: 0.9841, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 396, Loss: 1.0046, Val_Loss: 0.9838, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 397, Loss: 0.9988, Val_Loss: 0.9835, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 398, Loss: 0.9927, Val_Loss: 0.9832, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 399, Loss: 1.0153, Val_Loss: 0.9829, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 400, Loss: 1.0090, Val_Loss: 0.9826, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 401, Loss: 1.0086, Val_Loss: 0.9823, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 402, Loss: 1.0051, Val_Loss: 0.9820, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 403, Loss: 1.0046, Val_Loss: 0.9817, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 404, Loss: 1.0034, Val_Loss: 0.9815, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 405, Loss: 1.0152, Val_Loss: 0.9812, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 406, Loss: 1.0062, Val_Loss: 0.9809, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 407, Loss: 1.0074, Val_Loss: 0.9806, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 408, Loss: 1.0011, Val_Loss: 0.9803, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 409, Loss: 1.0003, Val_Loss: 0.9801, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 410, Loss: 1.0036, Val_Loss: 0.9798, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 411, Loss: 1.0056, Val_Loss: 0.9795, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 412, Loss: 0.9997, Val_Loss: 0.9792, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 413, Loss: 0.9988, Val_Loss: 0.9789, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 414, Loss: 1.0035, Val_Loss: 0.9787, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 415, Loss: 1.0039, Val_Loss: 0.9784, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 416, Loss: 1.0083, Val_Loss: 0.9781, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 417, Loss: 1.0062, Val_Loss: 0.9778, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 418, Loss: 0.9992, Val_Loss: 0.9776, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 419, Loss: 1.0005, Val_Loss: 0.9773, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 420, Loss: 1.0028, Val_Loss: 0.9770, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 421, Loss: 1.0016, Val_Loss: 0.9768, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 422, Loss: 1.0063, Val_Loss: 0.9765, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 423, Loss: 1.0012, Val_Loss: 0.9762, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 424, Loss: 1.0077, Val_Loss: 0.9760, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 425, Loss: 1.0088, Val_Loss: 0.9757, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 426, Loss: 1.0013, Val_Loss: 0.9755, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 427, Loss: 1.0038, Val_Loss: 0.9752, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 428, Loss: 1.0075, Val_Loss: 0.9750, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 429, Loss: 1.0058, Val_Loss: 0.9748, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 430, Loss: 1.0085, Val_Loss: 0.9745, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 431, Loss: 0.9962, Val_Loss: 0.9743, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 432, Loss: 1.0069, Val_Loss: 0.9741, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 433, Loss: 0.9988, Val_Loss: 0.9738, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 434, Loss: 1.0006, Val_Loss: 0.9736, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 435, Loss: 0.9959, Val_Loss: 0.9734, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 436, Loss: 1.0026, Val_Loss: 0.9732, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 437, Loss: 1.0023, Val_Loss: 0.9729, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 438, Loss: 0.9988, Val_Loss: 0.9727, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 439, Loss: 1.0006, Val_Loss: 0.9725, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 440, Loss: 0.9970, Val_Loss: 0.9723, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 441, Loss: 1.0007, Val_Loss: 0.9721, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 442, Loss: 1.0026, Val_Loss: 0.9718, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 443, Loss: 1.0005, Val_Loss: 0.9716, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 444, Loss: 1.0013, Val_Loss: 0.9714, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 445, Loss: 0.9985, Val_Loss: 0.9712, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 446, Loss: 1.0098, Val_Loss: 0.9710, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 447, Loss: 1.0051, Val_Loss: 0.9708, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 448, Loss: 1.0003, Val_Loss: 0.9706, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 449, Loss: 1.0063, Val_Loss: 0.9704, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 450, Loss: 0.9939, Val_Loss: 0.9702, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 451, Loss: 1.0025, Val_Loss: 0.9700, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 452, Loss: 1.0001, Val_Loss: 0.9698, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 453, Loss: 1.0065, Val_Loss: 0.9696, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 454, Loss: 0.9914, Val_Loss: 0.9694, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 455, Loss: 1.0077, Val_Loss: 0.9693, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 456, Loss: 0.9967, Val_Loss: 0.9691, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 457, Loss: 1.0043, Val_Loss: 0.9689, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 458, Loss: 0.9960, Val_Loss: 0.9687, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 459, Loss: 0.9997, Val_Loss: 0.9685, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 460, Loss: 0.9969, Val_Loss: 0.9683, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 461, Loss: 0.9898, Val_Loss: 0.9682, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 462, Loss: 1.0017, Val_Loss: 0.9680, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 463, Loss: 0.9954, Val_Loss: 0.9678, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 464, Loss: 1.0041, Val_Loss: 0.9676, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 465, Loss: 0.9944, Val_Loss: 0.9674, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 466, Loss: 0.9985, Val_Loss: 0.9673, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 467, Loss: 1.0000, Val_Loss: 0.9671, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 468, Loss: 0.9996, Val_Loss: 0.9669, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 469, Loss: 0.9997, Val_Loss: 0.9668, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 470, Loss: 1.0067, Val_Loss: 0.9666, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 471, Loss: 1.0062, Val_Loss: 0.9665, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 472, Loss: 0.9982, Val_Loss: 0.9663, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 473, Loss: 0.9959, Val_Loss: 0.9662, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 474, Loss: 1.0138, Val_Loss: 0.9661, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 475, Loss: 0.9978, Val_Loss: 0.9659, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 476, Loss: 0.9973, Val_Loss: 0.9658, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 477, Loss: 1.0034, Val_Loss: 0.9657, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 478, Loss: 0.9989, Val_Loss: 0.9655, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 479, Loss: 0.9979, Val_Loss: 0.9654, Train: 0.4512, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 480, Loss: 1.0055, Val_Loss: 0.9653, Train: 0.4550, Val: 0.4500 Test: 0.4900\n",
      "Epoch: 481, Loss: 0.9949, Val_Loss: 0.9651, Train: 0.4537, Val: 0.4500 Test: 0.4900\n",
      "Epoch: 482, Loss: 1.0008, Val_Loss: 0.9650, Train: 0.4562, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 483, Loss: 0.9966, Val_Loss: 0.9649, Train: 0.4525, Val: 0.4300 Test: 0.4900\n",
      "Epoch: 484, Loss: 0.9994, Val_Loss: 0.9647, Train: 0.4562, Val: 0.4300 Test: 0.4900\n",
      "Epoch: 485, Loss: 0.9982, Val_Loss: 0.9646, Train: 0.4550, Val: 0.4500 Test: 0.5000\n",
      "Epoch: 486, Loss: 0.9979, Val_Loss: 0.9645, Train: 0.4637, Val: 0.4600 Test: 0.5000\n",
      "Epoch: 487, Loss: 0.9997, Val_Loss: 0.9644, Train: 0.4612, Val: 0.4700 Test: 0.5000\n",
      "Epoch: 488, Loss: 0.9881, Val_Loss: 0.9642, Train: 0.4625, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 489, Loss: 1.0026, Val_Loss: 0.9641, Train: 0.4637, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 490, Loss: 0.9923, Val_Loss: 0.9640, Train: 0.4625, Val: 0.4600 Test: 0.5000\n",
      "Epoch: 491, Loss: 1.0051, Val_Loss: 0.9638, Train: 0.4637, Val: 0.4600 Test: 0.5000\n",
      "Epoch: 492, Loss: 0.9984, Val_Loss: 0.9637, Train: 0.4625, Val: 0.4600 Test: 0.5000\n",
      "Epoch: 493, Loss: 0.9915, Val_Loss: 0.9636, Train: 0.4625, Val: 0.4600 Test: 0.5000\n",
      "Epoch: 494, Loss: 0.9968, Val_Loss: 0.9635, Train: 0.4625, Val: 0.4600 Test: 0.5000\n",
      "Epoch: 495, Loss: 0.9976, Val_Loss: 0.9633, Train: 0.4625, Val: 0.4600 Test: 0.5000\n",
      "Epoch: 496, Loss: 0.9827, Val_Loss: 0.9632, Train: 0.4625, Val: 0.4600 Test: 0.5000\n",
      "Epoch: 497, Loss: 0.9949, Val_Loss: 0.9631, Train: 0.4625, Val: 0.4600 Test: 0.5000\n",
      "Epoch: 498, Loss: 0.9993, Val_Loss: 0.9629, Train: 0.4625, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 499, Loss: 0.9918, Val_Loss: 0.9628, Train: 0.4625, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 500, Loss: 0.9921, Val_Loss: 0.9627, Train: 0.4625, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 501, Loss: 0.9967, Val_Loss: 0.9625, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 502, Loss: 0.9950, Val_Loss: 0.9624, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 503, Loss: 0.9916, Val_Loss: 0.9623, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 504, Loss: 1.0004, Val_Loss: 0.9622, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 505, Loss: 0.9964, Val_Loss: 0.9620, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 506, Loss: 0.9941, Val_Loss: 0.9619, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 507, Loss: 0.9960, Val_Loss: 0.9618, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 508, Loss: 0.9924, Val_Loss: 0.9617, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 509, Loss: 0.9925, Val_Loss: 0.9615, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 510, Loss: 0.9942, Val_Loss: 0.9614, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 511, Loss: 0.9825, Val_Loss: 0.9613, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 512, Loss: 0.9928, Val_Loss: 0.9612, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 513, Loss: 0.9968, Val_Loss: 0.9610, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 514, Loss: 0.9996, Val_Loss: 0.9609, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 515, Loss: 0.9923, Val_Loss: 0.9608, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 516, Loss: 0.9984, Val_Loss: 0.9607, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 517, Loss: 0.9951, Val_Loss: 0.9606, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 518, Loss: 0.9939, Val_Loss: 0.9605, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 519, Loss: 0.9944, Val_Loss: 0.9604, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 520, Loss: 0.9940, Val_Loss: 0.9603, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 521, Loss: 0.9967, Val_Loss: 0.9602, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 522, Loss: 0.9911, Val_Loss: 0.9601, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 523, Loss: 1.0021, Val_Loss: 0.9600, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 524, Loss: 0.9864, Val_Loss: 0.9599, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 525, Loss: 1.0000, Val_Loss: 0.9598, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 526, Loss: 0.9997, Val_Loss: 0.9597, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 527, Loss: 0.9963, Val_Loss: 0.9596, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 528, Loss: 0.9881, Val_Loss: 0.9595, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 529, Loss: 0.9933, Val_Loss: 0.9594, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 530, Loss: 0.9784, Val_Loss: 0.9593, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 531, Loss: 0.9914, Val_Loss: 0.9592, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 532, Loss: 0.9823, Val_Loss: 0.9591, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 533, Loss: 0.9915, Val_Loss: 0.9590, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 534, Loss: 0.9856, Val_Loss: 0.9589, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 535, Loss: 1.0039, Val_Loss: 0.9588, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 536, Loss: 1.0003, Val_Loss: 0.9587, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 537, Loss: 0.9891, Val_Loss: 0.9586, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 538, Loss: 0.9878, Val_Loss: 0.9585, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 539, Loss: 0.9849, Val_Loss: 0.9584, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 540, Loss: 1.0024, Val_Loss: 0.9584, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 541, Loss: 1.0034, Val_Loss: 0.9583, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 542, Loss: 0.9918, Val_Loss: 0.9582, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 543, Loss: 0.9918, Val_Loss: 0.9581, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 544, Loss: 0.9926, Val_Loss: 0.9581, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 545, Loss: 0.9966, Val_Loss: 0.9580, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 546, Loss: 0.9779, Val_Loss: 0.9579, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 547, Loss: 0.9928, Val_Loss: 0.9578, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 548, Loss: 0.9815, Val_Loss: 0.9577, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 549, Loss: 0.9885, Val_Loss: 0.9576, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 550, Loss: 0.9850, Val_Loss: 0.9575, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 551, Loss: 0.9972, Val_Loss: 0.9575, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 552, Loss: 0.9879, Val_Loss: 0.9574, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 553, Loss: 0.9903, Val_Loss: 0.9573, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 554, Loss: 0.9958, Val_Loss: 0.9572, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 555, Loss: 0.9897, Val_Loss: 0.9571, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 556, Loss: 0.9728, Val_Loss: 0.9570, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 557, Loss: 0.9963, Val_Loss: 0.9570, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 558, Loss: 0.9931, Val_Loss: 0.9569, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 559, Loss: 0.9818, Val_Loss: 0.9568, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 560, Loss: 0.9904, Val_Loss: 0.9567, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 561, Loss: 0.9983, Val_Loss: 0.9567, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 562, Loss: 0.9967, Val_Loss: 0.9566, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 563, Loss: 0.9891, Val_Loss: 0.9565, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 564, Loss: 0.9877, Val_Loss: 0.9565, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 565, Loss: 0.9989, Val_Loss: 0.9564, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 566, Loss: 0.9884, Val_Loss: 0.9563, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 567, Loss: 0.9983, Val_Loss: 0.9563, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 568, Loss: 0.9870, Val_Loss: 0.9562, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 569, Loss: 0.9931, Val_Loss: 0.9562, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 570, Loss: 0.9932, Val_Loss: 0.9561, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 571, Loss: 1.0011, Val_Loss: 0.9561, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 572, Loss: 0.9935, Val_Loss: 0.9560, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 573, Loss: 0.9895, Val_Loss: 0.9560, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 574, Loss: 0.9873, Val_Loss: 0.9559, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 575, Loss: 0.9876, Val_Loss: 0.9558, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 576, Loss: 0.9859, Val_Loss: 0.9558, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 577, Loss: 0.9961, Val_Loss: 0.9557, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 578, Loss: 0.9817, Val_Loss: 0.9557, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 579, Loss: 1.0003, Val_Loss: 0.9556, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 580, Loss: 0.9932, Val_Loss: 0.9556, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 581, Loss: 0.9917, Val_Loss: 0.9555, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 582, Loss: 0.9912, Val_Loss: 0.9555, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 583, Loss: 0.9994, Val_Loss: 0.9554, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 584, Loss: 0.9969, Val_Loss: 0.9554, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 585, Loss: 0.9908, Val_Loss: 0.9554, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 586, Loss: 0.9807, Val_Loss: 0.9553, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 587, Loss: 0.9879, Val_Loss: 0.9553, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 588, Loss: 0.9904, Val_Loss: 0.9552, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 589, Loss: 0.9832, Val_Loss: 0.9552, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 590, Loss: 0.9931, Val_Loss: 0.9552, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 591, Loss: 0.9994, Val_Loss: 0.9551, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 592, Loss: 0.9883, Val_Loss: 0.9551, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 593, Loss: 0.9935, Val_Loss: 0.9550, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 594, Loss: 0.9851, Val_Loss: 0.9550, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 595, Loss: 0.9933, Val_Loss: 0.9550, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 596, Loss: 0.9790, Val_Loss: 0.9549, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 597, Loss: 0.9942, Val_Loss: 0.9549, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 598, Loss: 0.9886, Val_Loss: 0.9549, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 599, Loss: 0.9890, Val_Loss: 0.9548, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 600, Loss: 0.9869, Val_Loss: 0.9548, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 601, Loss: 0.9902, Val_Loss: 0.9547, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 602, Loss: 0.9885, Val_Loss: 0.9547, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 603, Loss: 0.9924, Val_Loss: 0.9547, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 604, Loss: 0.9971, Val_Loss: 0.9546, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 605, Loss: 0.9802, Val_Loss: 0.9546, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 606, Loss: 0.9905, Val_Loss: 0.9546, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 607, Loss: 0.9870, Val_Loss: 0.9545, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 608, Loss: 0.9861, Val_Loss: 0.9545, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 609, Loss: 0.9806, Val_Loss: 0.9545, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 610, Loss: 0.9930, Val_Loss: 0.9544, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 611, Loss: 0.9935, Val_Loss: 0.9544, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 612, Loss: 0.9917, Val_Loss: 0.9544, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 613, Loss: 0.9969, Val_Loss: 0.9543, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 614, Loss: 0.9850, Val_Loss: 0.9543, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 615, Loss: 0.9912, Val_Loss: 0.9543, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 616, Loss: 0.9942, Val_Loss: 0.9543, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 617, Loss: 0.9934, Val_Loss: 0.9543, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 618, Loss: 0.9910, Val_Loss: 0.9543, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 619, Loss: 0.9939, Val_Loss: 0.9542, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 620, Loss: 0.9897, Val_Loss: 0.9542, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 621, Loss: 0.9924, Val_Loss: 0.9542, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 622, Loss: 0.9858, Val_Loss: 0.9542, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 623, Loss: 0.9849, Val_Loss: 0.9542, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 624, Loss: 0.9847, Val_Loss: 0.9541, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 625, Loss: 0.9856, Val_Loss: 0.9541, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 626, Loss: 0.9915, Val_Loss: 0.9541, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 627, Loss: 0.9864, Val_Loss: 0.9541, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 628, Loss: 0.9845, Val_Loss: 0.9540, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 629, Loss: 0.9822, Val_Loss: 0.9540, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 630, Loss: 0.9898, Val_Loss: 0.9540, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 631, Loss: 0.9888, Val_Loss: 0.9540, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 632, Loss: 0.9861, Val_Loss: 0.9539, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 633, Loss: 0.9834, Val_Loss: 0.9539, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 634, Loss: 0.9774, Val_Loss: 0.9539, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 635, Loss: 0.9928, Val_Loss: 0.9539, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 636, Loss: 0.9826, Val_Loss: 0.9538, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 637, Loss: 0.9900, Val_Loss: 0.9538, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 638, Loss: 0.9829, Val_Loss: 0.9538, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 639, Loss: 0.9769, Val_Loss: 0.9537, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 640, Loss: 0.9912, Val_Loss: 0.9537, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 641, Loss: 0.9930, Val_Loss: 0.9537, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 642, Loss: 0.9863, Val_Loss: 0.9537, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 643, Loss: 0.9855, Val_Loss: 0.9536, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 644, Loss: 0.9825, Val_Loss: 0.9536, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 645, Loss: 0.9931, Val_Loss: 0.9536, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 646, Loss: 0.9931, Val_Loss: 0.9536, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 647, Loss: 0.9859, Val_Loss: 0.9536, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 648, Loss: 0.9882, Val_Loss: 0.9536, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 649, Loss: 0.9857, Val_Loss: 0.9535, Train: 0.4612, Val: 0.4600 Test: 0.5100\n",
      "Epoch: 650, Loss: 0.9911, Val_Loss: 0.9535, Train: 0.4625, Val: 0.4600 Test: 0.5200\n",
      "Epoch: 651, Loss: 0.9865, Val_Loss: 0.9535, Train: 0.4625, Val: 0.4600 Test: 0.5200\n",
      "Epoch: 652, Loss: 0.9820, Val_Loss: 0.9535, Train: 0.4625, Val: 0.4600 Test: 0.5200\n",
      "Epoch: 653, Loss: 0.9926, Val_Loss: 0.9535, Train: 0.4625, Val: 0.4600 Test: 0.5200\n",
      "Epoch: 654, Loss: 0.9848, Val_Loss: 0.9535, Train: 0.4637, Val: 0.4600 Test: 0.5200\n",
      "Epoch: 655, Loss: 0.9964, Val_Loss: 0.9535, Train: 0.4637, Val: 0.4600 Test: 0.5200\n",
      "Epoch: 656, Loss: 0.9863, Val_Loss: 0.9535, Train: 0.4637, Val: 0.4600 Test: 0.5200\n",
      "Epoch: 657, Loss: 0.9893, Val_Loss: 0.9535, Train: 0.4637, Val: 0.4600 Test: 0.5200\n",
      "Epoch: 658, Loss: 0.9936, Val_Loss: 0.9534, Train: 0.4637, Val: 0.4600 Test: 0.5200\n",
      "Epoch: 659, Loss: 0.9901, Val_Loss: 0.9534, Train: 0.4650, Val: 0.4700 Test: 0.5200\n",
      "Epoch: 660, Loss: 0.9842, Val_Loss: 0.9534, Train: 0.4663, Val: 0.4700 Test: 0.5200\n",
      "Epoch: 661, Loss: 0.9802, Val_Loss: 0.9534, Train: 0.4663, Val: 0.4700 Test: 0.5200\n",
      "Epoch: 662, Loss: 0.9828, Val_Loss: 0.9534, Train: 0.4675, Val: 0.4700 Test: 0.5200\n",
      "Epoch: 663, Loss: 0.9846, Val_Loss: 0.9534, Train: 0.4688, Val: 0.4700 Test: 0.5200\n",
      "Epoch: 664, Loss: 0.9910, Val_Loss: 0.9534, Train: 0.4688, Val: 0.4800 Test: 0.5200\n",
      "Epoch: 665, Loss: 0.9873, Val_Loss: 0.9534, Train: 0.4700, Val: 0.4900 Test: 0.5200\n",
      "Epoch: 666, Loss: 0.9930, Val_Loss: 0.9534, Train: 0.4712, Val: 0.4900 Test: 0.5300\n",
      "Epoch: 667, Loss: 0.9951, Val_Loss: 0.9534, Train: 0.4725, Val: 0.4800 Test: 0.5300\n",
      "Epoch: 668, Loss: 0.9800, Val_Loss: 0.9534, Train: 0.4725, Val: 0.4800 Test: 0.5300\n",
      "Epoch: 669, Loss: 0.9831, Val_Loss: 0.9534, Train: 0.4725, Val: 0.4800 Test: 0.5300\n",
      "Epoch: 670, Loss: 0.9908, Val_Loss: 0.9534, Train: 0.4725, Val: 0.4800 Test: 0.5300\n",
      "Epoch: 671, Loss: 0.9879, Val_Loss: 0.9534, Train: 0.4725, Val: 0.4800 Test: 0.5300\n",
      "Epoch: 672, Loss: 0.9793, Val_Loss: 0.9534, Train: 0.4700, Val: 0.4800 Test: 0.5300\n",
      "Epoch: 673, Loss: 0.9817, Val_Loss: 0.9534, Train: 0.4675, Val: 0.4800 Test: 0.5300\n",
      "Epoch: 674, Loss: 0.9866, Val_Loss: 0.9534, Train: 0.4663, Val: 0.4800 Test: 0.5200\n",
      "Epoch: 675, Loss: 0.9844, Val_Loss: 0.9534, Train: 0.4675, Val: 0.4800 Test: 0.5200\n",
      "Epoch: 676, Loss: 0.9907, Val_Loss: 0.9534, Train: 0.4688, Val: 0.4800 Test: 0.5200\n",
      "Epoch: 677, Loss: 0.9877, Val_Loss: 0.9534, Train: 0.4688, Val: 0.4800 Test: 0.5100\n",
      "Epoch: 678, Loss: 0.9793, Val_Loss: 0.9534, Train: 0.4675, Val: 0.4800 Test: 0.5100\n",
      "Epoch: 679, Loss: 0.9834, Val_Loss: 0.9534, Train: 0.4675, Val: 0.4800 Test: 0.5100\n",
      "Epoch: 680, Loss: 0.9798, Val_Loss: 0.9533, Train: 0.4675, Val: 0.4800 Test: 0.5100\n",
      "Epoch: 681, Loss: 0.9899, Val_Loss: 0.9533, Train: 0.4675, Val: 0.4800 Test: 0.5100\n",
      "Epoch: 682, Loss: 0.9939, Val_Loss: 0.9533, Train: 0.4663, Val: 0.4800 Test: 0.5100\n",
      "Epoch: 683, Loss: 0.9856, Val_Loss: 0.9533, Train: 0.4663, Val: 0.4800 Test: 0.5100\n",
      "Epoch: 684, Loss: 0.9867, Val_Loss: 0.9533, Train: 0.4663, Val: 0.4800 Test: 0.5100\n",
      "Epoch: 685, Loss: 0.9903, Val_Loss: 0.9533, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 686, Loss: 0.9854, Val_Loss: 0.9533, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 687, Loss: 0.9850, Val_Loss: 0.9533, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 688, Loss: 0.9915, Val_Loss: 0.9533, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 689, Loss: 0.9846, Val_Loss: 0.9533, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 690, Loss: 0.9823, Val_Loss: 0.9533, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 691, Loss: 0.9815, Val_Loss: 0.9533, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 692, Loss: 0.9917, Val_Loss: 0.9533, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 693, Loss: 0.9913, Val_Loss: 0.9533, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 694, Loss: 0.9785, Val_Loss: 0.9532, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 695, Loss: 0.9926, Val_Loss: 0.9532, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 696, Loss: 0.9908, Val_Loss: 0.9532, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 697, Loss: 0.9842, Val_Loss: 0.9532, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 698, Loss: 0.9910, Val_Loss: 0.9532, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 699, Loss: 0.9725, Val_Loss: 0.9532, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 700, Loss: 0.9785, Val_Loss: 0.9532, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 701, Loss: 0.9780, Val_Loss: 0.9532, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 702, Loss: 0.9819, Val_Loss: 0.9532, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 703, Loss: 0.9764, Val_Loss: 0.9532, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 704, Loss: 0.9777, Val_Loss: 0.9532, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 705, Loss: 0.9864, Val_Loss: 0.9531, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 706, Loss: 0.9925, Val_Loss: 0.9531, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 707, Loss: 0.9895, Val_Loss: 0.9531, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 708, Loss: 0.9917, Val_Loss: 0.9531, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 709, Loss: 0.9834, Val_Loss: 0.9531, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 710, Loss: 0.9798, Val_Loss: 0.9531, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 711, Loss: 0.9884, Val_Loss: 0.9531, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 712, Loss: 0.9851, Val_Loss: 0.9531, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 713, Loss: 0.9910, Val_Loss: 0.9531, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 714, Loss: 0.9893, Val_Loss: 0.9531, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 715, Loss: 0.9800, Val_Loss: 0.9531, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 716, Loss: 0.9896, Val_Loss: 0.9531, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 717, Loss: 0.9823, Val_Loss: 0.9531, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 718, Loss: 0.9880, Val_Loss: 0.9531, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 719, Loss: 0.9883, Val_Loss: 0.9531, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 720, Loss: 0.9856, Val_Loss: 0.9531, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 721, Loss: 0.9881, Val_Loss: 0.9531, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 722, Loss: 1.0041, Val_Loss: 0.9531, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 723, Loss: 0.9723, Val_Loss: 0.9531, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 724, Loss: 0.9910, Val_Loss: 0.9531, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 725, Loss: 0.9797, Val_Loss: 0.9531, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 726, Loss: 0.9778, Val_Loss: 0.9531, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 727, Loss: 0.9746, Val_Loss: 0.9531, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 728, Loss: 0.9959, Val_Loss: 0.9531, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 729, Loss: 0.9789, Val_Loss: 0.9531, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 730, Loss: 0.9909, Val_Loss: 0.9531, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 731, Loss: 0.9894, Val_Loss: 0.9531, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 732, Loss: 0.9781, Val_Loss: 0.9531, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 733, Loss: 0.9844, Val_Loss: 0.9532, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 734, Loss: 0.9755, Val_Loss: 0.9532, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 735, Loss: 0.9870, Val_Loss: 0.9532, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 736, Loss: 0.9854, Val_Loss: 0.9532, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 737, Loss: 0.9834, Val_Loss: 0.9532, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 738, Loss: 0.9996, Val_Loss: 0.9532, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 739, Loss: 0.9938, Val_Loss: 0.9532, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 740, Loss: 0.9753, Val_Loss: 0.9532, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 741, Loss: 0.9826, Val_Loss: 0.9532, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 742, Loss: 0.9736, Val_Loss: 0.9532, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 743, Loss: 0.9737, Val_Loss: 0.9532, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 744, Loss: 0.9937, Val_Loss: 0.9532, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 745, Loss: 0.9883, Val_Loss: 0.9532, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 746, Loss: 0.9793, Val_Loss: 0.9532, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 747, Loss: 0.9819, Val_Loss: 0.9532, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 748, Loss: 0.9905, Val_Loss: 0.9533, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 749, Loss: 0.9891, Val_Loss: 0.9533, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 750, Loss: 0.9752, Val_Loss: 0.9533, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 751, Loss: 0.9935, Val_Loss: 0.9533, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 752, Loss: 0.9780, Val_Loss: 0.9533, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 753, Loss: 0.9850, Val_Loss: 0.9533, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 754, Loss: 0.9842, Val_Loss: 0.9533, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 755, Loss: 0.9926, Val_Loss: 0.9533, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 756, Loss: 0.9873, Val_Loss: 0.9533, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 757, Loss: 0.9753, Val_Loss: 0.9533, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 758, Loss: 0.9867, Val_Loss: 0.9533, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 759, Loss: 0.9841, Val_Loss: 0.9533, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 760, Loss: 0.9865, Val_Loss: 0.9533, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 761, Loss: 0.9784, Val_Loss: 0.9533, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 762, Loss: 0.9836, Val_Loss: 0.9533, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 763, Loss: 0.9754, Val_Loss: 0.9533, Train: 0.4663, Val: 0.4700 Test: 0.5100\n",
      "Epoch: 764, Loss: 0.9763, Val_Loss: 0.9533, Train: 0.4650, Val: 0.4700 Test: 0.5100\n",
      "Early stopping...\n",
      "Median time per epoch: 0.0019s\n"
     ]
    }
   ],
   "source": [
    "# MLP classifier\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(32, 3),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_relu_stack(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "model = NeuralNetwork().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.0005)\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(train_x)\n",
    "    loss = F.nll_loss(out, train_y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # Validation\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():  # Disable gradient calculation for validation\n",
    "        val_output = model(val_x)\n",
    "        val_loss = F.nll_loss(val_output, val_y)\n",
    "    return float(loss), float(val_loss)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    pred = model(patient_embedding).argmax(dim=-1)\n",
    "    train_acc = float((pred[:int(num_patients*0.8)] == train_y).float().mean())\n",
    "    val_acc = float((pred[int(num_patients*0.8):int(num_patients*0.9)] == val_y).float().mean())\n",
    "    test_acc = float((pred[int(num_patients*0.9):] == test_y).float().mean())\n",
    "    return train_acc, val_acc, test_acc\n",
    "\n",
    "import time\n",
    "\n",
    "times = []\n",
    "best_loss = float('inf')\n",
    "\n",
    "for epoch in range(1, 1001):\n",
    "    start = time.time()\n",
    "    loss, val_loss = train()\n",
    "    # Early stopping\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        patience = 50  # Reset patience counter\n",
    "    else:\n",
    "        patience -= 1\n",
    "        if patience == 0:\n",
    "            print(\"Early stopping...\")\n",
    "            break\n",
    "    train_acc, val_acc, test_acc = test()\n",
    "    print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}, Val_Loss: {val_loss:.4f}, Train: {train_acc:.4f}, Val: {val_acc:.4f} '\n",
    "        f'Test: {test_acc:.4f}')\n",
    "        \n",
    "    times.append(time.time() - start)\n",
    "print(f\"Median time per epoch: {torch.tensor(times).median():.4f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC score: 0.5183\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(patient_embedding).cpu()\n",
    "    prob = F.softmax(out, dim=1)\n",
    "auc = roc_auc_score(test_y.cpu(), prob[int(num_patients*0.9):], multi_class='ovr')\n",
    "print(f'ROC AUC score: {auc:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neurovasc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
